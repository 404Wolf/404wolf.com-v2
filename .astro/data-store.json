[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.13.7","content-config-digest","677e06e0b7d84d91","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://404wolf.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,27,28,39,40,50,51,63,64,76,77,89,90,100,101,111,112,122,123],"404wolfcom",{"id":11,"data":13,"body":23,"filePath":24,"digest":25,"deferredRender":26},{"title":14,"type":15,"date":16,"covers":17,"tags":19,"description":22},"404Wolf.com","project","2024",[18],"https://static.404wolf.com/postEditor_0001.png",[20,21],"featured","academic","How I made my website, and an overview of all its unique features, including an integrated what-you-see-is-what-you-get markdown viewer, and fully featured Obsidian plugin. I explain my stack choice, how the posts are managed, and present my custom in-site editor. I also talk briefly about some specific struggles when building the website, like creating a good backup protocol and getting my markdown to parse properly and in a very specific way.\n","# Making [404Wolf.com](https://404wolf.com)\n\n## Inspiration\n\nI initially was planning on working on various other ongoing projects, such as an English flashcard generating service, but decided that I wanted to have a good place to document my progress on everything else. I started by making a simple homepage, and then when determining how to integrate a post/blog system fell down the rabbithole of learning my first stack and javascript for the first time. I could've used Wordpress, but I thought it would be a good chance to learn how to do it from scratch.\n\nI began working on this website during my second week of my batch at [Recurse Center](https://recurse.com), an amazing open-ended programming retreat based in Brooklyn, NY.\n\n## Choosing frameworks\n\nOne of the main goals of this website was to get started with webdev. It was my first proper \"full stack project.\" Previously I'd experimented with some templated HTML, but I hadn't done `javascript` before. I really enjoyed Harvard's CS50 React Native course's video, where they speed-teach the language (which is public and can be found [here](https://www.youtube.com/watch?v=X52b-8y2Hf4)). I also read over the [learnxinyminutes.com](https://learnxinyminutes.com) website to get a brief overview of the syntax. I've found that javascript (and, later, typescript), is weird, and not quite like other languages I've used. In some ways it's functional, especially with such flexible lambdas and promises. Yet, in others, it's object-orianted. Everything is a descendent of a primitive `object`, much like Python, and you can have \"classes,\" but using it functionally is often a lot more straightforward than in Python in my opinion.\n\nFor the frontend I went with React; I liked the idea of a composable component based system, it is quite popular, and I didn't really know what I was getting into. At the time I didn't know about React component libraries like MUI, so I ended up building everything from the ground up, which I think was good for learning, but bad for longer term maintenance.\n\nSince I knew I was going to want a backend for things like managing the posts on my website, I did a bit of research on how to set up a webserver for the project. I considered using `express` or `django` to serve the site and corresponding API, but a Recurse batch-mate introduced me to [NextJS](https://nextjs.org/), which was very easy to scaffold and fun to work with. It was also very easy (and free!) to get it hosted with [Vercel](https://vercel.com/). NextJS is an interesting framework that blends React for the frontend with Express for the backend, so that you can write your backend would-be API code in the same file directory, and even files, as your frontend code.\n\nOriginally, I made my website pages pull from a folder of markdown files which I edited and directly commited to Github. Eventually, though, it became unwieldy, and I met someone who introduced me to [Prisma](https://www.prisma.io/) and `Postgresql`. Postgres is a relational database system, an extension of SQL, and Prisma is a popular Javascript ORM (object-relational-mapping framework, used for querying the database with JS-object-style calls instead of literal SQL queries). I stored the posts and some resources in AWS S3 blob storage (flat ID to file storage), which at first was a pain to set up but got easier as I tinkered with the settings and began to understand how it (and auth) worked, referencing the images of my posts with keys that I stored in the database. It was a rabbit hole into my first full stack web app.\n\nI originally was planning on just using basic CSS, but after stumbling upon tailwind in a react guide I was reading, I decided that it was worth giving it a shot. It was simple to get set up, and made styling the website much easier and faster. Tailwind is a \"utility-first\" styling framework that provides a ton of useful classes that pre-pack styles so that you can do the styling directly in HTML instead of ever needing any CSS, sort of like bootstrap. For example, to add a bit of padding to the left of a `div`, you can simply add the class \"pl-3\". It has classes for pretty much every possible CSS style, and has predetermined discrete choices for sizes and colors (which can be customized, but in general help development more than hurt).\n\nOne fun thing I came across when designing the homepage UX is [Typewriter.js](https://safi.me.uk/typewriterjs/), a library for simulating typing. I thought it'd be a cool affect to apply to headers on my site, and then later experimented with other animations and animation frameworks a bit. It lets you do typewriter animations in a very nice modonic style.\n\n```js\ntypewriter\n  .typeString(\"Hello World!\")\n  .pauseFor(2500)\n  .deleteAll()\n  .typeString(\"Strings can be removed\")\n  .pauseFor(2500)\n  .deleteChars(7)\n  .start();\n```\n\n## Post Management\n\nOne of the key features of my website is its integrated post management system. Though originally my purpose was to just have a collection of my projects on the site, as a portfolio of sorts, I decided to also add the ability to post blogs too. All posts on the site are of a specific `type`, either `project` or `blog`. By going to the pattern matched URL `https://404wolf.com/posts/\u003Ctype>s`, you can view all posts of a specific type. Each post has a specific schema, which is used to generate a grid of tiles that present post previews.\n\n```ts\ninterface PostData {\n  coverUrl: string | null;\n  coverAlt: string | null;\n  id: string;\n  title: string;\n  description: string;\n  date: string;\n  tags: string[];\n}\n```\n\n![href](https://static.404wolf.com/postsPage_0001.png)\n\nThe posts themselves store this metadata in the database, and have corresponding markdown files in S3. When the page is loaded to view a post, the S3 markdown is prefetched, and is rendered client side. I use [react-markdown](https://github.com/remarkjs/react-markdown) to render the markdown, with some custom overrides to handle images and codeblocks. I talk about this more [in this blog I wrote](/posts/blog/imageBlocks), but I've also added image blocks to markdown, to allow for groups of many images to show up inline in the rendered markdown, by actually toying with the markdown to html `AST` process.\n\n### Post editor\n\n![Post editor](https://static.404wolf.com/postEditor_0001.png)\n![Editable S3 text area](https://static.404wolf.com/editAboutMe_0001.png)\n\nTo edit my posts, I decided to add an integrated editor into my website. This is because I want the website to automatically handle storage of associated post resources (images, files, markdown versions, etc.), rather than having to manually deal with uploading things to S3 and directly using the hard S3 links. My goal was to be able to reference the resources by keys (unique IDs I assign to them) directly in the markdown, and by integrating the editor into the website I've made it possible to have a what-you-see-is-what-you-get markdown editing system where the markdown automatically replaces aliases live. The actual post-editing page is quite complex since it includes many different fields to enter metadata for the project post, a split screen viewing panel, and a resource panel. The most notable part, seen in the `Editable S3 text area` image above, is I've made a component to directly edit an AWS S3 file with simple text area entry that is capable of displaying images and other artifacts as rendered markdown.\n\n![The resource editor](https://static.404wolf.com/https://static.404wolf.com/Post-20240710001621089.webp)\n\nThis system of using IDs was done with keys that are required to be unique by the user, which is probably not the ideal way of doing it -- I was manually naming files uniquely. I decided to solve this problem by making all posts upload with a unique ID automatically based on their filename, by appending `000n` to the end, like `filename_0004.png`. It's a bit clunky and requires iteration to find an unused file name, but it works for now.\n\n## Exporting/importing\n\n![Export tree](https://static.404wolf.com/exportTree_0001.png)\n\nIn general, I don't like locking myself into ecosystems, so from the beginning I knew I wanted a system to export the contents of the site. This website obviously relies on the hosting of various different cloud providers, which, though reliable, are fail points. When I created the post editing system, I also wrote two basic scripts to upload and download the all the posts and their respective metadata. All files associated with a specific post get stored in a folder alongside the post's markdown, and the metadata is saved as a simple `json`. Writing these scripts was rather simple, since I'm iteratively doing a database call for each post to download all the resources using the public S3 URL, and my ORM can give me a metadata json automatically. This script was made obsolete by my new Obsidian plugin for the website, though.\n\n# Obsidian Plugin\n\nComing soon!\n\n## Teasers\n\n![The commands](https://static.404wolf.com/Post-20240716223544716.webp)\n\n![My website's contents](https://static.404wolf.com/Post-20240716223617606.webp)","src/posts/404Wolf.com.mdx","983da45bd7005dee",true,"androidinthebrowser",{"id":27,"data":29,"body":36,"filePath":37,"digest":38,"deferredRender":26},{"title":30,"type":15,"date":16,"covers":31,"tags":33,"description":35},"Android in the Browser",[32],"https://static.404wolf.com/Post-20240807144914137.webp",[34],"personal","An ongoing effort to get a fully functional android phone embedded into an iFrame in a browser, with as little latency as possible. Note: post in progress!\n","# Notice\n\nThis is a long-term, very much in-progress project, so this post currently serves primarily as a progress tracker and collection of notes on the project, along with some information what I've accomplished so far.\n\n# Presentation\n\nA recreation of a 5 minute presentation I gave at [Recurse Center](https://recurse.com) for the project.\n\n![Quick overview](5minuteDemo.webm|float=none|width=100)\n\n# Inspiration\n\nSo, earlier this year I was working on my first React Native application, which was a small app to plan events. It was my first mobile project in general, and at the time I got started with it on a Windows machine to avoid the pains that I assumed may accompany mobile development package management.\n\nIt didn't take long to realize that it's very very hard to develop for IOS on anything other than a mac, because tooling to \"talk\" to iPhones for development relies on xCode. That means that I couldn't use a physical iPhone, or even run an emulator (a \"fake\" phone running on my computer).\n\n![Android studio](https://static.404wolf.com/https://static.404wolf.com/Post-20240807195159827.webp)\n\nFor Android development, Google is much nicer about it, and lets you run emulators and bridge a physical phone over on all platforms. They recommend using [Android Studio](https://developer.android.com/studio), which is _fine_, but a pain in the neck. The process to create an emulator in android studio is pretty straight forward, but requires a lot of clicking and is quite tedious and magical. You can choose a \"skin\" and android image, sometimes you configure the virtual device a bit further with their gui, and then run it from the app.\n\nI used it for the project, and it worked well enough for my needs. Metro, a bundler that creates and ships APKs based on your `react native` project works well with Android studio's emulator, and it was fine at the time, and was only moderately annoying to work with.\n\n## Android Development on Linux\n\nFast forward a few months, and I was returning to the project, but this time was running NixOS. It was my first half of a [Recurse Center](https://recurse.com) batch, and I was still figuring a lot of things out. One of the things that `nix` claims to be very good at is reproducible builds, meaning that I would expect packages in the `nixpkgs` package registry to \"just work.\"\n\nSo, I tried adding `android-studio` to my developer environment, and then ran it. It launched!\n\nI browsed through the configuration options to go and set up an emulator again, and, upon launching the emulator, it would say emulator booting, but fail to actually start.\n\nDebugging this, as a pretty new `nix` user at the time, was painful. I ran `android studio` from the command line, and enabled debug logs. It looked like there were a whole bunch of dependencies not for android studio, but for the emulators it was spawning. Part of the issue is that android studio usually pulls emulator images when you set up the emulators, not when you install android studio, so while the build of android studio that I got was working just fine, the emulators that it was pulling were broken, likely because of upstream changes.\n\nAndroid Studio is very heavy though; it's an entire Jetbrains IDE. I find it really clunky to use, and when things don't work it's a pain in the neck to debug since it's a massive GUI desktop app that does so many things under the hood.\n\nPlus, the licensing and codebase for it seems a bit sketchy and obstruce.\n\n> The code is under Apache-2.0, but:\n> If one selects Help -> Licenses in Android Studio, the dialog shows the following:\n> \"Android Studio includes proprietary code subject to separate license,\n> including JetBrains CLion(R) (www.jetbrains.com/clion) and IntelliJ(R)\n> IDEA Community Edition (www.jetbrains.com/idea).\"\n> Also: For actual development the Android SDK is required and the Google\n> binaries are also distributed as proprietary software (unlike the\n> source-code itself).\n\nI like open source tooling, and I don't like magic. So, I decided that it would be productive to delve deeper and figure out how to do what it's doing without the entire desktop app.\n\n## Android CLI Tooling\n\nThe general breakdown is that there's an android SDK toolkit that provides you with a ton of command line utilities for android development and debugging. This SDK is included in distributions of android studio, and they are what Android studio uses internally under the hood. Furthermore, `nixpkgs` already has packaged them under`android-tools` (source [here](https://github.com/NixOS/nixpkgs/blob/nixos-24.05/pkgs/tools/misc/android-tools/default.nix#L28)). Because the android build system is so annoying to deal with, someone created [a project to build these cli tools with cmake](https://github.com/nmeum/android-tools), which is what the official `nixpkgs` package of them uses.\n\nThere's a lot of CLI tools for dealing with android, but it turns out that there's a few critical ones for getting an emulator up and running, and having a better conceptual understanding of the magic behind android studio.\n\n![Our tools](https://static.404wolf.com/https://static.404wolf.com/Post-20240807233545196.webp)\n\nFirst things first. You need to set `ANDROID_SDK_ROOT` to the directory with the various binary cli tools we were just talking about (so like, if they're in your PATH, you can figure out where to set `ANDROID_SDK_ROOT`to by doing `which emulator`, where emulator is an example of one of the many android tools). Since I'm using `nix` , can easily keep things simple and automated with a devshell. Okay, now we want to\n\nThen we do something along the lines of\n\n```bash\navdmanager create avd -n my_avd -k \"system-images;android-30;google_apis;x86\" -d \"pixel\"\n```\n\nTo create an \"avd,\" which is basically a configuration file that specifies the details of some virtual android device that could be run as an emulator. AVD stands for \"android virtual device.\"\n\nWe can check to make sure that our avd shows up by doing\n\n```bash\navdmanager list avd\n```\n\nAnd then, to run it, use\n\n```bash\nemulator -avd my_avd\n```\n\nAnd then, if you're on `nix`, you'll be welcomed with a friendly message informing you that there's some dynamically linked dependency that you don't have, that you'll have to wrap with `LD_LIBRARY_PATH`. And then again.\n\nAll of these android CLI tools are very powerful, and have a gagillion options that you can fiddle with.\n\nI spent almost a week trying to get all the packages and settings fine tuned for `nix`, and ended up getting very close -- I had a bash script that would create an `avd`, create a virtual `sdcard` for \"external\" device storage, and then actually run the emulator. But for some reason I was getting a segfault, and it wouldn't launch, or give me any more useful error messages.\n\nAt that point I decided to do further research on android emulators on Nix, and found that Nix's standard library itself has already solved the problem.\n\nThere's [\"documentation\" here](https://nixos.wiki/wiki/Android) that talks about android development on Nix. Here, they talk about some helper functions that you can use to create android SDKs with the exact binaries that you need, nicely purely packaged with nix.\n\nWith the help of that guide and reading some source, I was able to scrap together this flake, which gets you most of the lower level CLI tools you need for android development. It's not entirely trivial, and not every tool is necessary for every task, but it's quite good.\n\n### A working flake\n\n```nix\n{\n  description = \"React native environment\";\n\n  inputs = {\n    nixpkgs.url = \"github:NixOS/nixpkgs\";\n    flake-utils.url = \"github:numtide/flake-utils\";\n  };\n\n  outputs = { self, nixpkgs, flake-utils, ... }@inputs:\n    flake-utils.lib.eachDefaultSystem (system:\n      let\n        pkgs = import nixpkgs {\n          inherit system;\n          config = {\n            allowUnfree = true;\n            android_sdk.accept_license = true;\n          };\n        };\n\n        pinnedJDK = pkgs.jdk17;\n        buildToolsVersion = \"34.0.0\";\n        ndkVersion = \"25.1.8937393\";\n        androidComposition = pkgs.androidenv.composeAndroidPackages {\n          cmdLineToolsVersion = \"8.0\";\n          toolsVersion = \"26.1.1\";\n          platformToolsVersion = \"34.0.4\";\n          buildToolsVersions = [ buildToolsVersion \"33.0.1\" ];\n          includeEmulator = false;\n          emulatorVersion = \"30.3.4\";\n          platformVersions = [ \"34\" ];\n          includeSources = false;\n          includeSystemImages = false;\n          systemImageTypes = [ \"google_apis_playstore\" ];\n          abiVersions = [ \"armeabi-v7a\" \"arm64-v8a\" ];\n          cmakeVersions = [ \"3.10.2\" \"3.22.1\" ];\n          includeNDK = true;\n          ndkVersions = [ ndkVersion ];\n          useGoogleAPIs = false;\n          useGoogleTVAddOns = false;\n          includeExtras = [ \"extras;google;gcm\" ];\n        };\n      in {\n        devShells.default = pkgs.mkShell rec {\n          packages = [\n            pkgs.android-tools\n            pkgs.nodejs\n            pkgs.corepack\n            pkgs.zulu17\n          ];\n\n          JAVA_HOME = pinnedJDK;\n          ANDROID_SDK_ROOT = \"${androidComposition.androidsdk}/libexec/android-sdk\";\n          ANDROID_NDK_ROOT = \"${ANDROID_SDK_ROOT}/ndk-bundle\";\n          GRADLE_OPTS = \"-Dorg.gradle.project.android.aapt2FromMavenOverride=${ANDROID_SDK_ROOT}/build-tools/${buildToolsVersion}/aapt2\";\n          shellHook = ''\n            export PATH=$PATH:${androidComposition.androidSdk}/bin\n            adb start-server\n            adb devices\n          '';\n        };\n      });\n}\n```\n\n### An emulator, in 4 lines of Nix\n\nAfter reading through all the source though, a super neat function caught my eye: [emulate app](https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/mobile/androidenv/emulate-app.nix). It wasn't documented anywhere, but it was super promising. Here's the entire source code, which is definitely worth reading over. It's very similar to what I was doing, but they were able to figure out how to get all the flags just right.\n\n```nix\n{ composeAndroidPackages, stdenv, lib, runtimeShell }:\n{ name\n, app ? null\n, platformVersion ? \"33\"\n, abiVersion ? \"armeabi-v7a\"\n, systemImageType ? \"default\"\n, enableGPU ? false # Enable GPU acceleration. It's deprecated, instead use `configOptions` below.\n, configOptions ? (\n    # List of options to add in config.ini\n    lib.optionalAttrs enableGPU\n      (lib.warn\n        \"enableGPU argument is deprecated and will be removed; use configOptions instead\"\n        { \"hw.gpu.enabled\" = \"yes\"; }\n      )\n  )\n, extraAVDFiles ? [ ]\n, package ? null\n, activity ? null\n, androidUserHome ? null\n, avdHomeDir ? null # Support old variable with non-standard naming!\n, androidAvdHome ? avdHomeDir\n, deviceName ? \"device\"\n, sdkExtraArgs ? { }\n, androidAvdFlags ? null\n, androidEmulatorFlags ? null\n}:\n\nlet\n  sdkArgs = {\n    includeEmulator = true;\n    includeSystemImages = true;\n  } // sdkExtraArgs // {\n    cmdLineToolsVersion = \"8.0\";\n    platformVersions = [ platformVersion ];\n    systemImageTypes = [ systemImageType ];\n    abiVersions = [ abiVersion ];\n  };\n\n  sdk = (composeAndroidPackages sdkArgs).androidsdk;\nin\nstdenv.mkDerivation {\n  inherit name;\n\n  buildCommand = ''\n    mkdir -p $out/bin\n\n    cat > $out/bin/run-test-emulator \u003C\u003C \"EOF\"\n    #!${runtimeShell} -e\n\n    # We need a TMPDIR\n    if [ \"$TMPDIR\" = \"\" ]\n    then\n        export TMPDIR=/tmp\n    fi\n\n    ${if androidUserHome == null then ''\n      # Store the virtual devices somewhere else, instead of polluting a user's HOME directory\n      export ANDROID_USER_HOME=$(mktemp -d $TMPDIR/nix-android-user-home-XXXX)\n    '' else ''\n      mkdir -p \"${androidUserHome}\"\n      export ANDROID_USER_HOME=\"${androidUserHome}\"\n    ''}\n\n    ${if androidAvdHome == null then ''\n      export ANDROID_AVD_HOME=$ANDROID_USER_HOME/avd\n    '' else ''\n      mkdir -p \"${androidAvdHome}\"\n      export ANDROID_AVD_HOME=\"${androidAvdHome}\"\n    ''}\n\n    # We need to specify the location of the Android SDK root folder\n    export ANDROID_SDK_ROOT=${sdk}/libexec/android-sdk\n\n    ${lib.optionalString (androidAvdFlags != null) ''\n      # If NIX_ANDROID_AVD_FLAGS is empty\n      if [[ -z \"$NIX_ANDROID_AVD_FLAGS\" ]]; then\n        NIX_ANDROID_AVD_FLAGS=\"${androidAvdFlags}\"\n      fi\n    ''}\n\n    ${lib.optionalString (androidEmulatorFlags != null) ''\n      # If NIX_ANDROID_EMULATOR_FLAGS is empty\n      if [[ -z \"$NIX_ANDROID_EMULATOR_FLAGS\" ]]; then\n        NIX_ANDROID_EMULATOR_FLAGS=\"${androidEmulatorFlags}\"\n      fi\n    ''}\n\n    # We have to look for a free TCP port\n\n    echo \"Looking for a free TCP port in range 5554-5584\" >&2\n\n    for i in $(seq 5554 2 5584)\n    do\n        if [ -z \"$(${sdk}/bin/adb devices | grep emulator-$i)\" ]\n        then\n            port=$i\n            break\n        fi\n    done\n\n    if [ -z \"$port\" ]\n    then\n        echo \"Unfortunately, the emulator port space is exhausted!\" >&2\n        exit 1\n    else\n        echo \"We have a free TCP port: $port\" >&2\n    fi\n\n    export ANDROID_SERIAL=\"emulator-$port\"\n\n    # Create a virtual android device for testing if it does not exist\n    if [ \"$(${sdk}/bin/avdmanager list avd | grep 'Name: ${deviceName}')\" = \"\" ]\n    then\n        # Create a virtual android device\n        yes \"\" | ${sdk}/bin/avdmanager create avd --force -n ${deviceName} -k \"system-images;android-${platformVersion};${systemImageType};${abiVersion}\" -p $ANDROID_AVD_HOME/${deviceName}.avd $NIX_ANDROID_AVD_FLAGS\n\n        ${builtins.concatStringsSep \"\\n\" (\n          lib.mapAttrsToList (configKey: configValue: ''\n            echo \"${configKey} = ${configValue}\" >> $ANDROID_AVD_HOME/${deviceName}.avd/config.ini\n          '') configOptions\n        )}\n\n        ${lib.concatMapStrings (extraAVDFile: ''\n          ln -sf ${extraAVDFile} $ANDROID_AVD_HOME/${deviceName}.avd\n        '') extraAVDFiles}\n    fi\n\n    # Launch the emulator\n    echo \"\\nLaunch the emulator\"\n    $ANDROID_SDK_ROOT/emulator/emulator -avd ${deviceName} -no-boot-anim -port $port $NIX_ANDROID_EMULATOR_FLAGS &\n\n    # Wait until the device has completely booted\n    echo \"Waiting until the emulator has booted the ${deviceName} and the package manager is ready...\" >&2\n\n    ${sdk}/libexec/android-sdk/platform-tools/adb -s emulator-$port wait-for-device\n\n    echo \"Device state has been reached\" >&2\n\n    while [ -z \"$(${sdk}/libexec/android-sdk/platform-tools/adb -s emulator-$port shell getprop dev.bootcomplete | grep 1)\" ]\n    do\n        sleep 5\n    done\n\n    echo \"dev.bootcomplete property is 1\" >&2\n\n    #while [ -z \"$(${sdk}/libexec/android-sdk/platform-tools/adb -s emulator-$port shell getprop sys.boot_completed | grep 1)\" ]\n    #do\n        #sleep 5\n    #done\n\n    #echo \"sys.boot_completed property is 1\" >&2\n\n    echo \"ready\" >&2\n\n    ${lib.optionalString (app != null) ''\n      # Install the App through the debugger, if it has not been installed yet\n\n      if [ -z \"${package}\" ] || [ \"$(${sdk}/libexec/android-sdk/platform-tools/adb -s emulator-$port shell pm list packages | grep package:${package})\" = \"\" ]\n      then\n          if [ -d \"${app}\" ]\n          then\n              appPath=\"$(echo ${app}/*.apk)\"\n          else\n              appPath=\"${app}\"\n          fi\n\n          ${sdk}/libexec/android-sdk/platform-tools/adb -s emulator-$port install \"$appPath\"\n      fi\n\n      # Start the application\n      ${lib.optionalString (package != null && activity != null) ''\n          ${sdk}/libexec/android-sdk/platform-tools/adb -s emulator-$port shell am start -a android.intent.action.MAIN -n ${package}/${activity}\n      ''}\n    ''}\n    EOF\n    chmod +x $out/bin/run-test-emulator\n  '';\n}\n```\n\nBasically, they generate a bash script that \"hard codes\" paths to the various packages (things like `${pkgs.hello}` here evaluate to `/nix/store/jfe...hash...jfei`), which are locations of specific, version locked dependencies.\n\nAnd it works! Using this function turns out to be extremely trivial. It's literally 4 lines of nix, since you don't need to build an android composition to use the function, even though you can.\n\nSo, here it is. Just a few lines of nix, and then `nix run`, and you have yourself a full on, working, ready to go android emulator, running on your computer.\n\n```nix\nemulator = pkgs.androidenv.emulateApp {\n    name = \"AndroidEmulator\";\n    platformVersion = \"30\";\n    abiVersion = \"x86_64\"; # armeabi-v7a, mips, x86_64\n    systemImageType = \"google_apis_playstore\";\n};\n```\n\nIt's so crazy cool, it just works!\n\nAfter all of the suffering to get a good working android emulator, and I had an idea: what if I could solve this problem for everyone, on every OS, everywhere, once and for all? What if I could offer android phones as a service, just using nix builds?\n\n### RobotNix\n\nOne of the massive pain points for Android development is actually building Android itself. Google provides scattered documentation that helps with this process, but it's very disparate, and just about everyone uses prebuilt images as a result.\n\nIn the process of tinkering, I [came across a project that packaged all google android images with `nix`](https://github.com/tadfisher/android-nixpkgs). It seemed pretty neat and promising here, since it would solve the issue of having images change over time in ways that break my setup because of differing dependencies.\n\n![Nixified android builds](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144625586.webp)\n\nI spent a long time trying to get an emulator working using the images it provided, and in the process I learned a ton about android cli tooling.\n\nTad Fisher's project that I just mentioned also packages android sdk tools too, and his are much finer grain.\n\n```nix\n android-sdk.packages = sdk: with sdk; [\n    build-tools-34-0-0\n    cmdline-tools-latest\n    emulator\n    platforms-android-34\n    sources-android-34\n];\n```\n\n### ADB\n\nOne of the most important android `cli` tools is `adb`, or `android debug bridge`. `adb` is a CLI tool (and has a socket interface) that is used to communicate with android devices.\n\nIt automatically can pick up on all android devices plugged into your computer over USB, running on the local network with wireless debugging enabled, and even emulators (which will be important later).\n\nIt's very powerful. It can do just about everything. To use `adb` for android development, usually you begin by running\n\n```bash\nadb start-server\n```\n\nWhich starts a server running on a socket on your computer that lets you use the `adb` cli client, or other community clients, to manipulate android devices.\n\nWhat isn't as commonly known is that there's also a `tcp/ip` mode for `adb` with `adb tcpip`. Once `adb` is running, you can then list out android devices with\n\n```bash\nadb devices\n```\n\nAnd start issuing commands. You can do things like\n\n- `adb shell` to enter a shell on the android phone itself.\n- `adb push` to push a file onto the device, and `adb pull` to yoink files off it\n- `adb (un)(in)stall` to un/install apks (android apps) to the device\n- `adb input tap x y` to tap the screen at a specific spot\n\nand a ton of other manipulation commands to do things like entering text input, capturing screenshots, recording audio, and more.\n\nIt's super powerful, and will play an important role in this project, as I'll discuss a bit later.\n\nWhat I've learned while researching these tools is that they are really horribly documented. [Google has docs](https://source.android.com/docs/setup/create/coding-tasks) that talk about the general processes, and, if you can figure out where to find things, you can get the source code for the CLI programs too. But, it seems like their main intent is to try to get people to use Android Studio as much as possible.\n\n## [Appetize.io](https://appetize.io)\n\n[Appetize.io](https://appetize.io) is a company that offers roughly just that. I found out about them later on when reviewing the `react native` documentation -- all the little phone widgets that say \"Expo\" on them are powered by `appetize.io` devices. They let you spin up android or IOS devices, and then embed them in webpages with decently low latency. They also expose an API, so that you can do CI/CD testing with proper phones.\n\n![React native docs](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144243428.webp)\n![It's expensive](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144458831.webp)\n![Appetize.io](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144317000.webp)\n![It \"just works\"](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144431237.webp)\n\nBut appetize comes at a cost. It's very expensive, and very restrictive. It isn't open source, and definitely isn't nixified.\n\nIt's worth mentioning them here before going further though, since what they do is genuinely really cool. Their main thing is automated testing, although they may also be able to do android development with the regular SDK tools too.\n\n# Ideation\n\nSo, I started brainstorming. My idea was to stream `nix` packaged android emulators to web browsers.\n\nOne other super cool thing that `nix` rocks at is containerizing things. Nix's standard library provides [utilities to create docker base images](https://nix.dev/tutorials/nixos/building-and-running-docker-images.html) that have all of the dependencies and transient dependencies of a nix derivation (build).\n\nThis means that you can implicitly refer to libraries in the function nix provides that creates docker images, and if they get mentioned then they get \"baked\" into the container. Part of what's so nice about dockering android emulators is that the processes live and die with the containers, since the qemu processes that the emulators create are kind of a pain to track and to kill.\n\nI am still working on getting them working in a docker container, but what I currently have, which is pretty close to what I expect to work, looks like this\n\n```nix\n{\n  pkgs,\n  android-tools,\n  system,\n  ...\n}: let\n  run-android-emulator = import ./emulator.nix {inherit pkgs;};\n  android-sdk = import ./android-sdk.nix {inherit pkgs system android-tools;};\nin\n  pkgs.dockerTools.buildImage {\n    name = \"android-emulator\";\n    tag = \"latest\";\n\n    copyToRoot = pkgs.buildEnv {\n      name = \"root\";\n      pathsToLink = [\"/bin\"];\n      paths = [\n        pkgs.jdk\n        pkgs.coreutils\n        pkgs.bash\n        pkgs.busybox\n        pkgs.bun\n        android-sdk\n      ];\n    };\n\n    config = {\n      Env = [\n        \"JAVA_HOME=${pkgs.jdk}\"\n      ];\n      # has the script that uses the emulate-app function\n      Cmd = [\"./${run-android-emulator}/bin/android-emulator\"];\n    };\n\n    extraCommands = ''\n      mkdir -p tmp\n    '';\n  }\n```\n\nWhich does basically exactly what I just explained -- although we do need some dependencies that I didn't reference directly off of a nix object, like coreutils, and cli things like grep.\n\n## The browser?\n\nMy goal was to have the easiest, most streamlined devex possible for the project. So, ideally the final product for the end user could just be a react component, or even an `iframe` eventually. Right now, the interface is this.\n\n```typescript\nexport default function App() {\n  return \u003CAndroid ip={[\"ws://127.0.0.1:8188\"]} apiSecret=\"secret\" />\n}\n```\n\n### Streaming\n\nThe first step is to stream android at as low latency as possible out of the phone. As close to literal 0 as possible (so in reality, ideally \\\u003C300ms). Doing this, however, is actually pretty nontrivial.\n\nThe first step is figuring out how we can stream the screen out of the android device. Obviously this is possible, because when you run android emulators (without the `-no-window` flag) you get a window that shows the screen of the android device and is interactive. Android studio also natively provides this feature.\n\n`adb`, which we talked about earlier, does do this out of the box with a built in android utility called screenrecord. More information about that can be found [here](https://android.stackexchange.com/questions/7686/is-there-a-way-to-see-the-devices-screen-live-on-pc-through-adb).\n\nTo use it, it looks something like this...\n\n```bash\nsudo apt-get install adb ffmpeg\nadb exec-out screenrecord --output-format=h264 - |\n   ffplay -framerate 60 -probesize 32 -sync video  -\n```\n\nIt does work, but unfortunately the latency is pretty high. My tests got about 6 seconds of latency, although this stackoverflow post's video seems to have gotten about a second or less with some `ffmpeg` tuning. I may return to this later, but for now I am using what I believe to be a better alternative, [`scrcpy`](https://github.com/Genymobile/scrcpy).\n\n#### Scrcpy\n\n![Scrcpy](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144813092.webp)\n![Scrpy in action](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144914137.webp)\n\nScrcpy (\"screen copy\") is a 3rd party utility that lets you stream an android device to a window at extremely low latency (30ms or less!). It \"just works,\" and is available on `nixpkgs` (nix package manager). Using it is literally as simple as plugging in an android device, or running an emulator (headless or headed, it doesn't care), and then running\n\n```bash\nscrcpy\n```\n\nAnd a window will open up on your computer with `scrcpy`, which has a live stream of the screen of the android phone. The window it opens up is interactive and is very responsive, and audio works out of the box too.\n\nThe client (the window it opens up) has support for more advanced things too.\n\nSo, they seem to have solved the issue of `adb` having bad latency, but how?\n\nTo figure out how scrcpy works behind the scenes, they lay out a general schematic in [their docs](https://github.com/Genymobile/scrcpy/blob/master/doc/develop.mdhttps://github.com/Genymobile/scrcpy/blob/master/doc/develop.md) for contributors. I'm not going to reiterate everything they say there, but there are a few important takeaways if I want to use scrcpy's system of streaming at low latency.\n\nThe general idea of how `scrcpy` works is that you run it, and it ships a `apk` (android app) to the phone in its `/tmp` directory (which android phones have, they are also `unix`!). This `apk` exposes a server on the android phone on a `scrcpy` port on the phone, which then the `scrcpy` client can access and send data through.\n\n![Scrcpy server with audio](https://static.404wolf.com/https://static.404wolf.com/Post-20240808142749751.webp)\n\nThis `scrcpy` server that runs on the phone is implemented in `java` (which most android apps are), and acts as just a server running in the background.\n\nIt turns out that `scrcpy` streams out data over three \"channels,\" where the first connection gets a connection to stream video, followed by audio, and finally data (interactions like gestures, which scrcpy also handles itself with a super low-latency custom binary interface).\n\nTheir client itself is very complex, and is implemented in raw C and uses some very advanced frameworks to optimize for very high performance. That's less relevant here.\n\n#### Hijacking Scrcpy\n\n![Standalone scrcpy sever](https://static.404wolf.com/https://static.404wolf.com/Post-20240808142957377.webp)\n\nKnowing that scrcpy is just using a server running on the phone was enticing though.\n\nAt this point, I really just wanted to pipe the video into `ffmpeg` so that I could do things with it, since I still didn't know how I would stream it, but I knew that pretty much no matter what `ffmpeg` should be able to do the necessary forwarding.\n\nI did a bit of googling, and it looks like it is possible to do. I found a github issue, with a link to a VLC (the video viewing program) PR that fixes a latency issue having to do with how VLC throttles video stream outputs.\n\n![Piping Scrcpy to FFMPEG](https://static.404wolf.com/Post-20240807145206166.webp)\n\nI was able to follow their steps, and the main pain point was getting `adb` to forward a tcp port.\n\nTo use `scrcpy` to stream video output, you need to put and then start `scrcpy` on the phone, and then remap the port on the phone that scrcpy is using to a different port on your computer.\n\nThey provide an example script that shows how to do this, which is this\n\n```bash\nadb push scrcpy-server-v2.1 /data/local/tmp/scrcpy-server-manual.jar\nadb forward tcp:1234 localabstract:scrcpy\nadb shell CLASSPATH=/data/local/tmp/scrcpy-server-manual.jar \\\n    app_process / com.genymobile.scrcpy.Server 2.1 \\\n    tunnel_forward=true audio=false control=false cleanup=false \\\n    raw_stream=true max_size=1920\n```\n\nIt's not that bad. Another important thing here is that you need to set `raw_stream` to `false`, since usually `scrcpy` sends some metadata at the start of streams, which could stop `ffmpeg` from correctly interpreting the stream.\n\nI found [this great medium post](https://pencilflip.medium.com/how-to-use-adb-forward-6a8cbfa04907) that talks about how `adb forward` works, since it is horribly documented. They mention [SERVICES.txt](https://cs.android.com/android/platform/superproject/+/master:packages/modules/adb/SERVICES.TXT), which has even more helpful docs.\n\nI didn't end up needing to fiddle around much with the default command that they say to use to forward the port, but I like having an understanding of how it works.\n\nSo, at this point I have a working script that can copy `scrcpy` over to an android phone, run it on the phone, and then stream out the screen.\n\nNow, what's actually coming out of the port? Raw h264 video. Great! Viewing the screen using the `vlc` command they say to try **does** work, but it is very very laggy just like they say.\n\n## WebRTC?\n\nOkay, I now (kinda) have a working stream coming out of the android device. This is pretty sick. Now comes the hard part... how do I take this output stream and get it to a web browser, with as low latency as possible.\n\n### Streaming to a browser\n\n![Enter WebRTC](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145608512.webp)\n\nI did some research here, and there's a ton of different streaming protocols that browsers support. A google search reveals many, like\n\n- HLS\n- WebRTC\n- Real-time Messaging Protocol (Rtmp)\n- Secure Reliable Transport (Srt)\n- Real-time Streaming Protocol (Rtsp)\n- Microsoft Smooth Streaming\n- RTMP\n- RTSP\n\nHLS is really nice, it's just relaying video live over http, and you can access the media directly through a `\u003Cvideo>` element. Also, `ffmpeg` supports streaming out `\u003Cvideo>` out of the box, which is nice. Unfortunately though, it adds a lot of latency. To achieve the latency I want in this case, I really don't have any option other than WebRTC.\n\nWebRTC is a peer to peer browser streaming protocol. It's pretty simple to use since browsers have a good unified API for it, where you essentially have a signaling websocket server that tells clients when and whom to connect to, and then they can stream things to each other from the browser itself, like video camera output.\n\nThat's great, but in my case I am doing a very centralized broadcast. I have media streaming off of a server, and want to ship that media to browsers directly. This is something that WebRTC isn't really designed for.\n\n![Janus Gateway](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145641902.webp)\n![Janus (the god)](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145717463.webp)\n\nThere's a few implementations of the WebRTC protocol, like Google's official c++ one, or a community rewrite in rust (of course). To use them, there are projects like [node webrtc](https://github.com/node-webrtc/node-webrtc), which expose a javascript API to set up webrtc connections. The issue here is that I would have to implement a lot of the handshake process myself, even if I didn't need to actual manage the packets and connection stuff myself, which is a lot of work that I rather not get caught up in.\n\n![Ugh](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145750847.webp)\n\nIt turns out that I'm not the first person who has wanted extremely low latency server based WebRTC, and there's tools that do roughly exactly what I was trying to do. There's a super cool project called [Janus Gateway](https://github.com/meetecho/janus-gateway) that is designed to be a centralized WebRTC server.\n\n![Nix to the rescue!](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145822398.webp)\n\nIt can do a ton of different things, and is a very large C project. The build process is a huge pain in the neck, but thankfully it was already packaged with `nix`, so I didn't need to deal with it.\n\nThe important thing to know about Janus Gateway is that it has a whole bunch of plugins to do common WebRtc media server tasks.\n\nThere are [a whole bunch](https://janus.conf.meetecho.com/docs/pluginslist.html) documented on their website here. Some fun ones:\n\n- **VideoCall plugin**: \"Peer to peer\" calling each other, but the data flows through Janus gateway\n- **Sip Plugin** is a plugin that abstracts authing into a SIP server and then lets you do SIP calls through the browser over WebRTC\n- **VideoRoom** is a plugin meant for many to many streaming, so like, conference calls\n- **TextRoom**, which is a plugin that deals only with WebRTC's data channel to stream raw data at very low latency over WebRTC.\n\n![Jangouts](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145842880.webp)\n\nThere's a really neat project called [Jangouts](https://github.com/jangouts/jangouts) that lets you do google hangouts style conference calling through an open source server running Janus Gateway, which is a neat showcase of the capabilities of the plugins.\n\nFinally, there's an API to create plugins for Janus Gateway in `C`, `lua`, and even `javascript`, if none of the preexisting plugins work. I think this is really neat and may return to trying out their API, or once I learn `C` write a native plugin, but for now I found what I needed: the `streaming` plugin.\n\nThe `streaming` Janus Gateway plugin lets you read prerecorded media or access prerecorded media and broadcast it over WebRTC, but, more importantly, it also lets you pass in media over [RTC](https://en.wikipedia.org/wiki/Real-time_Transport_Protocol). Here's how they word it\n\n> the plugin is configured to listen on a few ports for RTP: this means that the plugin is implemented to receive RTP on those ports and relay them to all peers attached to that stream. Any tool that can generate audio/video RTP streams and specify a destination is good for the purpose: the examples section contains samples that make use of GStreamer (http://gstreamer.freedesktop.org/) but other tools like FFmpeg (http://www.ffmpeg.org/), LibAV (http://libav.org/) or others are fine as well. This makes it really easy to capture and encode whatever you want using your favourite tool, and then have it transparently broadcasted via WebRTC using Janus. Notice that we recently added the possibility to also add a datachannel track to an RTP streaming mountpoint: this allows you to send, via UDP, a text-based message to relay via datachannels (e.g., the title of the current song, if this is a radio streaming channel). When using this feature, though, beware that you'll have to stay within the boundaries of the MTU, as each message will have to stay within the size of an UDP packet.\n\nLiterally just what I want! How convenient. But now, we have to configure it.\n\nTo say it briefly, Janus is a pain in the neck to configure. It's a monster of a project and there are a billion different options that all need to be configured correctly.\n\n## Janus Gateway\n\nWe talked about this a bit earlier, but Janus is a all in one WebRTC, so that means that it both is a \"peer\" that is sharing media, but also is the signaling server itself (the thing that tells clients to start getting content and from who).\n\nIt does the signaling through an API that it exposes, which can either be in the form of Websockets or HTTP (or both!). It also exposes a \"admin api\" that you can use to query metadata about `janus`, like\n\n### Configuring\n\nFirst things first, Janus has its own DSL for configuring it. It's not that bad, it's mostly key pair stuff.\n\nJanus provides a nice set of example configurations that have a ton of comments [on their github](https://github.com/meetecho/janus-gateway/blob/master/conf/janus.jcfg.sample.in). It's very helpful, and I would have been totally lost (well, more totally lost than the amount of totally lost that I was) had they not had example configurations.\n\nThere's a few different things we have to configure:\n\n- The signaling server that `Janus` hosts and general configuration\n- The streaming plugin itself\n- The admin server, which is a separate server for managing janus itself (getting information like what ports things are running on, statuses, etc)\n\nThere were a few manual things I had to set up for the configuration. First, I explicitly enabled the streaming plugin. I'm not entirely sure if this is necessary, but I believe it would at least give me an error if it couldn't find it, which is good.\n\n```janus\nplugins: {\n  enable = \"janus.plugin.streaming\"\n}\n```\n\nSecond, I set up an admin secret, but `janus` does have support for token auth and eventually I'll figure out how to use that. For auth, it's not well documented, but to pass the token when accessing you literally just add a `apisecret: \"secret\"` header to the request.\n\n```janus\nadmin_secret = \"secret\"\t# String that all Janus requests must contain\n```\n\nThere's also a configuration section called `media`, which seems to be where you put global media configuration settings.\n\nI'll be honest in saying I don't totally understand every setting here, but I did a lot of trial and error to figure things out. It seems pretty important to do `nack_optimizations` for this project, which significantly helped with stuttering.\n\n```janus\nmedia: {\n\t#ipv6 = true\n\t#ipv6_linklocal = true\n\tmin_nack_queue = 1200\n\trtp_port_range = \"20000-40000\"\n\tdtls_mtu = 1500\n\tno_media_timer = 2\n\tslowlink_threshold = 4\n\ttwcc_period = 100\n\tdtls_timeout = 500\n\n\t# Janus can do some optimizations on the NACK queue, specifically when\n\t# keyframes are involved. Namely, you can configure Janus so that any\n\t# time a keyframe is sent to a user, the NACK buffer for that connection\n\t# is emptied. This allows Janus to ignore NACK requests for packets\n\t# sent shortly before the keyframe was sent, since it can be assumed\n\t# that the keyframe will restore a complete working image for the user\n\t# anyway (which is the main reason why video retransmissions are typically\n\t# required). While this optimization is known to work fine in most cases,\n\t# it can backfire in some edge cases, and so is disabled by default.\n\tnack_optimizations = true\n\n\t# If you need DSCP packet marking and prioritization, you can configure\n\t# the 'dscp' property to a specific values, and Janus will try to\n\t# set it on all outgoing packets using libnice. Normally, the specs\n\t# suggest to use different values depending on whether audio, video\n\t# or data are used, but since all PeerConnections in Janus are bundled,\n\t# we can only use one. You can refer to this document for more info:\n\t# https://tools.ietf.org/html/draft-ietf-tsvwg-rtcweb-qos-18#page-6\n\t# That said, DON'T TOUCH THIS IF YOU DON'T KNOW WHAT IT MEANS!\n\tdscp = 46\n}\n```\n\n![Streaming plugin](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145901202.webp)\n\nGetting the streaming plugin working at first seemed really annoying since the plugins are all `C` files that have to be built and then placed in a specific folder on your system, which is not that portable. However, analyzing the `janus` cli (the thing you use to launch `janus gateway`), I figured out that you can set flags to specify where to look things up. Also, the `nix` build of `janus` comes with all the plugins!\n\n```bash\n$JANUS \\\n    -P \"$JANUS_INSTALL\" \\\n    -F \"$JANUS_CONFIG_DIR\" \\\n    -C \"$JANUS_CONFIG\"\n```\n\nNow these official names are really confusing. TLDR: `$JANUS_INSTALL` is the directory where the `janus` binary and plugin binaries live, `$JANUS_CONFIG_DIR` is the location where plugin configuration files go, and `$JANUS_CONFIG` is where the general configuration file goes.\n\nNow, where is Janus installed? Well, I've installed it with nix, so the location it's installed will look something like `/nix/store/jfieao...hash....fjeioa/bin/janus`, which could change and is bad practice to directly reference. Eventually, I'll generate a shell script using `pkgs.writeShellScriptBin`, where I can then reference Janus's root path in the generator for the shell script as `pkgs.janus`. To get started though, I did just do it in a janky way with `which`. I'll clean it up eventually.\n\nThe directory where `janus` lives looks like\n\n```\n/nix/store/46c284cqdgia0dxzmi8rs5vzwszxalwg-janus-gateway-1.2.3\n bin\n  janus\n  janus-cfgconv\n  janus-pp-rec\n  mjr2pcap\n lib\n     janus\n         events\n          libjanus_gelfevh.la\n          libjanus_gelfevh.so -> libjanus_gelfevh.so.2.0.3\n          libjanus_gelfevh.so.2 -> libjanus_gelfevh.so.2.0.3\n          libjanus_gelfevh.so.2.0.3\n          libjanus_sampleevh.la\n          libjanus_sampleevh.so -> libjanus_sampleevh.so.2.0.3\n          libjanus_sampleevh.so.2 -> libjanus_sampleevh.so.2.0.3\n          libjanus_sampleevh.so.2.0.3\n          libjanus_wsevh.la\n          libjanus_wsevh.so -> libjanus_wsevh.so.2.0.3\n          libjanus_wsevh.so.2 -> libjanus_wsevh.so.2.0.3\n          libjanus_wsevh.so.2.0.3\n         loggers\n          libjanus_jsonlog.la\n          libjanus_jsonlog.so -> libjanus_jsonlog.so.2.0.3\n          libjanus_jsonlog.so.2 -> libjanus_jsonlog.so.2.0.3\n          libjanus_jsonlog.so.2.0.3\n         plugins\n          libjanus_audiobridge.la\n          libjanus_audiobridge.so -> libjanus_audiobridge.so.2.0.3\n          libjanus_audiobridge.so.2 -> libjanus_audiobridge.so.2.0.3\n          libjanus_audiobridge.so.2.0.3\n          libjanus_echotest.la\n          libjanus_echotest.so -> libjanus_echotest.so.2.0.3\n          libjanus_echotest.so.2 -> libjanus_echotest.so.2.0.3\n          libjanus_echotest.so.2.0.3\n          libjanus_nosip.la\n          libjanus_nosip.so -> libjanus_nosip.so.2.0.3\n          libjanus_nosip.so.2 -> libjanus_nosip.so.2.0.3\n          libjanus_nosip.so.2.0.3\n          libjanus_recordplay.la\n          libjanus_recordplay.so -> libjanus_recordplay.so.2.0.3\n          libjanus_recordplay.so.2 -> libjanus_recordplay.so.2.0.3\n          libjanus_recordplay.so.2.0.3\n          libjanus_sip.la\n          libjanus_sip.so -> libjanus_sip.so.2.0.3\n          libjanus_sip.so.2 -> libjanus_sip.so.2.0.3\n          libjanus_sip.so.2.0.3\n          libjanus_streaming.la\n          libjanus_streaming.so -> libjanus_streaming.so.2.0.3\n          libjanus_streaming.so.2 -> libjanus_streaming.so.2.0.3\n          libjanus_streaming.so.2.0.3\n          libjanus_textroom.la\n          libjanus_textroom.so -> libjanus_textroom.so.2.0.3\n          libjanus_textroom.so.2 -> libjanus_textroom.so.2.0.3\n          libjanus_textroom.so.2.0.3\n          libjanus_videocall.la\n          libjanus_videocall.so -> libjanus_videocall.so.2.0.3\n          libjanus_videocall.so.2 -> libjanus_videocall.so.2.0.3\n          libjanus_videocall.so.2.0.3\n          libjanus_videoroom.la\n          libjanus_videoroom.so -> libjanus_videoroom.so.2.0.3\n          libjanus_videoroom.so.2 -> libjanus_videoroom.so.2.0.3\n          libjanus_videoroom.so.2.0.3\n         transports\n             libjanus_http.la\n             libjanus_http.so -> libjanus_http.so.2.0.3\n             libjanus_http.so.2 -> libjanus_http.so.2.0.3\n             libjanus_http.so.2.0.3\n             libjanus_pfunix.la\n             libjanus_pfunix.so -> libjanus_pfunix.so.2.0.3\n             libjanus_pfunix.so.2 -> libjanus_pfunix.so.2.0.3\n             libjanus_pfunix.so.2.0.3\n             libjanus_websockets.la\n             libjanus_websockets.so -> libjanus_websockets.so.2.0.3\n             libjanus_websockets.so.2 -> libjanus_websockets.so.2.0.3\n             libjanus_websockets.so.2.0.3\n```\n\nSo to reference the plugins in the correct places I can write a bash script that uses `janus` like this\n\n```bash\necho \"Starting janus in $PWD\"\nCONFIGS=./src/janus/configs\nJANUS_INSTALL=$(dirname \"$(dirname \"$(which janus)\")\")\necho \"$JANUS_INSTALL\"\njanus -P \"$JANUS_INSTALL/lib/janus/plugins\" -F \"$CONFIGS\" -C \"./src/janus/janus.jcfg\"\n```\n\n`dirname` just gets the directory something is located in, so I'm effectively hopping two directories up.\n\nOnce I was able to load the `streaming` plugin, I then had to configure it. They have this [handy example](https://github.com/meetecho/janus-gateway/blob/master/conf/janus.plugin.streaming.jcfg.sample.in) of various streaming plugin setups for different use cases.\n\nThe first example that called out to me is this one:\n\n```\n# This is an example of an RTP source stream, which is what you'll need\n# in the vast majority of cases: here, the Streaming plugin will bind to\n# some ports, and expect media to be sent by an external source (e.g.,\n# FFmpeg or Gstreamer). This sample listens on 5002 for audio (Opus) and\n# 5004 for video (VP8), which is what the sample gstreamer script in the\n# plugins/streams folder sends to. Whatever is sent to those ports will\n# be the source of a WebRTC broadcast users can subscribe to.\n#\nrtp-sample: {\n\ttype = \"rtp\"\n\tid = 1\n\tdescription = \"Opus/VP8 live stream coming from external source\"\n\tmetadata = \"You can use this metadata section to put any info you want!\"\n\taudio = true\n\tvideo = true\n\taudioport = 5002\n\taudiopt = 111\n\taudiocodec = \"opus\"\n\tvideoport = 5004\n\tvideopt = 100\n\tvideocodec = \"vp8\"\n\tsecret = \"adminpwd\"\n}\n```\n\nSince it claims to do exactly what I want: stream media over RTP.\n\nOkay, so now I just place all the `janus` configurations into the right spots, run `janus` using my hacky `bash` script, and pray it works.\n\nIt didn't at first, or for the first day of trying to get it to, but eventually I got it to a functional state where janus would at least run.\n\nNow what? I'm not streaming media, but now I _can_ stream through RTP on localhost.\n\nThe next step was to figure out how to **connect** to `janus`, which is totally a pain in the neck and nontrivial.\n\n## Browser again\n\nModern browsers are designed to support WebRTC connections. They provide an API that you can use to create `PeerConnections`, do handshaking, set up media streams, and all that fun stuff.\n\nIt looks something like this (signalingServer is a websocket server)\n\nNote that this example is ai-written/modified, since it's a very minimal short example of what you'd do to get webrtc working.\n\nICE is a network technique to establish peer to peer connections while going through some central server (google hosts a commonly used one).\n\n```typescript\n// 1. Create peer connection\nconst pc = new RTCPeerConnection();\n\n// 2. Create and set local description\nconst offer = await pc.createOffer();\nawait pc.setLocalDescription(offer);\n\n// 3. Send offer to remote peer (via signaling server)\nsignalingServer.send(JSON.stringify(offer));\n\n// 4. Receive answer from remote peer\nsignalingServer.onmessage = async (event) => {\n  const answer = JSON.parse(event.data);\n  await pc.setRemoteDescription(answer);\n};\n\n// 5. Exchange ICE candidates\npc.onicecandidate = (event) => {\n  if (event.candidate) {\n    signalingServer.send(JSON.stringify(event.candidate));\n  }\n};\n\n// 6. Handle incoming ICE candidates\nsignalingServer.onmessage = async (event) => {\n  const iceCandidate = JSON.parse(event.data);\n  await pc.addIceCandidate(iceCandidate);\n};\n\n// 7. Handle getting streams\npc.ontrack = (event) => {\n  const [remoteStream] = event.streams;\n  console.log(\"Received remote stream\", remoteStream);\n  // Use the remoteStream, e.g., attach it to a video element\n  const videoElement = document.querySelector(\"#remoteVideo\");\n  videoElement.srcObject = remoteStream;\n};\n\n// 8. Connection established\npc.onconnectionstatechange = (event) => {\n  if (pc.connectionState === \"connected\") {\n    console.log(\"WebRTC connection established!\");\n  }\n};\n```\n\nIn this case though, I didn't actually create the websocket server, and `janus`'s is much more complicated than this. It isn't a matter of just accepting the first message received, but rather `janus` requires you actually have a \"conversation\" and tell it what you want -- you ask for a list of streams, choose one by id, etc, all while sending heartbeats. It's annoying to work with, but they provide a [`javascript` sdk](https://www.npmjs.com/package/janus-gateway) with type declarations.\n\n### A Little React\n\nTo procrastinate figuring out how to use their sdk, I started off by setting up a super simple `vite` `react` app so that I could nicely abstract things.\n\nI learned a bit about `vite` here, since I hadn't used it before. It seems like vite's entrypoint is an `index.html` file that looks something like\n\n```html\n\u003Cbody>\n  \u003Cdiv id=\"root\">\u003C/div>\n  \u003Cscript type=\"module\" src=\"/src/main.tsx\">\u003C/script>\n\u003C/body>\n```\n\nWhere vite hosts a server, and then automatically intercepts requests for `/src/main.tsx` and serves a `javascript` bundle (which it can prebuild or dynamically generate).\n\nI found this [random helpful example](https://github.com/LorisGiann/simple-janus-streaming-client/blob/master/streamingtest.js) usage that was pretty helpful for figuring out how to interface with `janus`.\n\nReact [hooks](https://react.dev/reference/react/hooks) are ways to move logic away from your components so that you can reuse logic. The difference between hooks and regular functions is that you can use `hooks` within `hooks`, so, like, hooks can call `useState` to maintain their own states.\n\nThe way you define a hook is by placing it in a file with the `use` prefix, and then provide a function as a default export. So like, in my case, `useJanusStream.ts` and `export default function useJanusStream`.\n\nIn that function body we can use hooks like `useState`. Okay, so let's start writing the logic for connecting to `janus` and getting a stream into a `\u003Cvideo>` element\n\n[There are docs](https://janus.conf.meetecho.com/docs/JS.html) on their javascript sdk, but it's kinda awful to work with. It sends all the right `api` requests and works well enough, but it was implemented before `const` and `async/await`, so it's full of callbacks and is awful to deal with.\n\nTo start, we `init` `Janus`, which is already kinda yucky -- we're setting a global state of how `Janus` is to behave.\n\n```typescript\nJanus.init({\n  debug: true,\n  dependencies: Janus.useDefaultDependencies({ adapter }),\n  callback: () => {\n```\n\nWe get the `adapter` with `import adapter from \"webrtc-adapter\"`, which is from [here](https://www.npmjs.com/package/webrtc-adapter). It's a common package that exposes the `WebRTC` api in a browser agnostic way.\n\nOkay, now we instantiate a new Janus (yes, init just set a global config state, it didn't actually create a connection or anything like that)\n\n```typescript\nconst janus = new Janus({\n    server: servers, // a list of websocket/http server IPs (of janus servers)\n    apisecret: \"secret\",\n    success: () => {\n```\n\nAnd, once the Janus gets created we handle the success with\n\n```typescript\njanus.attach({\n    plugin: \"janus.plugin.streaming\",\n```\n\nThis attaches the streaming plugin, which makes an api request that asserts that there is a streaming plugin running on the `janus` server, and then (as usual) has a callback for once the assertion is done...\n\n```typescript\nsuccess: (receivedStreamingPluginHandle) => {\n  console.debug(\"Got streaming plugin information\");\n  streamingPluginHandle = receivedStreamingPluginHandle;\n  console.debug(\"Requesting stream from plugin\");\n  streamingPluginHandle.send({\n    message: { request: \"list\" },\n    success: (list: any) => {\n      console.debug(\"Listed!\", list);\n    },\n  });\n  streamingPluginHandle.send({\n    message: { request: \"info\", id: 1 },\n    success: (info: any) => {\n      console.debug(\"Got info\", info);\n    },\n  });\n  streamingPluginHandle.send({\n    message: { request: \"watch\", id: 1 },\n    success: (resp: any) => {\n      console.debug(\"Resp\", resp);\n      console.debug(\n        \"Watching stream success. Now waiting to start stream.\",\n      );\n    },\n  });\n},\n```\n\nOnce it is ready, we ask for a list of streams (to log, for debugging purposes for now), we get information on the stream that should be the one we are going to connect to, and then we ask to watch the stream.\n\nThat last step is the tricky part -- in order to obtain a `MediaStreamTrack`, which is just raw video/audio, which we can create a `MediaStream` from, which can be slotted into a `\u003Cvideo>` element, we need to ask `janus` to send us the stream, and define a callback for once it's done so.\n\nBefore it can send the stream, it'll ask to do an auth handshake, which\n\n```typescript\nonmessage: (msg, jsep) => {\n  console.debug(\"Received msg from Janus server\", msg, jsep);\n  if (streamingPluginHandle === null) return;\n  if (jsep === undefined) return;\n  console.debug(\"Received JSEP!\", jsep);\n  console.debug(\"Answering the JSEP request.\");\n  streamingPluginHandle.createAnswer({\n    jsep: jsep,\n    media: { audioSend: false, videoSend: false },\n    success: (jsep: any) => {\n      console.debug(\"Successful SDP answer created\");\n      let body = { request: \"start\" };\n      streamingPluginHandle.send({ message: body, jsep: jsep });\n    },\n    error: (error: any) => {\n      console.error(\"WebRTC Error:\", error);\n    },\n  });\n},\n```\n\nJSEP's a complicated handshake that goes on to establish a `webrtc` connection. It's mostly abstracted away from us.\n\nOnce the handshake is done, we just define a `onRemoteTrack` function for when the `MediaTrack` is ready.\n\n```typescript\nonremotetrack: (track: MediaStreamTrack) =>\n    onReceivedMediaTrack(track),\n```\n\nThen we have a `MediaStreamTrack`, which is a \"container\" of sorts for the inbound video stream. We can attach it to our `\u003Cvideo>` element by creating a `MediaStream` with it. A `MediaStream` is a collection of tracks -- so, like, audio and video, for example.\n\nIt looks something like this...\n\n```typescript\nconst setupStream = () => {\n  if (videoPlayer.current && mediaStreamTrack.readyState === \"live\") {\n    const newMediaStream = new MediaStream();\n    newMediaStream.addTrack(mediaStreamTrack);\n    videoPlayer.current.srcObject = newMediaStream;\n  }\n};\n```\n\n`MediaStreamTracks` have a enum for their readyState, so if it's \"live\" (video is streaming), then create a `MediaStream` with the track, and then set our video element (which we can grab with a selector, or, in this case, a `useRef`)'s srcObject (what it's playing) to the track we just received.\n\nOkay, so I implemented all of this, and, to put it briefly, it did connect, but no video would play.\n\n### Video Encoding Transcoding\n\nThe next major obstacle\n\n![Your tab crashed!](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145930075.webp)\n\n![NALUs](https://static.404wolf.com/https://static.404wolf.com/Post-20240807145949240.webp)\n\n# Other Cool Things\n\n![RobotNix](https://static.404wolf.com/https://static.404wolf.com/Post-20240807144559741.webp)","src/posts/AndroidInTheBrowser.mdx","ab61832ee7916eb9","cwrufreefoodfinder",{"id":39,"data":41,"body":47,"filePath":48,"digest":49,"deferredRender":26},{"title":42,"type":15,"date":16,"covers":43,"tags":45,"description":46},"CWRU Free Food Finder",[44],"https://static.404wolf.com/Screenshot_from_2024-03-28_23-28-45_0001.png",[20,21],"For Case Western's 2024 hackathon two friends and I built a webapp for locating free food on our campus. We used a fine tuned OpenAI model to read all events from our university's event management system and determined whether they had food, of what type, and to generate other metadata. We then displayed this data on a appealing MUI-React UI, and set up a deployment at free-cwru-food.404wolf.com! We added various additional features like a calendar integration, and have continued to improve the site.\n","# [The Website](https://free-cwru-food.404wolf.com)\n\n![Main website](Screenshot_from_2024-03-25_13-20-22_0001.png|float=none|width=80)\n\nFor Case Western's 2024 hackathon two friends (Mars and Mark) and I got together and built a webapp for locating free food on campus. We used a fine tuned OpenAI model to read all events on our universities event management service page and determine whether they had food, of what type, and to generate other metadata. We then displayed this data on a appealing MUI-React UI, which can be found at **[free-cwru-food.404wolf.com](https://free-cwru-food.404wolf.com)**. The github is located [here](https://github.com/404Wolf/Food-Finder).\n\nUpdate (2/2025): CampusGroups seems to have made a change that broke our web scraping logic. This will be fixed but currently the website will not show many or any events.\n\n## Inspiration\n\nThough we were not supposed to begin working on our hackathon project before the official start time, we did decide to meet to brainstorm for the project a bit ahead of time. The two friends I teamed up with had previously worked on a separate hackathon at WashU to create an app for automatically registering for classes as spots opened up. Taking inspiration, we also wanted to work on a project that interfaced with some campus service to make it better and easier to use, but weren't sure which. We knew it'd need to be something that had an obvious and applicable use case, and something vague enough that we could make it more prompt-specific when the hackathon were to actually start.\n\n![CampusGroups events](https://static.404wolf.com/Screenshot_from_2024-03-28_23-28-45_0001.png)\n\nAs we brainstormed, we came up with a list of a few campus services that we could extend. Canvas is our school's portal for classes -- course announcements, grading, etc. We thought of adding a \"Submit to Canvas\" button Google Docs extension, or a \"scheduled submit\" option for Cavnas. I proposed the idea of making a syllabus scanner to find easy important details on each classes' syllabus. Eventually, we decided on creating some sort of analysis tool for our school's event management system CampusGroups. CampusGroups is a great platform for garnering all events happening on campus, but it can be a bit intimidating and confusing to filter through all the events.\n\nWe thought that, since we'd be heavily constrained on time, we would start with something more simple and constrained, which is when I came up with the specific idea of filtering for events that had food. Funny as it may be, free food is a big motivator for college students everywhere, including Case. Wouldn't it be great to have a website that analyzes the descriptions of all events on campus and screens for the ones that have free food? So that's just what we did.\n\n# Making it\n\n## Prompting\n\nFor the actual event's prompt, it was an AI based theme. The track we participated in was the \"maker\" track. We thought that our choice to use AI as an analysis tool would fit well with the prompt.\n\n> Prompt: Welcome to the AI-Powered Innovation Challenge, where visionaries and trailblazers converge to harness the transformative potential of Artificial Intelligence (AI). In this Hackathon's Innovation Track, your goal is to leverage AI technologies to pioneer groundbreaking solutions that address real-world challenges and propel us into a future defined by innovation. Imagine a world where AI is not just a tool but a catalyst for unprecedented innovation across all sectors. Your mission is to conceptualize and develop a project that embodies this vision, pushing the boundaries of what's possible with AI-driven innovation. Whether it's revolutionizing industries, improving societal well-being, or enhancing human capabilities, let AI be the engine driving your creative and innovative endeavor.\n\nWhen we outlined the project, we decided on a few core features, and since then, we've added some more. To start, we made the main page of the website a grid of events that each had free food. Since CampusGroups does have its own proper event registration system, and some events require registration, we knew that we'd want a way for people to easily access the actual CampusGroups page, so we made the tiles clickable links. Because of this, we thought it'd be okay to truncate the descriptions and information about the event, since the event tiles would serve primarily as information about the food content of the event.\n\nOur goal was to provide food-centric event information, so we spent some time brainstorming what food-related information we could feasibly gather from event descriptions, and that would be useful. We decided that the cuisine of the food would be most important, and then for fun also added an AI generated rating of the food. Since we noticed that there are a lot of food-drive type events, we also added a flag for whether the event was a volunteering event (so, not really free food for the taking).\n\n## How it works\n\nThe project's basic structure is as follows: we have a user facing frontend, which is a `react` based website, which hooks into a database that has all the food-related events. Every 5 days an automatic `google run` lambda is run to fetch new events. Below are some more specific details about the innerworkings.\n\n## Frameworks\n\n![Cloud run logs fetching events](https://static.404wolf.com/Screenshot_from_2024-03-29_01-44-19_0001.png)\n\nWe also decided early on on the tools that we wanted to use -- mostly typescript ones :).\n\nWe decided that it'd be easiest to use react for our frontend, and to integrate a backend with `nextjs`. nextjs is a super cool framework that is sort of like `create-react-app` in that it bootstraps the `react` project creation process, but unique in that it adds many of its own abstractions like powerful server side components and \"server actions\" for running code server side, and sets up directory based routing. Next does SSR and handles hydration for you. It lets you write API routes in the same folder and even files as your `react` UI code, which makes it super easy to rapidly develop a webapp. And, [Vercel](https://vercel.com/) , the company that maintains it, offers free hobbist hosting in a way that's very convenient, including setting up CI.\n\nI wrote most of the frontend of the app, and to get a nice looking UI up and running quickly, decided to use react material UI (`MUI`), and `tailwindcss`. MUI provides a bunch of premade components that work and look nice out of the box, and tailwind lets you customize styles with simple classes, much like bootstrap, but with more flexibility.\n\nFor our database to store the events, we chose to set up a free tier `MongoDb` database, since, besides being free, we'd still be prototyping the app as we were building it. Being able to not have to worry much about migrations or constantly updating a schema would be very helpful. We used the default `MongoDb` client instead of bothering to learn and implement `mongoose` (schema enforcement for `MongoDb` for `Typescript`), and just wrote our out `Typescript` interfaces to prevent typing issues.\n\n## Getting the Events\n\nOne of the first challenges we had to tackle was actually going about getting a listing of all the CampusGroups events from a script without a webpage. After some exploring I found that the `ical` integration that CampusGroups offers to add events to your calendar had most of the descriptions in it too! By just fetching [the ical](https://community.case.edu/ical/ical_cwru.ics) from CampusGroups we were most of the way there.\n\nHowever, the public URL fails to include some important information about the event; namely, the location for most events is hidden. To solve this issue, I wrote a basic script that uses a headless chromium browser to fetch an auth token, which we use to make authed requests to CampusGroups to get information like event location.\n\n```ts\nasync function getAuthHeaders(caseId: string, casePassword: string) {\n  const browser = await puppeteer.launch({\n    headless: true,\n    args: [\"--no-sandbox\"],\n    ignoreDefaultArgs: [\"--disable-extensions\"],\n  });\n  const page = await browser.newPage();\n\n  await page.goto(\"https://login.case.edu/cas/login\");\n  await page.type(\"#username\", caseId);\n  await page.type(\"#password\", casePassword);\n  await page.click('input[name=\"submit\"]');\n\n  await page.goto(\"https://www.campusgroups.com/shibboleth/login?idp=cwru\");\n  await page.waitForSelector('[id=\"button-menu-mobile\"]', { timeout: 100000 });\n  await page.goto(URL);\n\n  const cookies = await page.cookies();\n  const headers = {\n    Cookie: cookies.map((ck) => `${ck.name}=${ck.value}`).join(\"; \"),\n  };\n  await browser.close();\n\n  return headers;\n}\n```\n\n## Screening Events\n\nTo screen all the events and populate the database with the events that had food and the metadata about the food, we decided to use an OpenAI fine tuned model. The idea behind a fine tuned model is that you take a base model from OpenAI, like GPT3 (regular tier chat GPT at the time of writing), and then feed it a bunch of example inputs and outputs, like so.\n\n```json\n{\n        description:\n            \"Join the African Student Association at our annual Karaoke event, Battle of the Mics, where you can watch and/or sing your favorite African songs, from Afrobeats to Amapiano! Enjoy lighthearted competition, great music, delicious food, and an incredible time to start off the spring semester.\",\n        rating: \"7\",\n        cuisine: \"Cultural\",\n        volunteer: \"false\",\n        onCampus: \"true\",\n    },\n    {\n        description:\n            \"Join us for Grind training on the FAB floor at think[box]. Proper PPE is Required [short sleeves, long pants, close-toed shoes, hair tied back, no jewelry An ability badge is required to complete this training. If you do not yet have an ability badge, you will need to complete a 15-minute safety tour prior to your training time\",\n        rating: \"0\",\n        cuisine: \"None\",\n        volunteer: \"false\",\n        onCampus: \"true\",\n    },\n    {\n        description:\n            \"Brew CWRU is hosting a field trip to Cleveland's very own Phoenix Coffee Co's HQ in downtown Cleveland! During this field trip, Phoenix Coffee's director will take us through various processes of QA/QC in specialty coffee including cupping, roasting, and dialing in a coffee recipe. Spots are limited due to transportation, so make sure you can commit to the registration and do not miss out on this amazing opportunity!\",\n        rating: \"4\",\n        cuisine: \"Beverages\",\n        volunteer: \"false\",\n        onCampus: \"false\",\n    },\n```\n\nThen you run a \"training\" session, which cost us only about $2, and you can use the fine tuned model by referencing its id as a model for OpenAI requests, in conjunction with a prompt, like this:\n\n```ts\nconst completion = await openai.chat.completions.create({\n  messages: [\n    {\n      role: \"system\",\n      content: `Your job is to read the description of a student-run event located at a local university, and determine whether the event provides food or not to people attending the event. You are also tasked with rating the food on a scale from 1 to 10, with 1 representing cheap canned food and 10 representing a fully prepared feast. If no part of the event mentions food, rate the food a 0. If there is food mentioned at all, give it at least a rating of 1, and at most a rating of 10. If there is food provided, to classify what cuisine the food is, which MUST be one of the following values: Pizza, Beverages, Cultural, Healthy, Sweets, Unknown, and None. Additionally, indicate whether this event is for volunteers or is offering food to the community. If it is, say true, otherwise if it is not or if taking food, say false. Additionally, indicate whether the location is on the university campus, or requires traveling off campus, also true or false (say true if it is unknown). Output each of these fields in a JSON object with the fields \"rating\", \"cuisine\", \"volunteer\", and \"onCampus\" \n                    \n                    Make sure that if there is food mentioned in the description that the rating is not a 0. Give a rating of 0 only if the event will not have any food.`,\n    },\n    { role: \"user\", content: description },\n  ],\n  model: FINE_TUNED_MODEL,\n});\n```\n\nUsing the OpenAI API is basically like simulating a user interaction with ChatGPT, but a bit more customization. What's super cool is that the AI gives us a `json` formatted response consistently and always in the right shape. I doubted the safety of this, but OpenAi's own documentation notes that this is a good method for this sort of thing.\n\nWe packaged this scanning system into a library that I then dockerized, and deployed to a Google cloud run lambda. I set it up to run every 2 days, so that events consistently get added to the database.\n\n## Finishing Touches\n\n![Mobile website](Screenshot_from_2024-03-25_13-21-22_0001.png|width=30)\n![Google calendar integration](https://static.404wolf.com/Screenshot_from_2024-03-25_13-22-03_0001.png)\n\nTo finalize the website, I added a few neat additional features. I added a [ical integration](https://www.npmjs.com/package/ical-generator) that lets you add all food events to your iCal supported calendar (like google calendar), and an add to Google Calendar link (if you click on the dates on any of the tiles it should take you to a pre-filled Google Calendar link). Getting Google Calendar template links to work was super cool, it's just a `url` template:\n\n```\n`https://www.google.com/calendar/render?action=TEMPLATE&text=${escapedName}&details=${escapedDescription}&location=${escapedLocation}&dates=${formattedDate}/${formattedEndDate}`;\n```\n\nI also added a filtering system, which initially was client side, but I moved to the backend to reduce client strain and bandwidth. I added basic Vercel analytics to the website, and have been improving the mobile UI too.\n\nAfter the hackathon we've done some optimization, improved event fetching concurrency, and in general cleaned up the UI a bit. I also added skeletons to the website for the loading screen, since the website dynamically loads.\n\n# Results\n\nIn the end, we presented to a panel of 5 judges, and came in first for the event. We had a fully completed webapp with a deployment and mobile UI, and overall I was very happy with how things turned out. It was a great experience, we learned a lot about OpenAI's fine tuning API, MUI, and the other frameworks we used. It was my first time using Mongo, and the flexibility it offered really was great. I look forward to hopefully participating in other hackathons down the line!\n\nFor some additional things that I'd like to look into adding, I'm considering adding Case SSO for getting more intimate event details like location, along with adding an email attendees button. Case SSO is technically not open source, but the system that Case uses is a common Java framework and getting the auth token from a redirect isn't hard. Retuning the model to have better training data, and also adding more information to the event tiles about the food itself would be nice to do as well.","src/posts/CWRUFreeFoodFinder.mdx","22ce8227b11869ef","coinsortbot",{"id":50,"data":52,"body":60,"filePath":61,"digest":62,"deferredRender":26},{"title":53,"type":15,"date":16,"covers":54,"tags":58,"description":59},"Coin Date Sorting Robot",[55,56,57],"https://static.404wolf.com/sideViewWithRamp_0001.png","https://static.404wolf.com/frontView_0001.png","https://static.404wolf.com/wiresOnTable_0001.png",[21,20],"Coin-sort-bot is a physical robot that I co-designed and wrote software for to automatically align and sort coins based on their physical features like year, date, and mint location. It is designed mostly with fusion360 for CAD and Python driver software, and is an IOT device that connects to a separate Django webserver. It is still in development, and currently has a basic 3d printed prototype and driver software to control the hardware.\n","# Inspiration\n\n![Coin scanner app images](https://static.404wolf.com/Screenshot_2024-02-05_151340_0001.png)\n\nA while back I was sifting through some older coins that my grandma had saved from many years ago, looking up their values one by one, and knew there had to be a better way. Some coins have collector value based how many were made that year, the denomination and composition, and various other factors. For example, some copper pennies from WW2 are worth a significant amount because most pennies then were made from zink.\n\nLooking at \"coin identifying\" apps on the app store, I found that there were already various different apps to identify coins, which acted as essentially a specialized reverse image search. They were all proprietary though, and I couldn't find a good API for the process.\n\nAdditionally, though these apps presented a software solution to make manually sifting through the coins more streamlined, I still found it tedious to have to lay down each coin on the table one by one, and snap photos of each side.\n\nI decided then, that, in addition to making a tool like that of the apps to convert an image of the coin to a dollar valuation, it would also be handy to have a robot lay out the coins, and physically move the coins into appropriate rolls based on their year and/or value.\n\n## Think\\[box] Grant\n\nAt my university (Case Western) there is a campus makerspace called \"Thinkbox.\" Each year they award various project ideas up to $2.5k to bring the ideas to fruition, through the [Thinkbox project fund](https://case.edu/thinkbox/funding/student-project-fund). Their grant page lists projects eligible as ones that are:\n\n- Individual and team-based extracurricular and personal projects.\n- Projects related to a competition.\n- Entrepreneurial projects that require prototypes.\n\nWhich my automatic coin sorter was a perfect match for. I applied for the grant, writing up the concept and creating a list of materials that I thought we'd need for the robot, like 3d printer filament, cameras, various sensors, and a raspberry pi. I invited a friend to help join me for the project, thinking that two people would increase the chance of approval. After a few weeks, we were approved for a $650 grant! I began ordering supplies, and started learning CAD and planning.\n\n### Senior project fair\n\n![Coin identifyer senior project](coinSorterSeniorProject_0001.JPG)\n![Coin identifyer senior project](secondPosterImage_0001.jpg)\n\nTowards the end of my first semester, I attended our senior project fair, and, after hearing from my logic design professor about the projects her students were working on, decided to check them out. It turned out that there were some groups in our electrical engineering department working on designing the software for identifying coin years and denominations, which was very similar to what I was working on. I noted their contacts at the time, and plan on reaching out when we get around to actually implementing the image identification for the device.\n\n# The project\n\n## Parts & Design\n\n[Coin aligner mechanism](sideViewWithPie_0001)rub\n[Coin chute component](coinChuteUnit_0001)\n\nEarly on we decided to make the project mostly 3D printed, to make fabrication simple so that we could focus on the design of the bot. The primary components of the coin sorting machine are the main tub where the input coins go, the alignment system, and the chute which dispenses the coins into coin rolls (a unique facet of our design, where we decided we'd let hang off the table).\n\nA lot of the initial steps for the process were drafting ideas on paper, trying to figure out an efficient way to sift through the coins while also taking photos of each side of every coin so that we could do processing. This turned out to be very tricky, since we wanted something that would be as foolproof as possible and wouldn't have a chance of jamming, since eventually we wanted to sift through very large amounts of coins.\n\nMost of the components 3D printed were designed in Fusion360, because of how easy it is to collaborate, but also because I liked the interface. Being able to parameterize parts has been super helpful; for example, the funnel that the coins fall into that has a coin roll in it can be generated for any type of coin by simply changing the coin diameter value, and everything updates to make it work.\n\nWe got Servos and most of our other parts through Adafruit. Adafruit makes coding for their controller boards super easy by providing libraries that 'just work.' Check out their motor controller board example code, for example...\n\n```py\nfrom adafruit_motorkit import MotorKit\n# Initialise the first hat on the default address\nkit1 = MotorKit()\n# Initialise the second hat on a different address\nkit2 = MotorKit(address=0x61)\n```\n\nWhat _was_ annoying, however, was getting RPIO (the interface that lets you use the raspberry pi hardware via Python) to work on my local machine for testing. What I figured out, however, is that I could remotely program to the Pi over SSH using either a VSCode remote development extension, or, Jetbrain's remote interpreter. With their remote interpreter add on, the code would automatically get shipped to the Pi's temp folder, and then would be executed on board, even though I'd be coding directly on my own computer. Now that I'm more into vim I probably could just do onboard development over ssh, though, which would be most ideal.\n\n![Coinbot discord IP finder](https://static.404wolf.com/coinbotDiscord_0001.png)\n![Tailscale dashboard](https://static.404wolf.com/tailscaleDashboard_0001.png)\n\nTo network to the Pi, originally I set up a simple cron job to automatically discord message me (using a discord webhook) the local IP of the device. However, configuring the IP address every time to be different was very annoying. Thankfully, I had recently learned about [tailscale](https://tailscale.com/), which has been absolutely amazing and seamlessly let me hook up the Pi (along with all of my other devices) to a secure personal virtual network. To install tailscale on the Pi, it was as simple as a single `curl` command.\n\n![Digital microscope coin image](microscopeImage_0001.jpg)\n\nFor the main board, we decided to go with a $50 Raspberry Pi 4 (which we were able to obtain from Adafruit quite easily), and for the electronics, we got 2 digital microscopes. The main issue with the cameras we got is that they need to be about 9 inches away from the coin to focus properly, but 3D printing a tube to hold them out at the proper distance should solve the problem. We purchased two LED lights to illuminate the sides of the coins, though we realized after receiving the cameras that they themselves also have lights built in. And, finally, we got 16 servos for controlling where to place the coins, along with a servo and motor controller.\n\n![Motor controller](motorHat_0001.jpg)\n![Servo controller](https://static.404wolf.com/servoControlled_0001.png)\n\nThe controllers were annoying to set up since they are designed to be \"HATs\" for the raspberry pi - that is, they are meant to literally sit on top of the board. To use multiple of them they had to be addressed (soldering binary IDs onto pads on the tops of them), and then right angle extenders needed to be soldered onto the bottom servo hat to allow the wires to stick out for the servos.\n\n## Initial design\n\n![Coin sorter inspiration](ElectricSorterLowAngleCoins061617_0001.jpg)\n\nFor the during school year portion of the project, I decided to work specifically on the hardware portion of the project, and am planning to work on the software over the summer. For now, we started off by working on how we would align the coins so that we could photograph them, process them, and then move the coins to proper output coin rolls.\n\n![Coins on rubber wheel](coinsOnRubber_0001.jpg)\n![Rubberized wheel](rubberizedWheel_0001.jpg)\n\nWe took inspiration from the a classic metal coin sorter with a rotated wheel, which the coins friction slide along as the wheel rotates and eventually dispenses them aligned onto a track by stripping them off the wheel. After a lot of experimentation, we designed something similar for use with a stepper motor, which we 3D printed. What's cool about stepper motors is how precise control we'd have over them; it may not have been necessary for this project, but it wouldn't hurt to have the extra torque and speed. During one day while working on the project, I came across some rubber sheeting that was being discarded, and decided to also lasercut that and apply it to the wheel to add friction for the coins.\n\n![The original \"drum to chute\" design](https://static.404wolf.com/Screenshot_from_2024-04-25_23-56-45_0001.png)\n\nThe next step for the spinning drum design was to \"slice\" the coins off the spinning wheel so that they could fall into a chute. We added a nib affixed to the side of the wheel to redirect the coins, and then added a pathway to allow the coins to eventually fall into a chute where we could then use servos to move them into view of a camera, and then, afterwards, separate them.\n\n![Beam break sensor](2168-04_0001.jpg)\n\nWe quickly realized, though, that it was entirely possible that multiple coins could end up on top of one another in the chute, and that multiple coins could jam the system. The issue was that, while only one coin could \"cling\" to the rubber at a time, the coins could fall one after another if the wheel were continuously spinning, allowing coins to drop faster than we could process them. We thought one good solution to this problem would be to use a beam break sensor, and so we ordered [this one](https://www.adafruit.com/product/2168) from Adafruit. But before we even tested it, we realized that adding a new chamber to limit the flow of coins would be a whole new challenge in it of itself.\n\n![The chute](https://static.404wolf.com/Screenshot_from_2024-04-25_23-55-17_0001.png)\n\nWe decided to take a break from the drum design, and move on to designing the actual chute that the channel would eventually connect to. The idea was that there would be many chutes stacked on top of each other, and we'd know which coin was falling ahead of time. We could have a single chute have its respective servo move into position so as to _redirect_ the current falling coin into a funnel, which would have an output coin roll in it. The system was very vertical, and would hang off the edge of the table, using gravity to move the coins along.\n\nThis system worked great! The servo was able to redirect the coin into the funnel, and the coin nicely fell into the bulbous shape of the funnel and aligned itself atop the other coins in the coin roll in the funnel. This system worked so well, in fact, that it inspired us to completely redesign our robot, and move away from the spinning drum method.\n\n## Redone design\n\n![New design dispensing coins one at a time](IMG_8408_0001_0001.webm|width=60)\n\nInstead of having a spinning drum with room for multiple coins to be dispensed at a time, we spent a while planning out new ideas, and came up with a different system that could allow a rotating motor arm to skimp off one coin at a time from a slightly elevated tube with a gap the size of a coin at the bottom. We reused the funnel design, but this time the funnel would redirect the coin into the chute. We considered that multiple coins could have different thicknesses, with some being even twice as thick as others (e.g. nickles and dimes), so we made the tube screw on to the main platform holding the controller, motor, platform, and funnel.\n\n![New design CAD](Screenshot_from_2024-04-26_00-06-42_0001.png|float=left)\n\nThe new design clamps on to the corner of a table, and the actual sorting chute is completely vertical. A friend mentioned that we could have it be at a slight diagonal to allow it to be slightly longer (since we're restricted to the height of the table), which is something we may consider later.\n\nFor our final project submission we provided the following rendering, since the actually physical assembly is still not quite complete and more parts need to be printed.\n\n![Final Rendering](https://static.404wolf.com/Post-20240727215816081.webp)\n\n## Driver software\n\nAlthough the actual processing of coin images is a later step that I haven't really gotten too far into yet, I have been making sure that the hardware components that we use are able to be controlled with our Pythonic on-board software. The software for this project is two-part: [software that will live on the board itself](https://github.com/Coin-Sort-Bot/OnBoard/), and a backend web server to control the board. The on board software at its current stage provides a very easy to use API interface for driving the various components, so that I can write code like this to control it.\n\n```py\nasync def main():\n    coinbot = CoinBot()\n    await coinbot.setup()\n\n    photo1 = await coinbot.cameras.camera1.capture()\n\n    await coinbot.servos.toggle(2)\n    await coinbot.motor.start(speed=40)\n```\n\nWhat I've done is added another layer of abstraction, so that we can control exactly what we need. For the cameras, I'm using `opencv`, which is a library for processing images, that also has the ability to interface with the OS to capture images. I experimented with `ffmeg` to capture the images by shelling out of my Python code, and was able to get it to work, but to keep the lights on the cameras on so that the cameras stay focused it made more sense to go with `opencv`. I may return to this eventually.\n\n## Next steps\n\nI'm currently brainstorming how to go about controlling the board remotely, and am toying with the idea of using [MTQQ](https://mqtt.org/), [RabbitMQ](https://rabbitmq-website.pages.dev/), or a simple websocket or webhook system with on-board flask endpoints to control the various components. Eventually, the machine will intake coins, send the photo data to the webserver, and then will wait until it is instructed to manipulate its servers to allow the coins to fall down the proper chute.\n\nIn the future, the plan is to actually implement OCR to scan the dates off of the coins and basic machine learning to identify denomination. I'm not entirely sure how I'll go about doing this, but it will likely involve image classification to determine the denomination and perhaps other details (such as whether or not the coin is a D-mark coin), and then either hosted OCR using an open source library like tesseract on the webserver controlling the raspberry pi, or an API like those that Amazon or Google offers to perform the text analysis.\n\n# Reflection\n\nUltimately, the prototyping and iterative design required for this project has been a lot more extensive than I expected. We've gone through a ton of rolls of filament to get a working prototype, and there's still more work that we could do. 3D printing has been great though, and allowed us to rapidly test out designs. I've also grown to really like Fusion360 (though, not it's cloud based model, and licensing model).\n\n# Additional Notes\n\nThis is still a work in progress! This post will continue to be updated.","src/posts/CoinSortBot.mdx","db39841d115b35b9","dnananotubesoftware",{"id":63,"data":65,"body":73,"filePath":74,"digest":75,"deferredRender":26},{"title":66,"type":15,"date":67,"covers":68,"tags":71,"description":72},"DNA Nanotube Design Software","2022",[69,70],"https://static.404wolf.com/example_nanotube.webp","https://static.404wolf.com/first_top_view.webp",[21,20],"NATuG (Nucleic Acid Tube Grapher) was an academic research project I undertook in my senior year that's a Python3-based desktop application designed to streamline the DNA nanotube design process. It's one of my most extensive projects, involving a complex and dynamic UI that lets users visualize DNA nanotube shapes, weave together strands of DNA with cross-strand exchanges, set and export sequences, and more. Here I discuss what the program does, a bit about how it works, and how I got involved.\n","# Getting Started\n\nA video \"TLDR\" of what NATuG can do --\n![Program preview](programOverview_0001_0001_0001.webm|autoplay=true|float=none|width=100)\n\nA technical briefing made in preparation for a \"technical deep dive\" presentation on the project can be found **[here](https://github.com/404Wolf/stainless-technical-breifing-natug/releases/download/v1.0/natug_technical_brief.pdf)**.\n\n## Involvement\n\nIt was early 2022, in my professor's Physics class, when in my first homework assignment, an about-me paper, I discussed my love of programming. I wrote about how I liked the problem solving that accompanied code, and how I was eager to learn more about Physics because of its practical applications. Fast forward a few months, and he reached out to see whether I'd be interested in contributing to an ongoing research project.\n\nWhat I love about code is that it enables actual application of theory (praxis). By implementing known algorithms computationally, processes become simpler and more practical. In some cases, they become simply _possible_ and, critically, accessible. This project was to involve me developing an application around a complex framework for designing DNA nanotubes, which I could see would enable never before possible designs, but also could serve as a tool for people with limited knowledge of the field in general to learn more too. The project builds off my professor's [research paper discussing DNA nanotube design](https://www.sciencedirect.com/science/article/pii/S0006349506726303) by implementing the theory in practical and user friendly software.\n\nBut before discussing the specific application I developed, it is important to gain a brief understanding of the field surrounding the software.\n\n## Background\n\nI like to think of DNA nanotubes as a niche in the complex and ever-growing realm of DNA nanotech. DNA nanotech is, essentially, taking advantage of how DNA operates (base pairing, shape, chemistry, etc.) to get it to form custom shapes. These shapes are super tiny, so in a way it's like nanoscale 3D printing. Nanotubes are, of course, just one of many different possible shapes that you can form DNA into, but one with particularly many use cases. Employing DNA nanotechnology, one can create a wide array of highly customizable, self-assembling, rigid nanostructures with precision of up around 1nm, and applications ranging from nanophotonics (altering light wavelengths by passing light through tiles of the DNA), to DNA based computing, to DNA-based sensors, and much more. The field is still developing, and new applications are sprouting constantly.\n\n### DNA Nanotubes\n\n![Example DNA nanotube](https://static.404wolf.com/example_nanotube.webp)\n\nThere are many different forms of nanotubes that can be created, but they are all somehow alterations of a hollow, tubular structure. They involve conjoining many cylindrical DNA double helices with cross-strand exchanges to 'bond' them together. DNA nanotubes possess significant potential for use in targeted drug delivery, tissue wound repair, and much more. The specific form of nanotube that I've focused on consist of multiple double helices running parallel to each other, a specific type of nanotube. Even with the constraints of the structure I was designing software to create, design is very complex, and there are many different possible tubes. The design process involves identifying a tube shape consistent with the intrinsic geometry of a DNA double helix, aligning helices, placing connections between helices as to generate minimal strain, and determining base sequences.\n\n![Staple Strands](dnaOrigami_0001.png|float=left)\n\nMaking matters more complex is the fact that to actually go about transferring the theoretical shapes to test tubes, these nanotubes generally utilize DNA origami, in which a long viral strand is folded into shape by short \"staple strands.\" This is because synthetic DNA can only be up to 100 base pairs in length or so, while viral DNA can cheaply be produced with thousands of bases. By using strand \"staples\" you can fold up the virus into a shape of your choosing.\n\nWhile the process needed for nanotube design has been known for a while, up until NATuG there's been no good computational tool specifically dedicated to their design. Previously bionanotechnologists could utilize a complex spreadsheet tool, allowing users to plot the side view and top view, but it could only support a fixed number of double helices (given the nature of spreadsheets), and did not provide the necessary interactivity that the software I developed, NATuG (nucleic-acid-tube-grapher), offers. Interestingly, NATuG also allows for the creation of non-tubular structures, with a super intuitive interface that is great for people first learning about DNA nanotech; though, however, there exists various tools for nontubular DNA nanostructure design.\n\n# The program\n\n![Side view plot](https://static.404wolf.com/side_view_plot.webp)\n\nNATuG is, most simply put, a Python-based desktop application that is designed to streamline the DNA nanotube design process. It provides a nice UI that allows users to easily set the angles between double helices as to change the overall shape of the tube, while computing a top and side view plot in real time. NATuG automatically lines up the double helices as to allow for cross-strand exchanges to hold together the structure. With a single click on overlapping nucleoside midpoints (spots between two nucleosides), NATuG automatically swerves the strands across double helices, creating a cross-strand exchange. Additionally, users can easily interact with specific nucleosides of the structure, create nicks and linkages, and more. By strategically placing junctions throughout the structure, the isolated double helices become a unified, rigid nanotube. The program provides an intuitive interface, allowing one to customize and visualize the nanotube shape, weave together helices in a matter of clicks, and apply/export sequences. So, while designing DNA nanotubes is a multistage endeavor, NATuG aims to make the process as dynamic as possibleletting users easily traverse the nanotube design process with ease.\n\nIt's a very complex tool that I've continued to build on extensively, and includes rendering features, a custom file format to bundle and save program states, and much more. I've\n\n![NATuG 2.0](https://static.404wolf.com/NATuG2.webp)\n\nThis is the third iteration of NATuG software, and is a complete overhaul both UI wise and design wise. The older versions were abandoned years ago, and at the time they had extensive limitations, such as only allowing for a single type of cross-strand exchange, insufficient for the design of structurally integral DNA nanotubes. This version of NATuG takes much inspiration from the interface of the previous versions, but is also original in many ways, and has many more features than its predecessors.\n\n## Getting started\n\n![NATuG interface](https://static.404wolf.com/interface.webp)\n\nWhen starting work on NATuG, I needed to start off choosing my UI and plotting framework, which would inevitably be a vital part of the program. Interactivity would be key, along with, ideally, a large amount of plug-and-play widgets to choose from. Ultimately, I settled on [QT's](https://www.qt.io/) UI framework, and Python as the primary language, given my experience with the paradigms of the language, and comprehensibility of the toolkit. Though the project was my first time with UI design, it was a great learning experience. For the plotting framework, though many to most Python projects implement [matplotlib](https://matplotlib.org/) as their, I elected to use [pyqtgraph](https://www.pyqtgraph.org/) because of how well it integrates with PyQt, and support for much needed interactivity. It's able to track clicks on individual points, and update in real time.\n\n## Adding features\n\n![NATuG's first plot](https://static.404wolf.com/first_top_view.webp)\n\nMy first step with the program, before building a complex, multi-widget UI, was to actually implement side and top view visualization plots. And for this, I began in Excel, tinkering with parameters while working on grasping the already known plotting algorithm I'd be using. Of course, Excel's plotting capabilities are much more limited than that of programic plotting in Python with pyqtgraph, and I'd have a lot I'd need to learn.\n\n## The plots\n\n![Side view plot](topView_0002.svg|float=none|width=100)\n\nNATuG consists of two plots: the **top view plot** and the **side view plot**. Both plots display the same nanostructures, but from two different perspectives.\n\n### Top View Plot\n\n![Heart shaped tube](https://static.404wolf.com/https://static.404wolf.com/heart_tube.webp)\n![Symmetrical design](https://static.404wolf.com/https://static.404wolf.com/symmetric_design.webp)\n\nThe top view plot displays a view of the nanotube from the top down, showcasing the overall shape of the tube. It allows the user to better visualize what they are actually constructing. As one tinkers with the inter-domain angles, the actual shape of the tube changes. Each circle represents a DNA double helix, and the fact that they all touch and eventually close up indicates that it is a closed tube. I've also made it so that if you click on a specific double helix in the top view plot, NATuG automatically pans to the side view plot location that has information on the nucleosides in that region.\n\n### Side View Plot\n\n![Creating cross-strand exchanges](https://static.404wolf.com/creating_junctions.webp)\n\nThe side view provides a view as if the nanotube had been unrolled flat, and is strategically distorted as to show all the nucleosides of the nanostructure without overlaps. This plot is much more complex and feature-rich than the top view plot, since it allows for user interaction with the actual nucleosides. It allows users to create nicks in strands, conjunct strands, link together the ends of strands to allow for DNA origami designs, and more. Lots of work has gone into converting the underlying datastructure for strands into a visualized pyqtgraph plot widget that has signals properly hooked up as to allow the user to manipulate the state.\n\n### Future Plots\n\nIn the future, there's many other types of plot improvements, and even entirely new plots that could be contrived. For instance, we have spent some time discussing the possibility of creating a 3D plot, to visualize the tube from any given perspective at once.\n\n### The interface\n\n![Multi-panel UI beginnings](https://static.404wolf.com/beginnings_of_ui.webp)\n\nThe primary goal of NATuG has been to make the nanotube design experience as pain-free and intuitive as possible. So, not only have I had to learn how to actually go about creating UIs, but I've also had to figure out the best way to position and size elements to make clear how to utilize the program. For the UX design, I choose to go with a three panel UI, with an undockable multi-use, tabbed configuration panel and a resizable top/side view plot area. This allows the user to view the different plots simultaneously or one at a time, while always having access to the configuration.\n\n### Junctions!\n\n![Possible junction types](https://static.404wolf.com/junction_types.webp)\n\nOne of the most important features of NATuG is the ability to create cross-strand exchanges. These exchanges implement \"Holliday\" junctions to allow strands to swerve across their helical domains, weaving together the nanostructure. This was definitely the most difficult to implement part of the project, since previous works showed that junctions were possible, but relied on intuition to determine how to go about making them. This makes sense, since if you look at a side view helix plot, it is fairly obvious to a human how to conjunct the strands. This was definitely one of my favorite parts of the project, since it was both challenging and rewarding to catalog the various cases. Unrelated: this was a classic case of [Moravec's Paradox](https://en.wikipedia.org/wiki/Moravec%27s_paradox), which is definitely an interesting read.\n\nAfter extensive tinkering and cogitating, I've come to the conclusion that there are a distinct amount of different possible cases for conjunction the strands of two arbitrary nucleoside-end midpoints, which are outlined in the tree to the right. As it turned out, there were specific pathways to the subcases, dependent on the closedness of the strands of the NEMids and whether the NEMids are in the same strand. Coming up with this algorithm is definitely one of my most impressive contributions to the project, and was almost equally annoying to implement. Implementing the algorithm required moving nucleosides between strands, of which some had no definite start/end and were closed loops. Determining how to reindex arrays to most efficiently build the new strands, and prevent off-by-one errors, was no easy feat, and if you're interested in reading through the code yourself you can find it [here](https://gist.github.com/404Wolf/bbbe24af9939124a0752b8909097e578).\n\n# Takeaways\n\nFirstly, there are many project-related takeaways worth discussing. Ultimately, NATuG showcases the powerful potential of software to allow for more complex, larger-scale tubular DNA nanostructures. The program allows the user to spend less time doing tedious computations, and more time focusing on the actual design of the structure. Additionally, with the ability to load and export structures into and out of NATuG, we expect collaborative design to become much more streamlined. Additionally, we also realized that the program holds the potential for non-closed tile design, which is common in the realm of DNA origami. While there are other tools that are more sophisticated and perhaps better suited for this specific purpose, NATuG provides a uniquely user friendly and convenient experience.\n\nAs I complete the initial version of the program, Professor Sherman and I are beginning to draft a paper for NATuG, which we intend to submit for publication to a peer reviewed journal within the next few months. We hope to discuss some of the potential shapes of nanotubes that can be created, the way that the program tackles conjoining strands, and the like.\n\n[Nadrian Seeman](nadrianSeeman_0001)\n\nTowards the end of the project we've also informally shared the project at NYU's Nadrian Seeman Memorial Symposium. Nadrian Seeman is the founder of DNA Nanotechnology as a field in general, and paved the path for not just this project, but all structural DNA related projects and software tools. He showed that it's possible to take a medium like DNA, the molecule of life, and turn it into something completely different: a nanoscale building block. The symposium at NYU was truly an amazing experience, where I got to hear about many different fascinating areas of the field and speak with people about countless ongoing related projects.\n\nOn a more personal note, this project was one of my best coding experiences yet. I've come to find that large scale projects are generally the best way for me to improve my coding abilities, and this was no exception. Spending countless hours figuring out how to detect the click of an up button on a double spin box may seem like a complete waste of time, but many different obsessions and debugging sessions throughout the project have greatly improved my bug-catching abilities. Working with libraries like PyQt may seem to provide niche skills that may only be useful for specific types of projects in the future, but they actually helped me grasp the complex concepts of OOP and better understand Python's weird quirks. I liked this project more than many of the others I've worked on because it's not a means to an end, but is an end that provides more means.\n\n## Learn more\n\n![Poster preview](natugPosterLowRes_0001.png|float=none|width=100)\n\nThough this post attempts to discuss the needed background to understand what NATuG does, I still strongly suggest reading my professor's [paper on the design of DNA nanotubes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1471877/). It's definitely quite complex, but it has useful diagrams and is a very interesting and mind bending read. Also, it may be helpful to check out [this poster I created showcasing the project](https://drive.google.com/file/d/1R7wqzkVm2bnDmgzsexkkqsYj_X0iEB5T/view?usp=sharing).\n\nIf you're interested in learning more about how to actually set up and use the program, check out its [user manual](https://github.com/NATuG3/Manual/blob/main/manual.pdf) and [Github repository](https://github.com/NATuG3/NATuG3). Also, feel free to reach out if you have any questions.","src/posts/DNANanotubeSoftware.mdx","9687177e286e94a0","forcedbrowserzoomextension",{"id":76,"data":78,"body":86,"filePath":87,"digest":88,"deferredRender":26},{"title":79,"type":15,"date":16,"covers":80,"tags":84,"description":85},"Forced Browser-Based Zoom Extension",[81,82,83],"https://static.404wolf.com/Post-20240716211735260.webp","https://static.404wolf.com/Post-20240716212111111.webp","https://static.404wolf.com/Post-20240716214604533.webp",[34],"This idea spawned from my hatred and eternal frustration with Zoom. It's just always a pain to deal with; screen share often just doesn't work right, the UI glitches out, audio is a mess and very inconsistent, and it eats up system resources and is probably doing mysterious things behind the scenes intruding on privacy. There's a solution though: use Zoom in your browser, only, always, ever! Zoom provides an option to join in the browser, given that you reject their popups to open in desktop, and then click through a chain of links to open in the browser. My solution: a browser extension to redirect Zoom links directly to browser Zoom.\n","# The Idea\n\nIt's just always a pain to deal with Zoom. Screen share often just doesn't work right, the UI glitches out, audio is a mess and very inconsistent, and it eats up system resources and is probably doing mysterious things behind the scenes intruding on privacy. But privacy aside, I'm sure everyone reading this has been late to **at least** 1 meeting because Zoom randomly decided to update (I thought that when I ditched windows that problem was behind me!).\n\nThere's a solution though: use Zoom in your browser, only, always, ever! Zoom provides an option to join in the browser, given that you reject their popups to open in desktop, and then click through a chain of links to open in the browser. The nice thing about the browser is that it's sandboxed, it's very limited in what access it has to your system, and is much more private than a closed source binary.\n\nMy solution: a browser extension to redirect Zoom links directly to browser Zoom. The idea was simple, I would create a simple plugin for Chrome (and eventually Firefox) to allow you to skip past popups to join Zoom rooms with the desktop app. The goal was so that you could paste in a Zoom URL as usual, and you would just be forwarded directly to the Zoom room in your browser.\n\n# Research\n\nThe first step was to figure out what the exact user flow that I would need to automate would be. It would be ideal if I could avoid simulating user actions as much as possible, and just take advantage of flags/find ways to redirect right to the browser Zoom interface.\n\nWhat I quickly figured out is that when you visit a Zoom URL, `onWindowLoad` it creates the infamous \"Open Zoom Workplace\" popup, **AND** appends `#success` to the URL.\n\nI figured out if you visit a Zoom URL that already has this `#success` flag at the end of it, that Zoom will not give you the popup. This turned out to be a great step, since apparently it's just straight up not possible to interact with browser dialogs with an extension. Not discovering this may have killed off the project.\n\n![Joining a Zoom](https://static.404wolf.com/Post-20240716211735260.webp)\n\nThe next step in the user flow I thought would be simple, just scrape the page and find the URL that the \"Join from your Browser\" button links to, and then redirect there, but it turns out that that button is actually executing javascript.\n\n![The button](https://static.404wolf.com/https://static.404wolf.com/Post-20240716212111111.webp)\n\nLuckily extensions do have the ability to modify the webpage, so I could just click the button for the user. This is annoying, but it works fine.\n\nNow let's dive into how I actually wrote it.\n\n# Implementation\n\nI'd never made a browser extension before, but it turns out that [Google](https://developer.chrome.com/docs/extensions)'s (and Mozilla's, as I'd figure out later when porting to Firefox) documentation for extensions is pretty good.\n\nBasically, the general flow is that you create a directory with a image icon for the extension, a `manifest.json` file that follows a specific schema (there's a few versions of the schema that chrome supports, but the most recent one is called \"Manifest v3\"), and then some javascript files (technically optional, but that's where the extension's logic goes). I started with a very bare bones basic manifest and just added permissions and configuration as I went.\n\n## Development\n\nGetting a developer environment set up for the project was pretty straight forward. First, I made a `nix` devShell that had a base chromium and firefox binary, so that I could test in a clean \"sandbox\"-y environment:\n\n```nix\n{\n  description = \"Force browser only Zoom\";\n\n  inputs = {\n    flake-utils.url = \"github:numtide/flake-utils\";\n    nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";\n  };\n\n  outputs =\n    {\n      self,\n      nixpkgs,\n      flake-utils,\n    }:\n    flake-utils.lib.eachDefaultSystem (\n      system:\n      let\n        pkgs = import nixpkgs { inherit system; };\n      in\n      {\n        devShells = {\n          default = pkgs.mkShell {\n            packages = [\n              pkgs.ungoogled-chromium\n              pkgs.librewolf\n            ];\n          };\n        };\n      }\n    );\n}\n```\n\nAnd then in Chrome (and in Firefox it was a similar process) I went to `chrome://extensions`, and enabled developer mode.\n\n![Developer mode](https://static.404wolf.com/Post-20240716222255624.webp)\n\nAfterwards, I clicked to load the \"unpacked\" (you can \"pack\" your extension as a special zip like file with encryption, in case you want to deploy a closed source extension) plugin.\n\n![Loading it in](https://static.404wolf.com/https://static.404wolf.com/Post-20240716222359125.webp)\n\nAnd it was added! It was a pretty easy process. I could get logs by clicking \"Details\", and could just reload it by clicking a little reload icon.\n\n## Redirecting\n\nThe first step of the project's implementation would be to figure out how to redirect users as they visit websites, as a function of the website they visit. Something like\n\n```ts\n(websiteVisited: string) => websiteVisited + \"#success\";\n```\n\nSuper simple, I know.\n\nI found [this super super helpful](https://stackoverflow.com/a/12070823) Stack Overflow post summarizing the options I had.\n\nThe options they presented:\n\n> 1. The[`webRequest`](https://developer.chrome.com/extensions/webRequest.html)API, specifically the[`onBeforeRequest`event](https://developer.chrome.com/extensions/webRequest.html#event-onBeforeRequest). (Even better, the upcoming[`declarativeWebRequest`API](http://developer.chrome.com/extensions/declarativeWebRequest.html)).\n\n(note that the url is broken -- it's now [here](https://developer.chrome.com/docs/extensions/reference/api/declarativeNetRequestk)\n\nThis initially seemed like the best option. It seemed like the \"proper\" way to do it. Firstly, it's pretty secure since I don't have to have my extension view the web content itself (although, as I'll explain later, this turned out to be necessary anyway)\n\n> The`chrome.declarativeNetRequest`API is used to block or modify network requests by specifying declarative rules. This lets extensions modify network requests without intercepting them and viewing their content, thus providing more privacy.\n\nMore importantly though, it seemed like this approach wasn't great for actual browser navigation, and was better for redirecting network requests, which wasn't really my goal.\n\n> 2. [Content scripts](http://developer.chrome.com/extensions/content_scripts.html). Inject`location.replace('http://example.com')`in a page.\n\nThis option basically means swapping out links that users would click on so that they never end up at the link without `#success` to begin with. This also means that it wouldn't work if users pasted the URL into the search bar, so this wouldn't work.\n\n> 3. The[`tabs`](http://developer.chrome.com/extensions/tabs.html)API. Use the[`onUpdated`event](http://developer.chrome.com/extensions/tabs.html#event-onUpdated)to detect when a page has changed its location, and[`chrome.tabs.update`](http://developer.chrome.com/extensions/tabs.html#method-update)to change its URL. Avoid an infinite loop though!\n\nThis option looked great! Just monitor when a tab starts to visit a URL and then before it can begin doing fetches swap the URL. This is what I went with, and it turned out later that Mozilla had an identical API and version for Firefox.\n\nThe code for deciding to redirect the user ended up being super minimal, and looked something like this...\n\n```js\nchrome.webNavigation.onCommitted.addListener(\n  (details) => {\n    zoomPattern =\n      /^https:\\/\\/(\\w+\\.)?\\w+\\.\\w+\\/j\\/\\d{10}\\?pwd\\=[a-zA-Z0-9]{32}$/;\n    if (details.url.match(zoomPattern))\n      chrome.tabs.update(details.tabId, { url: `${details.url}#success` });\n  },\n  { url: [{ urlMatches: \".*\" }] },\n);\n```\n\nIt's probably a bit more complicated than it had to be, but basically I wanted to be super careful that people would only be redirected when they visited Zoom URLs, and Zoom URLs can be hosted on your own enterprise domain (e.g. `zoom.yourcompany.com`), so I wanted to make sure that I pattern matched against the Zoom URL schema as tightly as possible with regex.\n\n## Clicking Past\n\nNow that I was able to skip the \"Open in Zoom Desktop\" popup, I'd need to click the button for the user to actually go to the browser Zoom page.\n\nThis is just a clickable element, and interacting with it was as simple as using a selector to select it, and calling the `click` method. I'd done quite a bit of `puppeteer` before and it was very similar syntax.\n\nThe full code for this looks like this\n\n```js\nconst elements = [\n  ...document.querySelectorAll('a[role=\"button\"][tabindex=\"0\"]'),\n];\nconst element = elements.find(\n  (el) => el.innerText === \"Join from Your Browser\",\n);\nif (element) {\n  element.click();\n}\n```\n\nIt's really not that bad. I just add a listener to execute this on all page loads (although I really should only do this on loads of Zoom-like-URLs, but optimizations come later!).\n\nI also added some code to remove the other buttons on the page and Ads to get the app, for while the page redirected you and things were still loading\n\n```js\n// findAndRemoveIfExists is a helper function I define elsewhere. It does what\n// the name says it does.\nfindAndRemoveIfExists(\"div.ifP196ZE.x2RD4pnS\");\nfindAndRemoveIfExists('hr[role=\"presentation\"]');\nfindAndRemoveIfExists('h3[class=\"rm-presentation\"]');\n```\n\nAnd had a bit of fun messing with the DOM...\n\n```js\nconst zoomAppSpans = document.querySelectorAll(\"span\");\nconst targetSpan = [...zoomAppSpans].find((span) =>\n  span.textContent.includes(\"with the Zoom\"),\n);\nif (targetSpan)\n  targetSpan.childNodes[0].textContent = \"Hate the Zoom Desktop App? \";\n```\n\nAnd then all I had to do was write a `manifest.json` with the right permissions. It turns out that ChatGPT is much better at writing the old `manifest V2` style manifests, but it was still helpful\n\nIt was pretty simple, and the hard part was just finding what I needed permissions for in the docs.\n\n```json\n{\n  \"manifest_version\": 3,\n  \"name\": \"Browser Zoom\",\n  \"description\": \"Only let zoom run in the browser. No more popups!\",\n  \"version\": \"1.0\",\n  \"background\": {\n    \"service_worker\": \"background.js\"\n  },\n  \"content_scripts\": [\n    {\n      \"matches\": [\"https://*/*\"],\n      \"js\": [\"content.js\"],\n      \"run_at\": \"document_idle\"\n    }\n  ],\n  \"permissions\": [\"activeTab\", \"webNavigation\"],\n  \"action\": {\n    \"default_popup\": \"popup.html\",\n    \"icons\": {\n      \"16\": \"https://static.404wolf.com/icon-16.png\",\n      \"48\": \"https://static.404wolf.com/icon-48.png\",\n      \"128\": \"https://static.404wolf.com/icon-128.png\"\n    }\n  }\n}\n```\n\n# Distributions\n\nThe plugin is available on my Github [here](https://github.com/404wolf/Browser-Zoom), and is totally open source.\n\n## Chrome\n\n![Google Extension Rejection](https://static.404wolf.com/https://static.404wolf.com/Post-20240716215027831.webp)\n\nIt was a bit tricky to publish the plugin on extension stores, mostly because it requires access to \"modify\" ANY website. Basically, in my extension's manifest (where metadata about it and its permissions live, which is mostly directives to the browser), it has\n\n```json\n...\n\"content_scripts\": [\n  {\n    \"matches\": [ \"https://*/*\" ],\n    \"js\": [ \"content.js\" ],\n    \"run_at\": \"document_idle\"\n  }\n],\n...\n```\n\nThis means that the extension has permission to modify the DOM of any website. Initially I was worried Google would reject the plugin because of this extensive requirement, but my main justification is that Zoom can run on any domain (like zoom.yourcompany.us), and thus I can't match domains without complicated regex (but the match pattern here is not regex, it's special google syntax).\n\n![Chrome developer dashboard](https://static.404wolf.com/https://static.404wolf.com/Post-20240716215251691.webp)\n\nIt turned out that the annoying parts of the Google extension publishing experience were elsewhere, though. To start, to even make a developer account for the Chrome webstore dashboard, you need to make a $5 deposit to \"fight spam and abuse.\" Then, after publishing it a few times, while they haven't gotten upset about the permission I thought they would be upset about, they've rather been nit picky on things that don't matter -- like, in my most recent rejection, they told me I don't have a privacy policy link, even though I don't collect user data and supplied a link to a plaintext file that says \"I do not collect any data.\" I'll keep trying to get it published on the Chrome extension store, though!\n\nFor now, to run it on Chrome just get it from the Github and install it yourself, it's not that hard and you don't need any developer experience.\n\n## Firefox\n\nMozilla on the other hand was really pleasant to deal with. Their store's developer UX was easier to navigate, there was no \"anti-spam\" deposit, and they gave me \"preliminary approval\" in two days.\n\n[It's here, on the Mozilla firefox extension store, if you use firefox!](https://addons.mozilla.org/en-US/firefox/addon/browser-zoom/reviews/)\n\n![Extension Listing](https://static.404wolf.com/https://static.404wolf.com/Post-20240716214604533.webp)\n![Extension ratings](https://static.404wolf.com/https://static.404wolf.com/Post-20240716214554914.webp)","src/posts/ForcedBrowserZoomExtension.mdx","cd56f3a8a51b8364","obsidiancontactimporter",{"id":89,"data":91,"body":97,"filePath":98,"digest":99,"deferredRender":26},{"title":92,"type":15,"date":16,"covers":93,"tags":95,"description":96},"Obsidian Contact Importer",[94],"https://static.404wolf.com/Post-20240806180300351.webp",[34],"A project to bring phone contacts into Obsidian as markdown files, converting vCard (.vcf) files exported from phones or Google Contacts into nicely formatted notes. By storing contacts as markdown, users can take better notes about people they meet and maintain richer context around relationships, while keeping everything in their Obsidian vault. The tool uses customizable templates and handles contact details like names, phones, emails, addresses and even contact photos, making contact information easily searchable and editable within Obsidian's ecosystem.&#x20;\n","# Everything in One Place\n\n[Obsidian](https://obsidian.md) is a very cool markdown driven notetaking app. It's a very general purpose note taking app, and can do everything that every other note taking app can. It is also super extendable, which is really nice -- it is an electron desktop app, so plugins can be literally anything. There are plugins that add full on react UI components, and plugins that mess with webasm. Plugins are implemented by hooking onto type declarations that Obsidian provides, and it all is very well integrated into the app.\n\n![Markdown all the way down](https://static.404wolf.com/https://static.404wolf.com/Post-20240802183939700.webp)\n\nThe coolest part of Obsidian though, in my opinion, is the fact that all of your notes live as a hierarchy of [obsidian flavored markdown](https://help.obsidian.md/Editing+and+formatting/Obsidian+Flavored+Markdown) files (which is just a superset of markdown that adds things like highlighting, comments, and callouts). Obsidian itself is closed source, but I'm okay with using it anyway since my files are not obfuscated in any way.\n\nA lot of people have extended obsidian with its powerful plugin plugin system, to let you do more than just take notes. One philosophy that seems to be catching on is to use Obsidian as a life manager; as an all in one tool.\n\n## Plugin Ecosystem\n\nObsidian is [Electron](https://www.electronjs.org/), so it can do anything a browser can. It's implemented very well and you don't really notice this unless you try, but if you want to _create_ for Obsidian, the sky is the limit -- heck, [you can create plugins with React](https://docs.obsidian.md/Plugins/Getting+started/Use+React+in+your+plugin).\n\n![Obsidian full calendar](https://static.404wolf.com/https://static.404wolf.com/Post-20240806165246984.webp)\n\nOne plugin I've started using is [Obsidian Full Calendar](https://github.com/obsidian-community/obsidian-full-calendar), which a cool calendar plugin for Obsidian that lets you unidirectionally sync iCal calendars, and then also manage events within Obsidian, where it can treat events as markdown files. That means that I can create an event within Obsidian, and it can create a corresponding event markdown file, that has the actual metadata for the event in it.\n\nThis is a paradigm that I really like -- treating usually structured and rigid data as semi-structured markdown.\n\nI have started to use templates to create other types of \"typically _structured_ dataforms\" in unstructured markdown. The nice thing about Obsidian is that this does **not** mean copying and pasting every single template. Templating in Obsidian is pretty mature at this point, it comes with a [built in templating plugin](https://help.obsidian.md/Plugins/Templates), and the community has created a much more powerful [Templator](https://github.com/SilentVoid13/Templater) plugin. You can plug in everything from the current note's file name, to the date, to arbitrary javascript function results.\n\n![Omnisearch](https://static.404wolf.com/https://static.404wolf.com/Post-20240806170104437.webp)\n\nI think one of the coolest Obsidian plugins, that really does totally change the experience, is [Omnisearch](https://github.com/scambier/obsidian-omnisearch), which is a super powerful search engine for your entire note vault. It does weighted fuzzy finding across all vault contents, and it even OCRs images and PDFs. What that means is that the search results are always as relevant as possible -- if you search for a file, files with some of that name (it's fuzzy!) show up first, followed by notes with titles, and contents. This means that if all of my life is stored as markdown in my Obsidian vault, then it's easy to find things later.\n\n## The dilemma\n\nPart of this does mean that I'm restarting systems that I've had in place for a really long time. I really like Google Calendar, but I _prefer_ being able to put whatever I want into _semi structured_ markdown files for events. Part of switching to this new system means that my old calendar events are not going to be in my Obsidian vault.\n\nImporting old calendar events is good enough for now, because since the plugin supports linking iCal I still can view all my old events, I just don't have the ability to have corresponding markdown files. I probably could figure out how to import them that way with some scripting, but that's not really worth it for me since those events are in the past and I'm probably not still taking notes on them.\n\n![My options](https://static.404wolf.com/https://static.404wolf.com/Post-20240806171003276.webp)\n\nOne area of my note-taking life that I would like to include in my Obsidian vault, though, is my contacts. All my contacts before this project were, as was the case for most people, stored in [Google Contacts](contacts.google.com), and on my phone in the Contacts app.\n\nWhat I wanted was a system to load my contacts into structured markdown files (ideally following some sort of template that I could fine tune and specify). Obsidian has a plugin registry built into the app, which you can contribute to by just PRing to a repo. It is pretty extensive, but there were really limited options that showed up. It's possible there's some project out there on Github, since Obsidian plugins get loaded by just plopping a manifest and js bundle into a specific spot, but I couldn't find any.\n\n## But also\n\nBut also, I want to get better at remembering people. I encounter a lot of people on a fairly regular basis, but I think I could do a better job of remembering what people do, interesting things I talk about with them, and that sort of thing.\n\nBy having all my contacts in raw markdown, I think it will both lower the barrier to someone being a \"contact,\" and will allow me to keep better records of how I know people and track my conversations with them over time.\n\n# The Project\n\n## Research\n\n![Google Contact export](https://static.404wolf.com/https://static.404wolf.com/Post-20240806171751994.webp)\n\nIf I assume that all my contacts are in Google contacts (a bold proposition), then I can export them as a CSV and then use them that way. But that also means that anyone who uses my tool will also need to be using Google contacts. I guess I could have asked people to upload their contacts to Google so that they could then download them as a CSV, but that's clunky, so I decided to look into how they exist on the phone itself.\n\nI can't really know how Apple is storing them on the file system because of how closed source it is, but one thing I did figure out is that Apple let's you export your contacts in a uniquely unintuitive way: press and hold on a contact \"list\", or \"all contacts\", and choose to Export (or email every single contact in your phone! I will not be doing that, though.).\n\n![Export all contacts](https://static.404wolf.com/https://static.404wolf.com/Post-20240806172024892.webp)\n![What?](https://static.404wolf.com/https://static.404wolf.com/Post-20240806172020774.webp)\n![Where to?](https://static.404wolf.com/https://static.404wolf.com/Post-20240806172014834.webp)\n\n### VCards\n\nIt turns out that the format that gets exported is the `vCard` format, which is the same format that gets used when you share contacts: if you text a friend your contact what you've sent them is a `vCard` file.\n\nvCards are kinda a pain. There's a proper [RFC](https://www.rfc-editor.org/rfc/rfc6350.html) on the spec. It's a 74 page PDF. [The vCard format was originally developed by Apple, AT\\&T, IBM, and Siemens.](https://www.loc.gov/preservation/digital/formats/fdd/fdd000616.shtml) as a new unified standard for contact storage, but it's very powerful.\n\nThe US library of congress has a section for the format, so I'll drop in their description and move on\n\n> Virtual Card Format (vCard) is a versatile data format designed for exchanging electronic representations of contact information. vCard is commonly referred to as an [\"electronic business card\"](https://www.nationalarchives.gov.uk/PRONOM/fmt/395) and is fully and openly standardized through IETF [RFC 6350](http://tools.ietf.org/html/rfc6350). The vCard file [contains](https://docs.fileformat.com/email/vcf/) the same type of content typically found on a physical business card, such as a contacts name, address, phone number and email but may also list more personal data such as birthday or even organizational data such as line of supervision. Being digital, vCards can contain graphics (including headshots or ID photos), video, and audio as well as textual data. These files are shared across a wide variety of communication channels including email, instant messaging, text messaging, and website embedding. A single vCard file can contain information for one or more contacts. These digital cards are versatile, extending beyond personal descriptions to potentially represent various directory objects, including organizations, departments, or even buildings. However, the [predominant use](https://www.oreilly.com/library/view/programming-internet-email/9780596802585/) remains the representation of individuals.\n\nThat's right, you can store arbitrary data in vCards! Pretty cool.\n\n## Parsing VCards\n\nSo, vCards are plain text that store contact information. Great, that means we can look at them (and diff them, and all the other fun things that plain text let's us do)...\n\nHere's my vCard\n\n```vcard\nBEGIN:VCARD\nVERSION:3.0\nPRODID:-//Apple Inc.//iPhone OS 17.5.1//EN\nN:Mermelstein;Wolf;;;\nFN:Wolf Mermelstein\nX-PHONETIC-FIRST-NAME:Wolf\nEMAIL;type=INTERNET;type=pref:wsm32@case.edu\nitem1.EMAIL;type=INTERNET:wolf.mermelstein@case.edu\nitem1.X-ABLabel:University Alt\nitem2.EMAIL;type=INTERNET:wsm32@cwru.edu\nitem2.X-ABLabel:University\nEMAIL;type=INTERNET:uptothebird@gmail.com\nitem3.X-ABLabel:Primary\nitem5.TEL;type=pref:(917) XXX-7875\nitem5.X-ABLabel:Backup\nTEL;type=CELL;type=VOICE:(929) XXX-7180\nNOTE:This is me.\nitem6.URL;type=pref:https://404wolf.com\nitem6.X-ABLabel:_$!\u003CHomePage>!$_\nBDAY:XXXX-REDACTED-08\nPHOTO;ENCODING=b;TYPE=JPEG:/9j/4AAQSkZJ....SIJ\nEND:VCARD\n```\n\nI got it by clicking \"share contact,\" and then I shared it via email with myself, and downloaded the email attachment.\n\nIt's pretty gnarly. I mean, it has all I need, but how do I actually yoink out the relevant pieces of information. Also notice that the image is just a regular base64 image -- that's pretty nice.\n\n![My options](https://static.404wolf.com/https://static.404wolf.com/Post-20240806173810683.webp)\n\nSo I looked into it a bit.\n\nIt turns out that there's not a ton of good options. I tried three different ones before arriving on [vcf](https://www.npmjs.com/package/vcf), and faced a few issues:\n\n- Wrong type of vCard. There's 4 different versions that aren't fully compatible.\n- Commonjs hell. I'm using [bun](https://bun.sh), which makes life so much easier, (you can import and require in the same file!), but I don't want to require. I want a pure esm solution.\n- Types. I want types. Typescript is a good thing.\n\n![VCF library](https://static.404wolf.com/https://static.404wolf.com/Post-20240806174857068.webp)\n\nSo, `vcf` isn't typed, but it does have type declarations, and works reasonably well. And, it turns out it's wicked fast with `bun`, which is really nice. I don't even have to bundle, just running it as a script is fast with `bun` (but we'll bundle later since I do need to package it with nix, since of course we do).\n\n```js\nvCard {\n  version: \"3.0\",\n  data: {\n    version: Property {\n      _field: \"version\",\n      _data: \"3.0\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    prodid: Property {\n      _field: \"prodid\",\n      _data: \"-//Apple Inc.//iPhone OS 17.5.1//EN\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    n: Property {\n      _field: \"n\",\n      _data: \"Mermelstein;Wolf;;;\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    fn: Property {\n      _field: \"fn\",\n      _data: \"Wolf Mermelstein\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    xPhoneticFirstName: Property {\n      _field: \"xPhoneticFirstName\",\n      _data: \"Wolf\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    email: [\n      [Object ...], [Object ...], [Object ...], [Object ...]\n    ],\n    xAbLabel: [\n      [Object ...], [Object ...], [Object ...], [Object ...], [Object ...]\n    ],\n    tel: [\n      [Object ...], [Object ...]\n    ],\n    note: Property {\n      _field: \"note\",\n      _data: \"This is me.\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    url: Property {\n      group: \"item6\",\n      type: \"pref\",\n      _field: \"url\",\n      _data: \"https://404wolf.com\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    bday: Property {\n      _field: \"bday\",\n      _data: \"XXXX-REDACTED-08\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n    photo: Property {\n      encoding: \"b\",\n      type: \"jpeg\",\n      _field: \"photo\",\n      _data: \"/9j/4An/1/...........S/+gpXY19Bc8RrU/9k=\",\n      is: [Function: is],\n      isEmpty: [Function: isEmpty],\n      clone: [Function: clone],\n      toString: [Function: toString],\n      valueOf: [Function: valueOf],\n      toJSON: [Function: toJSON],\n    },\n  },\n  get: [Function: get],\n  set: [Function: set],\n  add: [Function: add],\n  setProperty: [Function: setProperty],\n  addProperty: [Function: addProperty],\n  parse: [Function: parse],\n  toString: [Function: toString],\n  toJCard: [Function: toJCard],\n  toJSON: [Function: toJSON],\n}\n```\n\nSo we have what we need, and we can interface with it with `typescript` now. It seems everything that we need is there, but it's just a pain in the neck to extract anything.\n\n_some hours later_\n\nI decided to move on anyway, and write a wrapper that uses `vcf` to extract the necessary pieces of information from the `vCard` to craft a `Contact` object.\n\n```json\n{\n  \"name\": {\n    \"first\": \"Wolf\",\n    \"last\": \"Mermelstein\",\n    \"pronunciation\": \"Wolf\"\n  },\n  \"organization\": \"\",\n  \"title\": \"\",\n  \"phones\": [\n    {\n      \"number\": \"(917) XXX-7875\",\n      \"type\": \"Backup\"\n    },\n    {\n      \"number\": \"(929) XXX-7180\",\n      \"type\": \"Misc\"\n    }\n  ],\n  \"emails\": [\n    {\n      \"address\": \"wsm32@case.edu\",\n      \"type\": \"Misc\"\n    },\n    {\n      \"address\": \"wolf.mermelstein@case.edu\",\n      \"type\": \"University Alt\"\n    },\n    {\n      \"address\": \"wsm32@cwru.edu\",\n      \"type\": \"University\"\n    },\n    {\n      \"address\": \"uptothebird@gmail.com\",\n      \"type\": \"Misc\"\n    }\n  ],\n  \"addresses\": [],\n  \"websites\": [\n    {\n      \"url\": \"https://404wolf.com\",\n      \"label\": \"HomePage\"\n    }\n  ],\n  \"birthday\": \"undefined 8, XXXX\",\n  \"notes\": \"This is me.\",\n  \"image\": {\n    \"data\": \"/9j/4AAQSkrwZ/x..........XY19Bc8RrU/9k=\",\n    \"type\": \"jpeg\"\n  }\n}\n```\n\nMuch, much better.\n\nMost of the pain was in dealing with how Apple hacks about the `vCard` spec: for many of the elements in the parsed response I got, I would find that things like phone numbers would be labeled with tags like `label1`, `label2`, and `label3`, corresponding to `personal`, `home`, etc, where the mapping would exist elsewhere in the `vCard`, but not in one consistent spot.\n\nTo solve this, I just created a global hashmap for each `vCard` that had all of the label references and their corresponding proper labels...\n\n```typescript\nconst rawLabels = this.getPropertyJSONs(\"xAbLabel\");\nlet labelMap = [];\nif (rawLabels !== null) {\n  labelMap = Object.fromEntries(\n    rawLabels.map((label: any) => [\n      (label[1] as any).group,\n      sanitizeVCardLabel(label[3]).trim(),\n    ]),\n  );\n}\nconst getLabel = (label: string) => {\n  if (label in labelMap) return labelMap[label];\n  else return labelFallback;\n};\n```\n\nNow that I have a function that can take a contact and give me a pretty JSON, I'm just about ready to start generating markdown.\n\nFor this, I have a few options, but it was a tricky decision.\n\n## Creating Markdown\n\n![The template](https://static.404wolf.com/https://static.404wolf.com/Post-20240806180300351.webp)\n\nOne cool idea I had pretty early on for this project was the ability for the process to be bidirectional. That is, I thought it'd be pretty neat if you could take vCard contacts and turn them into markdown, and then eventually go the other direction: take the markdown you generated, and maybe some hand written contacts that follow the schema, and create vCards that you can load back into your phone.\n\nAlso, it would be important to parse the generated markdown later on just so that I could change the template if I ever wanted to. I love configuring things, and I think it's pretty likely that I would want to make some changes to the template down the line, like moving different sections of the contacts to different spots in the template.\n\nNo matter what I did, I'd probably be using [remark](https://unifiedjs.com/explore/package/remark/) to parse the markdown, which is a plugin for the `unified` AST ecosystem. It lets you create a traversable AST from a plaintext markdown file. I was thinking it might be fun in the future to create my own vCard parser using `unified` in pure `typescript`.\n\nThis was a really tough decision, I was brainstorming and devised a few different options that would be potentially bidirectional\n\nIt turns out that markdown allows you to inject something called \"frontmatter\", which is basically a yaml, at the top of your markdown file, surrounded by `---` on either side. That is, something like\n\n```\n---\nfoo: \"bar\"\ntags: foo, bar, buzz\n---\n```\n\n![Blog post metadata in frontmatter](https://static.404wolf.com/https://static.404wolf.com/Post-20240806180922110.webp)\n\nThis is something I do for the Obsidian plugin for my website; I embed metadata about the blog posts at the top of the blog post files.\n\nOne idea then, using this, would be to pick a template and stick to it, and write a parser that can parse it into a specific format. Then I could add something like \"parser-top-use\" (in more practicality, probably something like \"version\" or \"parserid\"), and then dynamically parse notes based on the schema that they are using.\n\nThis is a pretty high effort idea, so another idea I was flirting with was rendering markdown based on the frontmatter. I found a very straightforward Obsidian plugin that does this called [\"Obsidian handlebars\"](https://github.com/sbquinlan/obsidian-handlebars). [Handlebars](https://handlebarsjs.com/) is a javascript library that lets you compile and then use templates for text files. It's a templating language and is pretty good at what it does.\n\nThe plugin's example is something very similar to what I want to do...\n\n```markdown\n---\ntags:\n  - cool\n  - awesome\n---\n\n\\`\\`\\`handlebars\ntags: {{#each frontmatter.tags}}{{.}}, {{/each}}\n\\`\\`\\`\n```\n\nUnfortunately, however, it's one directional.\n\nI found an Obsidian plugin called [\"Obsidian meta bind\"](https://github.com/mProjectsCode/obsidian-meta-bind-plugin) that does something like this in a bidirectional way.\n\n![Obsidain meta bind](https://static.404wolf.com/https://static.404wolf.com/Post-20240806181431211.webp)\n\nYou can create \"inputs\" in the markdown template, and then when you edit them they automatically update the frontmatter, and then when the user edits the inputs OR the frontmatter it automatically updates the note/frontmatter.\n\nIt works pretty good, but it's fairly janky and requires a lot of boilerplate that I rather not deal with. It also seems a bit fragile, since it is assuming that their inputs work properly long term and don't have any weird plugin interactions.\n\nAnother idea that I had was to very simply just create a vCard viewer/editor for Obsidian. I've wanted to learn [Svelt](https://svelte.dev/) for a while, so I might still do this eventually, but for now, I decided on a simpler solution.\n\n## Obsidian Tables\n\nObsidian tables are pretty good for storing tabulated data, and they are very human readable. It's an accessible format, and remark/unified should be able to parse it easily.\n\n```markdown\n| This is a header | This is another header |\n| ---------------- | ---------------------- |\n| This is a value  | This is another value  |\n```\n\nI decided to go with storing the contact data in tables for now, since it'll be easy to parse them later on. To figure out which sections are which, for now I'll just have headings above the tables that say what is in them, that I can regex against when parsing. It is a bit restrictive to require that people structure their contact templates with markdown tables, but it's better than zero flexability, and it will let me figure out a better solution later on.\n\nThe example template that I came up with, which is very table driven to store the key-value pairs, is this.\n\n```markdown\n---\nobsidian-contact-importer-schema-version: \"1.0\"\ntags: [\"person\"]\n---\n\nImported with [Obsidian Markdown Importer](https://github.com/404Wolf/obsidian-contact-importer)\n\n---\n\n{{#if organization}}\n{{#if title}}\n{{title}} @ {{organization}}\n{{else}}\n{{organization}}\n{{/if}}\n{{else}}\n{{#if title}}\n{{title}}\n{{/if}}\n{{/if}}\n{{#if image}}\n![Image]({{image}})\n\n{{/if}}\n\n## Phones\n\n| Type | Number |\n| :--- | :----- |\n\n{{#each phones}}\n| {{type}} | `c!{{number}}`|\n{{/each}}\n\n## Emails\n\n| Type | Address |\n| :--- | :------ |\n\n{{#each emails}}\n| {{type}} | `c!{{address}}` |\n{{/each}}\n\n## Socials\n\n| Type | Handle |\n| :--- | :----- |\n\n## Links\n\n| Type | URL |\n| :--- | :-- |\n\n{{#each websites}}\n| {{label}} | `g!{{url}}` |\n{{/each}}\n\n## Addresses\n\n| Type | Street | City, State | Zip, Country |\n| :--- | :----- | :---------- | :----------- |\n\n{{#each addresses}}\n| {{type}} | {{street}} | {{city}}, {{state}} | {{zip}} {{country}} |\n{{/each}}\n\n## Other\n\n| Type | Value |\n| :--- | :---- |\n\n{{#if birthday}}\n| Birthday | `g!{{birthday}}` |\n{{/if}}\n| Imported | {{imported.month}} {{imported.day}}, {{imported.year}} |\n\n---\n\n{{notes}}\n```\n\nNotice that I'm using handlebars to finally slot the json values into the template. To make this future proof and allow people to modify it to their liking, instead of remapping all of the handlebar fields in the templator function, I'm just dynamically dumping the JSON into the handlebar...\n\n```typescript\nexport default async function templateMarkdown(\n  contact: ContactType,\n  markdownTemplate: string,\n) {\n  const template = handlebars.compile(markdownTemplate);\n  const now = new Date();\n  return template({\n    ...contact, // Dump the entire contact json as-is\n    image:\n      contact.image === null\n        ? null\n        : `${(await getB64Hash(contact.image.data)).slice(0, 16)}.${contact.image.type}`,\n    imported: {\n      month: getFullMonthName(new Date()),\n      day: now.getDate(),\n      year: now.getFullYear(),\n    },\n  });\n}\n```\n\nAnd it works! So now let's package it and call it a day.\n\n## Bun + Nix\n\nI'm not going to go too deep into Nix here, but the TLDR is that it's a programming langauge and utility library that makes it very easy to create extremely pure and consistent, builds of arbitrary files, including executable.\n\nThe catch with `nix` is that it doesn't just try to be pure, it **forces** purity. Purity is, in this context, that whenever you run a build, the output artifact will be bit for bit identical given the same inputs. This makes it annoying to package things some times, since the `sandbox` that `nix` builds happen in is very restrictive -- it doesn't have internet!\n\nThere's a bunch of utility builder functions that are able to prefetch all your dependencies. A common one is `buildNpmPackage`, which is _de facto_ standard for `nixpkgs` packages (packages on the official registry) that are in `ts/js`. I'm using `bun` though, not `npm` or `node`.\n\nPrefetching artifacts for builds is done by specifying the hash of the thing you're fetching, so that you can guarantee that the things you're fetching are what you think they are. This isn't that bad when you have a `package-lock.json` with `npm` or even with `yarn`. In that case all of the hashes already live there, and can be used to safely do the fetches.\n\nBut, in this case, I want to use `nix` to package a `bun` app. `bun` presents a few challenges; namely, it uses a [special binary lock file format](https://bun.sh/docs/install/lockfile) that's super fast and efficient, but isn't easily parseable. You can use `bun` to read it as a `yarn` lock file, and theoretically you could go down that route to do prefetching of deps, but I decided on a much easier approach: `fixed-point-derivations.`\n\n[Fixed point derivations](https://bmcgee.ie/posts/2023/02/nix-what-are-fixed-output-derivations-and-why-use-them/) are when you ALLOW internet access in the sandbox during the build. This sounds scary, and like a bad idea. It kinda is. The catch is that you specify the hash of the output before doing the build. If the build, using the internet during the build, results in an output with the same hash that you pre-specified, then the build succeeds an it's fine, but otherwise the build fails and you get nothing. In the case of `bun` though, this isn't that bad, since `bun` is very good at getting very consistent dependencies using its lock file. So it's probably good enough for now.\n\nHere's what it looks like in action, as a DIY nix builder utility function...\n\n```nix\n{\n  pkgs,\n  src,\n  name,\n  bun ? pkgs.bun,\n  buildCommand ? \"build\",\n  outputHash,\n  outputHashAlgo ? \"sha256\",\n  outputHashMode ? \"recursive\",\n  ...\n}:\npkgs.stdenv.mkDerivation {\n  inherit\n    name\n    src\n    outputHash\n    outputHashMode\n    outputHashAlgo\n    ;\n  buildInputs = [ bun ];\n  buildPhase = # bash\n    ''\n      bun install\n      bun run ${buildCommand}\n    '';\n  installPhase = # bash\n    ''\n      mkdir -p $out/bin;\n      cp -r ./dist/* $out/bin;\n    '';\n}\n```\n\nBasically, do the `bun install` in the sandbox, do the bundling in the sandbox, and then move the output to an output folder. To make it executable, I just wrap it in a shell script...\n\n```nix\ndefault = pkgs.writeShellScriptBin \"obsidian-contact-importer\" ''\n    ${pkgs.bun}/bin/bun run ${bun-utils.lib.${system}.buildBunPackage {\n      src = ./.;\n      name = \"obsidian-contact-importer\";\n      outputHash = \"sha256-H9hWffy5QUN/n9tgaOO51k92XPJyLQ/bneFRgseCiX0=\";\n    }}/bin/index.js\n'';\n```\n\nWhich just runs bun on the output file. Javascript isn't \"compiled\", so this is the best we're going to get. What this does do though is create a nice bundle, and makes the build nicely portable. It also means that anyone with nix can instantly run the project with a single CLI command (see the next section)!\n\n# Using it Yourself\n\nClone [this](https://github.com/404wolf/obsidian-contact-importer) repo, or create a `inputs` folder with a `vcards.vcf` and `template.md` file (see example template if you're creating your own).\n\nRun `nix run github:404wolf/obsidian-contact-importer` if you have `nix`. If not, make sure that `bun` is installed, and then run `bun install`. Export your contacts to a `.vcf` format, and then just run `bun run dev`.\n\n# @ing People\n\nAnother super cool Obsidian plugin, so cool that it's even baked into the program, is \"[Daily Notes](https://help.obsidian.md/Plugins/Daily+notes)\". Daily notes are exactly what they sound like -- Obsidian automatically creates a new note every day that you can dump information into, and by virtue of the structure they get sorted by date. I find it really useful for very quickly jotting things down, and since they're in my vault they're nicely searchable.\n\nA big inspiration for the project is making it easier to take notes on people, so the ability to make people reference-able in my vault with a single hotkey would rapidly reduce the friction needed to begin taking notes on a person. Since I'm used to pinging people on apps like Slack, Zulip, Discord, etc, I thought it'd be handy if I could just @ my contacts in my vault.\n\nEnter [@ symbol linking.](\u003C@ Symbol Linking](https://github.com/Ebonsignori/obsidian-at-symbol-linking)>).\n\n## Fixing [@ Symbol Linking](https://github.com/Ebonsignori/obsidian-at-symbol-linking)\n\nAn important part of Obsidian is the ability to easily link together your notes. Obsidian lets you reference any note in your vault by using `[[Note Title]]`, and it creates a clickable link. You can add a ! to cause the reference to actually show its contents in the current note.\n\n![Notes prefixed with @](https://static.404wolf.com/https://static.404wolf.com/Post-20240806234120398.webp)\n\n@ Symbol linking wasn't actually the first plugin I found for this purpose, though. I started off using [Obsidian at people](https://github.com/saibotsivad/obsidian-at-people), which claims to do literally exactly what I want. Unfortunately, even though there's a setting for it, all of the people notes you create with it end up in the top level directory of your Obsidian vault, when I want them to end up in a `/People` folder.\n\n@ Symbol linking is a pretty straightforward plugin that does the same thing. It's a bit more generalized -- you can use other symbols as prefixes besides @, and you can customize a few more options.\n\nOne thing that I did like that `Obsidian at people` did was prefixing all of my people notes with \"@\", though, which makes it really clear which notes are \"People\" notes. This was a feature lacking in @ Symbol linking so I[decided to implement it myself and PR the change](https://github.com/Ebonsignori/obsidian-at-symbol-linking/pull/34).\n\n![Example from github 1](https://static.404wolf.com/https://static.404wolf.com/Post-20240806233519583.webp)\n![What it becomes](https://static.404wolf.com/https://static.404wolf.com/Post-20240806233455698.webp)\n\n![Setting I added](https://static.404wolf.com/https://static.404wolf.com/Post-20240806233809359.webp)\n\nTheir plugin was written pretty well, though more comments and docs would've been nice. It mostly boiled down to adding\n\n```typescript\nif (settings.keepTriggerSymbol)\n  filePath = value.obj?.filePath.replace(\n    /([^/]+)$/,\n    `${settings.triggerSymbol}$1`,\n  );\nelse filePath = value.obj?.filePath;\n```\n\nAnd the relevant setting. To polish things off, I also updated the \"Create new note\" suggestion to suggest the name with the @ symbol if the setting was enabled.\n\n# Making it Look Pretty\n\n![Inline admonitions](https://static.404wolf.com/https://static.404wolf.com/Post-20240802211800749.webp)\n![Adding one](https://static.404wolf.com/https://static.404wolf.com/Post-20240802211839755.webp)\n\nTo make the templates look nicer, I decided to use a plugin I already was using in my vault for my contacts too. [Obsidian inline admonitions](https://github.com/scottTomaszewski/obsidian-inline-admonitions) lets you change the way that your inline code (\\` \\\u003Cthis type of thing> \\`) renders so that it can have a nice color and even be slightly translucent. I decided to change the template slightly to incorporate these.\n\n# Next Steps\n\n## Big things\n\n- Bidirectionality! I want to be able to take my note files that contain VCard data and then bring the markdown back into VCard format.\n\n## Smaller things\n\n- VCards are a massive spec. I don't implement nearly close to all of the possible things that they could. I want to implement some more things though, like pronouns and nicknames.\n- That is tested with Apple contacts so far. I want to test to see if this works with Google/other flavors of contacts too.","src/posts/ObsidianContactImporter.mdx","75664fa5640aa2f1","todayilearned",{"id":100,"data":102,"body":108,"filePath":109,"digest":110,"deferredRender":26},{"title":103,"type":9,"date":16,"covers":104,"tags":106,"description":107},"Today I Learned",[105],"https://static.404wolf.com/Post-20240713182452886.webp",[34,20],"A collection of random disconnected things that I learn on given days.","---\n\n# Today I Learned\n\nA collection of random disconnected things that I learn on given days.\n\n# 2024-11-20\n\nLearned about the \"Dreadful Diamond on Derivation\" problem...\n\n[Full credit to this stack overflow](https://stackoverflow.com/questions/21558/in-c-what-is-a-virtual-base-class)\n\nBasically, you have an inheritance hierarchy that looks like\n\n```\n  A\n / \\\nB   C\n \\ /\n  D\n```\n\nThe issue is that if we do `A.D()`, we could be referring to `A::B.D()` or `A::C.D()`.\n\nSo you can do `class A : public virtual B, public C{)` to only inherit the actual methods from `C` to solve this problem.\n\n# 2024-09-26\n\nI set up [neotest](https://github.com/nvim-neotest/neotest) and [nvim-dap](https://github.com/mfussenegger/nvim-dap) and then removed them from my config. Turns out I prefer `vscode` once I'm debugging/testing.\n\nSome `vscode` shortcuts:\n\n- `control+1` focus the text editor group\n- `control+shift+e` toggle focus of the file tree and text editor group\n- \"control+\\`\" focus the terminal, and then toggle it\n\n# 2024-09-25\n\n> In[information theory](https://en.wikipedia.org/wiki/Information_theory \"Information theory\"),[linguistics](https://en.wikipedia.org/wiki/Linguistics \"Linguistics\"), and[computer science](https://en.wikipedia.org/wiki/Computer_science \"Computer science\"), the**Levenshtein distance**is a[string metric](https://en.wikipedia.org/wiki/String_metric \"String metric\")for measuring the difference between two sequences. The Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It is named after Soviet mathematician[Vladimir Levenshtein](https://en.wikipedia.org/wiki/Vladimir_Levenshtein \"Vladimir Levenshtein\"), who defined the metric in 1965.[\\[1\\]](https://en.wikipedia.org/wiki/Levenshtein_distance#cite_note-1)\n\n# 2024-09-18\n\nLearned sql, used \\\u003Chttps://sqlbolt.com\\>. Very helpful tutorial.\n\n# 2024-09-08\n\nNeovim has built in spell checking! No plugins needed\n\nhttps://neovim.io/doc/user/spell.html\n`:setlocal spell spelllang=en_us` enable built in neovim spell checking\n`z=` - see spelling suggestions\n\n# 2024-09-05\n\nYou can use `git mv \u003Cfrom> \u003Cto>` and `git rm [-r] \u003Cthing>` to move and delete things with `git` without consequences if you have things like submodules.\n\n# 2024-08-15\n\nhttps://github.com/mikavilpas/yazi.nvim\n\nMake `dunst` go to the top right\n`origin = \"top-right\";`\n`offset = \"10x10\";`\n\n# 2024-08-15\n\nBash variable parameter extraction + glob stuff\n\n- `${VARIABLE}%ff` removes the first occurrence of `ff` from the back of `VARIABLE`\n- `${VARIABLE}#ff` removes the first occurrence of `ff` from the front of `VARIABLE`\n- `${variable//pattern/replacement}` is a form of parameter expansion that replaces all occurrences of pattern with replacement in the value of variable\n\n# 2024-08-14\n\nOpen source alternative to zoom https://p2p.mirotalk.com/\n\n# 2024-08-09\n\nIn `bash` you can use `**` to get a list of files in the current directory. You can even pattern match with like `**.nix`\n\nSo, like\n\n```bash\nfor file in **; do\n    echo $file\ndone\n```\n\nAnd `file` is the relative path of the file from where you run the command.\n\n# 2024-08-02\n\n`css` has a `not` function to target things that are not the thing. So like, `p:not(.foobar #barbar)` will target everything that is not of class `foobar` and with id `barbar`.\n\n# 2024-08-01\n\nCapital `W` and capital `B` take you forward and backward a word in vim without regard for periods\n\n![WindowsError](https://static.404wolf.com/https://static.404wolf.com/Post-20240801230814573.webp)\n\nApparently `python` has a Windows specific error message\n\n# 2024-07-22\n\nYou can easily get example videos in various formats at \\\u003Chttps://sample-videos.com/\\>\n\n# 2024-07-20\n\n`gm` goes to the center of the current line in `vim`\n\n# 2024-07-16\n\n[Powershell](https://github.com/PowerShell/PowerShell) is open source. You can use Powershell as your default Linux shell. Who knew.\n\n![Powershell as Linux Shell](https://static.404wolf.com/https://static.404wolf.com/Post-20240716222818686.webp)\n\nAlso, Windows has a cool app called \"Sandbox,\" which you can create `xml` config files to template, that are basically Windows virtual machines without bloat (not even the microsoft store). They work really well, I think they should be a distribution of Windows itself.\n\n![Windows sandbox](https://static.404wolf.com/https://static.404wolf.com/Post-20240716223116191.webp)\n\nhttps://learn.microsoft.com/en-us/windows/security/application-security/application-isolation/windows-sandbox/windows-sandbox-overview\n\n# 2024-07-13\n\n- [You can terminate a unified AST traversal early, or skip the iteration early, by returning special things](https://unifiedjs.com/explore/package/unist-util-visit-parents/#what-is-this). If you return `false` the iteration ends. If you return 'skip' then it skips the iteration right away (kinda like `continue` if it were a loop or walker iterator).\n\nUsed this to create this script to yoink a specific section from a markdown string (for example, I want to get the # Description section's paragraphs only)\n\n```ts\nexport function getContentOfSection(\n  markdown: string,\n  header: string,\n  depth: number = 1,\n) {\n  const ast = unified().use(remarkParse).parse(markdown);\n\n  const output: string[] = [];\n  let inHeadersSection = false;\n  let nextIsWantedText = false;\n  visit(ast, (node) => {\n    if (\n      node.type === \"heading\" &&\n      node.depth === depth &&\n      node.children.length > 0 &&\n      node.children[0].type === \"text\" &&\n      node.children[0].value === header\n    )\n      inHeadersSection = true;\n    if (inHeadersSection) {\n      if (nextIsWantedText) {\n        if (node.type === \"text\") output.push(node.value);\n        else if (node.type !== \"paragraph\") return false;\n      }\n      if (node.type === \"paragraph\") nextIsWantedText = true;\n    }\n  });\n  return output.join(\"\\n\");\n}\n```\n\n# 2025-03-31\n\nGoing to try to be more consistent with this again!\n\n```ts\nconsole.trace(\"How did we get here?\");\n```\n\nWill do a `console.log` and also spit out a stack trace\n\n# 2025-04-03\n\n![](https://static.404wolf.com/https://static.404wolf.com/73229a01-0bf5-430d-b119-7fab37d48aaf.webp)","src/posts/TodayIlearned.mdx","1f4441df1a71ba70","vealesnyderreflection",{"id":111,"data":113,"body":119,"filePath":120,"digest":121,"deferredRender":26},{"title":114,"type":9,"date":16,"covers":115,"tags":117,"description":118},"Veale Snyder Fellowship Reflection",[116],"https://static.404wolf.com/coverFountainVeale, VealeSnyderReflection-20241025150445254.webp, Pasted image 20241025041749.png, Pasted image 20241025035853.png",[21],"As part of CWRU's Veale Snyder entrepreneurial fellowship, I had the incredible opportunity to travel to San Francisco and meet with executives and engineers from various companies, including Fountain, NVIDIA, Rivian, Google, Y Combinator, and Giant. Throughout the trip, I gained valuable insights into entrepreneurship, company culture, and personal growth, which I will carry with me on my entrepreneurial journey. This eye-opening experience has not only expanded my network but also provided me with invaluable lessons that I look forward to applying to my future endeavors.\n","# Veale Snyder Fellowship\n\n## What is it?\n\nThe [Veale Snyder Fellowship](https://case.edu/entrepreneurship/fellowships/veale-snyder-fellowship-program) is a 1 year entrepreneurship fellowship program at [Case Western](https://case.edu/). It's a two part program directed by [Michael Goldberg](https://www.linkedin.com/in/mgoldberg2) and [Stacey Lotz](https://www.linkedin.com/in/stacey-lotz-982484162/), where in the fall semester there's a 1 credit course taught by [Jose Dias Salazar](https://www.linkedin.com/in/josediazs) and a trip to San Francisco to meet various companies in SF and Silicone Valley, along with many Case alum, VCs, and entrepreneurs, and then in spring a 3 credit course and international trip to the Czech Republic.\n\n## Why me?\n\nIf you read my blog, you probably already know that I'm a very technical person. I'm interested in software, computers, webdev/fullstack, devtools, devops/linux/nix, and a plethora of other techy things. I love reading about shiny new typescript frameworks, and how fixed points work in nix. What am I doing in an entrepreneurship program?\n\nIn my application, I stated this well and succinctly,\n\n> Over the years, I've arrived at an ultimate interest in entrepreneurial webdev. I love the interconnectedness and accessibility: how it enables rapid contrivance of genuinely useful applications. I have always found realizing my own project ideas to not just be the best way for me to learn, but also to be most satisfying and fun. My favorite thing in this world is learning things that I want to learn, because I want to learn them. I see impatience as a virtue, a quest for speed and efficiency that naturally harmonizes with my field of interest. In the end, I feel pretty strongly that, eventually, entrepreneurship will be a big part of my future vocational journey.\n\nThis is pretty consistent with my current thoughts on why I'm here now. The only realization I've had since is how my area of focus has shifted more specifically for creating things to solve the problems that I myself face, which there's hints of in my previous thoughts -- I'm most interested in using fullstack technologies that I love using to alleviate developer experience (devx) friction. I want to make the world (for developers) a better place, so they can make the world (in general) one.\n\n# San Francisco Trek\n\nOne of the hallmarks of this program is a trip to the bay area between the 20th to the afternoon of the 22nd of October (2024). It's a very packed two days, where we get to experience entrepreneurship in SF/the valley. It was a really, really fantastic, eye opening experience -- I got to meet so many people and see so many different company cultures and philosophies in such a short time span.\n\nThe other fellows in the fellowship --\n[Salma](https://www.linkedin.com/in/salmabhar/?lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_all%3BCEhhZxHjTLuqlquuMozLsQ%3D%3D), [Alvisa](https://www.linkedin.com/in/alvisakrasniqi), [Ariella](https://www.linkedin.com/in/wayzaro-ariella-yve-taylor-450956217), [Apeksha](https://www.linkedin.com/in/apeksha-malik-125a9a2b), [Sreya](https://www.linkedin.com/in/sreya-srinidhi), [Vinlaw](https://www.linkedin.com/in/vinlaw-mudehwe), [Kaleb](https://www.linkedin.com/in/kaleb-k), [Adam](https://www.linkedin.com/in/adam-hamdan), [RV](https://www.linkedin.com/in/rvyadav), [Alessandro](https://www.linkedin.com/in/alessandro-mason-117417260), and [Vidyut](https://www.linkedin.com/in/vidyutveedgav)\nwere amazing to be around for the trip! One of the coolest parts of the fellowship is how we stick around with the same group of people and all get to know each other. It was a really cool coincidence that most of us are CS people with technical interests; I've managed to sneak in some talk of flutter and system design with Allesandro, or system design, graph generation, and AWS lambdas with Vidyut!\n\nMany other fellows [have](https://www.linkedin.com/posts/salmabhar_this-fall-break-i-had-the-incredible-opportunity-activity-7254647616276226048-Yvta) [already](https://www.linkedin.com/posts/apeksha-malik-125a9a2b_over-the-past-semester-the-team-at-cwru-veale-activity-7255431533707030529-0SS8) [posted](https://www.linkedin.com/posts/rvyadav_vealesnyderfellowship-cwru-techinnovation-activity-7254613603188645888-6Zjx) really great reflections with awesome comprehensive overviews of everything that we did during the trip, so I want to focus on my takeaways, but I'll keep it sequential. This is my first (but definitely not my last) non-technical blog!\n\n## The trip\n\nSunday was more about experiencing SF. We rode in [Waymo](https://waymo.com/) self-driving cars, which was an incredible experience. There was a lot of talk about self driving cars across almost all the companies we visited; it's a trend that seems very well accepted. It seems like, for companies with the resources, there's a pretty big pressure, perhaps out of expectation, that they invest in \"trends\" that _could_ be the future.\n\n## Companies\n\n### Fountain\n\n![Visiting Fountain](https://static.404wolf.com/20241025035853.png)\n\nOn Monday, we started out by heading over to [Fountain](https://www.linkedin.com/company/fountaininc), a software company for recruiting and optimizing hourly wage employee retention. We spoke with [Sean Behr](https://www.linkedin.com/in/seanbehr) about his entrepreneurial philosophies.\n\nWe'd already met Sean virtually during our first class. I'd say his unique takes were that:\n\n- Speed is key. The faster you get a product out the door, the faster you can know to kill it (or accelerate it).\n- Adapting is necessary. The product isn't going to be a masterpiece unless you meld it into one.\n\nEntering his office, it was amazing to see how many people had migrated to work remotely. Honestly, it was a bit sad to see -- I'm a big proponent of pair programming and collaborative workflows, but it seems like it's a dying desire. Sean seemed to agree in the importance of in person collaboration (and cost of lack thereof).\n\nThis time, there was a bit more focus on company culture, and company/system dynamics.\n\n- Culture\n  - Culture is largely immutable. It is also in part a function of company success. Extreme success fosters positive culture. So, CEOs steer for economic success to drive culture.\n- Adaptability\n  - There's _discrete_ categories of success, and it's really, really hard to traverse up the latter. You _need_ to be able to adapt to climb in the long term. But, sometimes it's just not possible to adapt fast enough, and you can't win.\n- Organization\n  - Clarity is very important. Every piece of data _could_ be important, any abstraction on the raw datapoints is lossy.\n  - Having some sort of project management is really important. Set goals are good.\n- Market Research\n  - Connecting with clients and asking about their needs is the best way to improve. Directly mentioning the product induces bias, so sometimes it's best to ask surrounding questions.\n\nI really like Sean's focus on efficiency. I think his thoughts on culture are very valid, but that \"good\" culture doesn't _require_ vast success, just that the success is a way to induce culture. This is also why it's so critical to choose an awesome co-founder and founding team to establish a good culture from the start.\n\nSean believes that some traits are critical for a successful culture. I think he's right, but I also feel these traits are in flux with the state of the company.\n\nHe honors speed and responsibility, and I think these are really really important early on, but less so as the company evolves. By reducing responsibility and honoring exploration, larger companies we visited like Google and NVIDIA have been able to accomplish insane things.\n\nFinally, there seemed to be an emphasis on _identification_ and explicitness, on the \"tangibility\" of goals, values, ideas, and pretty much everything he values and talked about. Writing things down is really, really important. It makes them real.\n\n### NVIDIA\n\n![Nvidia \"Fireside chat\"](https://static.404wolf.com/20241025041604.png)\n![HQ Tour](https://static.404wolf.com/20241025041624.png)\n\nNvidia was an entirely different experience than Fountain. Walking into their headquarters was like walking into an airport. You could tell that if you were to talk to anyone, they'd have a really interesting story to tell, but everyone seemed to be laser focused on their own chunks of work.\n\n[Sabrina Trans](https://www.linkedin.com/in/sabrinatrans/?lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_all%3BrCNagRURT%2FiAl%2BHW861hEg%3D%3D) gave us a great tour of the facility -- it was massive; the outdoor area and open spaces were really, really nice.\n\nWe got a brief presentation about their products, which seemed a bit abstract to me. What it did show me though is that their demographic is a much wider net than I would have anticipated: they design tooling for a substantial audience -- yes, software engineers, but also datacenter designers, car companies, and many others.\n\nWhat was most important was the \"fireside\" chat afterwards; an open Q\\&A.\n\nWe met with [Kevin Kranzusch](https://www.linkedin.com/in/kevinkranzusch/), one of the first employees at NVIDIA, [Nick Triantos](https://www.linkedin.com/in/triantos/), VP of software, and [Sarah Yurick](https://www.linkedin.com/in/sarahyurick/), a recent case grad now working on [NeMo](https://www.nvidia.com/en-us/ai-data-science/products/nemo/), and a few others.\n\nSome key points from NVIDIA:\n\n- Their culture has **not** really changed over time. They really value intellectual honesty. I remember back to when I was starting my first batch at [Recurse Center](https://recurse.com), their introduction stresses the importance of being \"kind\" instead of \"nice,\" a similar idea: being honest and upfront over obscurative and always reinforcing.\n- Being well rounded is important. Kevin talked about how Nvidia hasn't really had major \"Pivot\"s, but that they do have to adapt. Something unique about a company their size is that they can invest in R\\&D in speculative fields to try to keep ahead of competition.\n- The best types of industries to go into are the ones with \"insatiable\" desire. Easier said than done.\n\nTheir actual work seemed to be very sharded. Different people we spoke to would work on totally different things, and not know much about what others were working on. I think that, while they value ideas, it seems like just by how their company flows, this \"segmentation\" may make it hard to contribute substantially as any given employee.\n\nOn the other hand, they seem to treat their employees really well. They talked about how there's a lot of respect for ideas and asking for help, which is always good to hear. Because of the specificity of what employees work on, it also seems to be the case that everyone is really interested in what they're working on. They are for sure solving some revolutionary technical problems.\n\n### Rivian\n\n![Rivian Presentation](https://static.404wolf.com/20241025041716.png)\n\nBetween Nvidia and Google, we visited [Rivian](https://www.linkedin.com/company/rivian), another totally different experience.\n\nAt Rivian we got a presentation on the EV industry, what the sorts of products they work on are, and an overview of the challenges they are facing and how they want to grow in the future.\n\nI found a few things neat about Rivian\n\n- The idea that you sell an \"experience\" with the car. This sort of \"buying into an experience\" idea is something I've come across a lot over recent years. I don't totally buy the value that turning conventional goods into \"experiences\" adds. But I understand the idea.\n- Their CI/CD process sounded very technically interesting. They're automatically pushing software updates to cars, so they have to have a pretty complex testing setup, and they talked about how they have stripped down hardware specifically for testing. I wish I could've asked more questions about their integration testing.\n- It's helpful to break down problems into specific concrete steps you can take to solve them.\n\nI wish we had more time to chat, but we were crunched on time since they had a relatively long technical presentation. I think that the most valuable insights have come from the loose conversations we've had during the trip.\n\nI'm still really curious how it's possible Rivian is able to maintain such low (negative) profit margins and operate at such a high loss, it seems like a dangerous recipe that surely must be intentional. It's an interesting stance on the side of the value of speed. The auto industry is definitely very capital intensive, and I feel like that fact can induce inefficiency.\n\n### Google\n\n![Google discussion](https://static.404wolf.com/20241025041749.png)\n\nAt first, I expected Google to share some resemblance with NVDIA. It's a massive company, with a ton of employees, and a ton of projects, too. They're both leaders in their industries.\n\nBut Google's really, really, different.\n\nJust as we walked in to Google, the aesthetic was extremely eccentric, with unique architecture and bright Google colors.\n\nWe met with [Gopi Kallayil](https://www.linkedin.com/in/gopikallayil/), chief business strategist and AI evangelist, and [TT Ramgopal](https://www.linkedin.com/in/ramgopal/), head of android relations.\n\nI was particularly excited to talk to TT about Android, but we were really crammed for time and I decided it best not to get bogged down in a super technical discussion. I'll have to reach out on linked in to begin the discussion on android devtools! My big takeaway from what TT had to say was how rampant competition is at all levels, even at the scales of Google; the importance of looking into the future to predict the battles of the future.\n\nGopi had a really great personality, and the passion you could feel he had was really incredible. He talked about Google AI, and how _vast_/general its potential may be. What's unique about Google is how broad their impact is, because of how massive their audience is. I think that's pretty cool.\n\nWhat Gopi said that I more personally resonate with, and I've now been much more conscious of is battling the tendency to complete other people's sentences. Not necessarily cutting people off, but overcoming the inclination to \"autocomplete\" people's train of thought, and then losing attention of the actual things they're communicating. To be honest, I definitely feel I sometimes tend to do this, and I think that being aware of it and more deeply listening is great general advice.\n\nI think one of the unique things about Google is how empowering the culture is. This is something that I've really never sensed to the degree I did at Google, and is probably why I know so many Recursers who were also ex-Googlers. It seems like Google encourages employees to ideate and explore beyond their specific area of work. There's some tangible ways that they mentioned they do this.\n\nI fear working at \"mega\" companies because I'm worried that I'll be trapped into doing work that I have little control over, and that my impact will be necessarily small in scope of the work that the company does, but Google seems to truly care about what areas you are interested in, so you can focus your energy on what matters.\n\n### Y-Combinator\n\n![Discussion](https://static.404wolf.com/20241025041821.png)\n![Space tour](https://static.404wolf.com/20241025041842.png)\n\nOn Tuesday, we visited Y Combinator and got to hear from [Pete Koomen](https://www.linkedin.com/in/petekoomen/) about his journey.\n\nPete was one of the original founders of [optimizely](https://www.optimizely.com), an A/B testing platform designed to be easy to use even for non developers to set up tests.\n\nThis is in a similar arena to the sorts of things I'm interested in, and I found it really interesting to hear about his journey. He was also inspired to start the company out of frustration with problems that he'd experienced as an engineer.\n\nWhat's cool about his role at YC is that he gets to experience working with a lot of different start ups all at once.\n\nSome themes were\n\n- What makes a good founder. Being very resourceful, learning things quickly, and never making the same mistake twice.\n- Running towards risk. Finding hard problems and solving them. This isn't necessarily required for success, but it seems to enable much vaster success.\n- Try to find customers as early as possible. Sean at Fountain also noted this, and I definitely see the value.\n\nI agreed with most of what he had to say. There's a pattern I've seen throughout the trip on how heavily to fund ventures. There seems to not be a clear cut answer -- some people, perhaps particularly YC, feel that it's important for there to be an aspect of scarcity; by not having huge amounts of funding you use what you have much more efficiently. On the other hand, companies like Rivian take a totally opposite approach, and raise substantial amounts of money to try to grow very quickly. My thought is that you _can_ put a price on speed, but sometimes it's not a price worth paying.\n\n### Giant\n\n![John @ Giant](https://static.404wolf.com/https://static.404wolf.com/VealeSnyderReflection-20241025172611110.webp)\n\nFinally, we met with [John Kobs](https://www.linkedin.com/in/johndkobs/). At most of the places that we visited, we would give brief 2 minute introductions about ourselves, but with John, he told us to \"talk about our most recent failure.\"\n\nI found this meaningful, because it got us to _think_ about why we failed and how we could prevent it from happening again. Being conscientious about it seemed key. I recalled how a few weeks ago I feel I did poorly in a Go debugging interview because I entered it with the wrong mindset and didn't fully _analyze the situation_: they wanted to see how I understood the adjacent concept to the thing I was debugging, but I was laser focused on getting the test to pass.\n\nHe went on to lead an extremely insightful Q\\&A.\n\nThe biggest takeaways:\n\n- Learning from mistakes. Never making the same mistake twice seems like a generally good piece of advice.\n- Recognizing bottlenecks, but transitioning carefull. He created what became Apartments List. He talked about how he saw how being an aggregator of aggregators was a bottleneck to scale, and how they needed to pivot to work with brokers themselves. This pivot initially was extremely painful, beyond expectation.\n- Being open. This is something that came up at Google too. Instead of being overly prescriptive, listening to others and thinking through things without the underlying assumption that there's nothing left to figure out or learn.\n\n## Chats\n\nThere were two networking events during the trip. There, there were a bunch of Case alums and people affiliated with Case.\n\nDuring the second event, I got to speak to [Donald Knuth](https://en.wikipedia.org/wiki/Donald_Knuth) himself; the creator of Tex and writer of 30+ books, including _The Art Of Computer Programming_. The chat reminded me a bit of some interviews with Linux Turvals that I watched a few years back. There's the debate between whether the right way to delve into CS is top down or bottom up; whether to start with high level languages and the ergonomics of abstraction, or to jump into registers, boolean algebra, and C. I personally don't think that it's a one size fits all topic, but Donald talked about how he think's its imperative that you learn how things work, and how abstractions can quickly become very dangerous. I think there's value to that train of thought. By the time I got to ask him about his thoughts on [typst](http://typst.app), a Tex competitor, it was clear what his opinion would be -- its ergonomics are a dangerous disguise. But I think it's fine for the application. Another really important topic was writing -- we both agree that it's really important to write about what you do and think. This includes non technical writing, like this post!\n\nIn the second networking session of the trip, I got to chat with [Stephen Hahn](https://www.linkedin.com/in/schahn/). We had some great technical conversation, and he talked about how he'd avoided tool related start ups (probably my most specific interest). When I asked why, it was pretty straightforward: it's hard to sell tools. Big companies will often release them for free and crush you, or it is just technically difficult to provide them in a way that requires payment. My interest is in bridging fullstack with developer tooling, and he seemed to agree that the idea of web based tooling is an interesting new category that may enable tooling related startups that may previously not have been possible.\n\nI had a ton of other really great chats, and I'll try to post some more reflections here over the next few days.\n\n# Reflections\n\n![](https://static.404wolf.com/https://static.404wolf.com/VealeSnyderReflection-20241025150445254.webp)\n\nThroughout the trip, I really felt the value of time. Being able to pack questions into such short sessions, and make meaningful connections with the execs and engineers that we met, seemed almost impossible. Time in general seems like one of the biggest limiting factors, and in general I tend to agree with Sean in how vital efficiency is.\n\nOne important insight was how it's important to be able to adapt to and embrace change rapidly. Speed is imperative, but sometimes overpriced. Efficiency is important too. There's a lingering question of how important it is to balance speed and quality, and being careful not to over complicate things (something I, and other engineers are particularly vulnerable to), is something I want to be more attentive to.\n\n\"Good\" culture seems like it can make or break a company, but it can often be largely immutable. Some companies value efficiency, some intellectual honesty, some curiosity, etc. I really liked the culture [Google](https://www.linkedin.com/feed/#) was targeting in particular; even in such a large company, they care that you contribute to specific things you care about. The power of \"passion\" for your work is massive.\n\nAlso, being open-minded and carefully considering others' ideas is always important. Specifically, deeply considering everyone's ideas, and, in the context of startups, close contact with clients. I found [Sean Behr](https://www.linkedin.com/feed/#)'s thoughts on the danger of data abstraction inspiring -- charts and summaries can mask critical insights; anecdotes are often most valuable.\n\nThe trip also placed a huge emphasis on the power of networking. I think sometimes networking takes on too formal a connotation -- but really, it's all about meeting and then staying in touch with an ever increasing amount of awesome, interesting people. I've always found that the most powerful \"network\" links are bidirectional, and in that way it's like making friends.\n\nMichael Goldberg is a big proponent of LinkedIn as a powerful networking tool, and it's absolutely true that I'll be able to stay in touch with a lot of the people that I met over the trip there. It's a really great tool. But I think there's a pretty significant personal aspect that's lost. It's an amazing tool to connect with and organize a network. But I think that LinkedIn is, perhaps reasonably so, somewhat superficial. It feels a bit out of place to post a real, thoughtful digest of things like this trip, in the form of a LinkedIn post, because of the context that it's placed in. Hopefully my insights here are useful and interesting. I'd love to chat further if you have any additional comments or thoughts! Email me at wolf@404wolf.com, or 212/767/9653.\n\nI cannot thank [CWRU Veale Institute for Entrepreneurship](https://www.linkedin.com/feed/#), [Michael Goldberg](https://www.linkedin.com/feed/#), [Stacey Lotz](https://www.linkedin.com/feed/#), and many others enough for making this opportunity possible! I've learned a ton, met so many interesting people, and have got to further connect with the other fellows!\n\n# Czech Republic Trip\n\n![Brno](https://static.404wolf.com/4be0999f-0483-4679-b0e1-dfb7c02451b6.webp)\n![Brno](https://static.404wolf.com/dc7621e3-95f4-4750-bfe0-4d785749bdc7.webp)\n![Kuku clock](https://static.404wolf.com/4a54acd9-f393-434b-aa38-75a90752fb59.webp)\n![Castle](https://static.404wolf.com/https://static.404wolf.com/c567ad77-0fbe-499a-b056-1d7c11159967.webp)\n\nThe second major part of the Veale Snyder fellowship is an international trip, with the same group as the first trip. For my year, we visited the Czech Republic (including a day in Austria)! We went on a lot of walking tours -- of Vienna, Brono, and Prague. It was in some ways a really unique and different experience, but in others bared many similarities to places in the US.\n\nWe went to Brno because our professor thought that Brno has various similarities to Cleveland. I think this is true in some ways, but I wouldn't really draw the comparison. I didn't find Brno very dense, it felt somewhat like busier towns in NJ -- not really a city, but also not really suburban. My favorite part of the trip was definitely visiting Prague, I think it bore many more similarities to NYC than Cleveland to Brno.\n\nThe second half of the program is lead by Michael Hill, former Medtronic CTO. What I like about Michael is that all of his advice is very practical, and his answers and thoughts are very to the point. Michael's shared a lot of insights, but there's some that particularly have stood out to me\n\n- During one of our lunches Michael talked about his decision to stay at Medtronic so long (almost 30 years!), and it boiled down to loving your boss, and aligning with your companies interests. These things sound obvious, and I agree with them -- but it is powerful to be able to pinpoint it directly.\n- It's really important to keep focused: on a specific audience, on your company's trajectory, mission, etc. Being clear and strategic is critical to success.\n\n## What makes an Innovator?\n\nThe Veale Snyder program for the second half of the semester is taught as a 3 credit course. The course Syllabus leaves a few ongoing questions for us to think about:\n\n1. What is innovation? What does it mean to be an innovator?\n2. What is entrepreneurship? What does it mean to be an entrepreneur?\n3. What is a \"value proposition\" and why does it matter?\n4. What mindset, skills, and tools are required for success in innovation (to be an entrepreneur)?\n\nAnd then, after our trip, asks us to reflect on the first question, \"What makes an innovator?\"\n\nIt's a subjective, multifaceted question. So here's the evolution of my thoughts on it--\n\nBefore the trip, we read _The Medici Effect_: \"what elephants and epidemics can teach us about innovation.\" I'd say it sums up pretty easily:\n\n- The _Intersection_ is a unique space that's created only when people of diverse backgrounds and interests, are brought together over a thread of commonality.\n- When you have an _Intersection_, you can achieve _The Medici Effect_. _The Medici Effect_ is named after the Medici family from Florence, a wealthy banking family who played a significant role in funding the Renaissance by funding a large variety of very different creative artisans. It describes the big results you can get by fostering *Intersection*s.\n\nWe also read _The Innovator Mindset_\nSome takeaways:\n\n- All products have target customers, and the target customer is never \"everyone.\" Identifying the people you're serving is critical to success.\n\n> Innovation is about people and their assumptions and subconscious thought patterns (a.k.a. their mindset) and their daily actions and habits that stem from that mindset (a.k.a. their behavior). Put all those together, add some procedures, rewards and penalties, social dynamics, unspoken rulesand a pinch of stressand you get a wonderfully messy, organic, and complex environment. An environment in which behavior, not lip service (although words are also important), drives the results. If you fail to address that daily behavior, even the greatest strategy and plan to drive innovation are doomed to fail.\n\n[Recurse Center](https://recurse.com) for me has one of the most impactful experiences of my life because, I think, of how it neatly conforms to the _Medici Effect_'s _Intersection_. The entire RC space is a carefully curated environment of serendipity and diversity. Theres a large variety of people from all different backgrounds. I've done pair programming with ML engineers, Art Historians, quant developers, and more. And, there's a closed space where everyone is packed together. This exposes you to so many different projects and thought processes; when directly pair programming with people, during lunch discussions, or group meetings.\n\nCreating a space of innovation to drive entrepreneurship I think requires a lot of similar components, which Czechia is trying to cultivate. It's a difficult process, though, because, as we've talked about a lot in our class, mindset is one of the most important factors. You can have a very accessible entrepreneurial environment (creating serendipity, providing spaces and resources, general access to funding, etc), which is one side of the puzzle and something that they haven't nailed yet, but there's also a component of convincing people that it's worth it in the first place. If people are content with work, like what they do, and are happy with where the amount they earn gets them, it is difficult to convince them to channel their ideas into startups -- it's a risk that people aren't internally justifying taking.\n\n### BIT Entrepreneurial Panel\n\nA panel of speakers answering the question \"Does Brno region need an Entrepreneurial Ecosystem?\"\n\nAt first, I'd thought that this was a rhetorical question, but the more we focused on it the more it seems that it's really a matter goals. When asked why Brno might want Entrepreneurship, the answer seemed to mostly be about freedom and self dependence, largely aligned with American values. The appeal seems to be largely that entrepreneurship can provide the city with more security and independence. Brno's economy and workforce is largely dependent on electron microscopes. We toured ThermoFisher Scientific's manufacturing facility, and the level of sophistication and and intelligence there is really incredible. ThermoFisher makes up a disproportionate share of Brno's GDP, and so it makes sense that they would want to foster entrepreneurship to diversify.\n\nDuring the trip we got to interact with some local students at BUT's entrepreeneurship school, and got to speak with them about what their educational experience has been. We also visited BUT's maker lab, and Strojlab, their maker space. Czech academics seems much more rigid -- there's much less time spent outside of classes, and there's not real choice over the classes you take. There's a lot of similar factors--Brno is somewhat of a \"college town,\" with about 20% of the population being students, and apparently it was common that students would go home for the weekend. I think this more traditional style of instruction provides more consistency, but that that's not necessarily a good thing.\n\nWe also visited Charles University, to learn about how they support entrepreneurship, and it seemed their focus was also largely environment, and their approaches were also more traditional; a lot of importance was placed on creating serendipitous spaces, networking opportunities, and preserving a specific reputation/brand. The difficulty seemed similar to that of many of the other places we visited: trying to motivate students to actually care about entrepreneurship. It seemed like a lot of people were more focused on academics for the sake of academics and thought of vocation interests as just that -- the students we spoke to had interesting ideas and were very interesting to learn from, but I felt they had a less clear sense of their \"passion\"s, and were more focused on *path*s.\n\n### Czech Invest\n\nDuring our trip, we went to a \"Czech Invest\" presentation at [The American Center](https://www.americkecentrum.cz/en/) at the US embassy. My takeaways:\n\n- There's a lot of legal challenges that Czech startups face. You can't really pay people in equity, and there's weird unfavorable tax intensives\n- Entrepreneurship isn't something baked into the culture, and it's not really taught very often (although that seems like it's slowly changing) (and, adding on, change in general seems relatively slow)\n- The intensives are placed in ways that are more favorable to larger businesses and less so to start ups\n- One big inhibitor seems like the \"failure is bad\" mindset. People don't want to make mistakes, so they don't take as many risks. Growing up, I've always been told to make mistakes to learn from them, but I don't think that's something that's stressed in Czechia.\n\n### JIC Entrepreneurship Center\n\n![](https://static.404wolf.com/VealeSnyderReflection20250328021818169.webp)\n\nAt JIC we met Czech entrepreneur Libor Horeni who shared his experiences with us. He talked about starting with his first company [Top Recepty.cz](http://toprecepty.cz/), sharing how he was inspired to create the website while in high school, from looking through most visited websites in Czechia and trying to find areas where he could make improvements. He found a recipe sharing website that got a huge amount of traffic, and thought that he could recreate it better. After Top Recepty, he talked about a company like (America's) Too Good to Go -- a company that reduces food waste by connecting restaurants with customers to pick up their leftover food at a greatly reduced cost at the end of the day.\n\nWhat I think made Libor such an effective entrepreneur was not just his true passion for what he does -- he admitted that he doesn't really cook himself -- but the process. It seems like there's a lot of different factors that draw people to entrepreneurship, and it can vary a lot person to person. In SF, it seemed like the primary motivators were value creation, and money. But in Czechia it was quite different -- in general the people innovating were more concerned on the common good and improving their community, going down paths that they wanted to go down. I get the sense that many less \"I have no idea where to start\" product ideas come to fruition because people want to reapply what they do to new contexts for different reasons.\n\nThis is certainty not a _bad_ thing per se, but it fosters a different type of entrepreneurship. Libor talked about how in growing Top Recepty, he reached out to a lot of Czech grandmothers, trying to curate blog posts; he talked about how he got companies to sponsor him with pots to give away for competitions to grow his platform. Here, I don't think people would feel the need to ask for permission to repost recipes, and people would be more inclined to pursue more direct forms of marketing.\n\nHis general model seemed to be to find existing products, and refine them to compete, leveraging their niche market in Czechia. That strategy seemed very effective in Czechia, but I feel that in the US that there's not many _tech_ ideas left to pull in from abroad and refine, and that most of the products we already have already are well refined; more importance is placed on original ideas.\n\n![Entrepreneurship Roundtable](https://static.404wolf.com/https://static.404wolf.com/e23c532d-7d7a-4203-aeb6-dd08b8733fb5.webp)\n\nWe had a roundtable event at Brno University of Tech (BUT) with Michael Goldberg and a a variety of experts on whether and why the South Moravian region needs more entrepreneurship. I love playing devils advocate, and I was curious at trying to grasp exactly why all of these experts seemed to agree that the answer was yes\n\n- America is often idolized for entrepreneurial success, and it seems like this strongly affects entrepreneurial sentiment abroad. they seemed to have a strong association between entrepreneurship and _freedom_. There's a fear that in Czechia (and even more so Europe as a whole), people are significantly more dependent on the government than in the US. One of the perks that Czechia officials consistently mentioned is the general political and economic stability of the country. By fostering entrepreneurship, Czechia fosters more self sufficiency.\n\nI think it's important to note though, that, while it does seem like Czechia struggles to foster an environment that can cultivate entrepreneurial mindsets and actually produce global startups, I don't that needs to be a \"bad\" thing; it just isn't a primary focus. Prague was still a beautiful and very energetic (and historical) city with what felt like more of an industrial focus than, say, NYC. In lue of the massive entrepreneurial focus of big US cities, there was more of a focus on everyday quality of life.","src/posts/VealeSnyderReflection.mdx","af796e312942ceb5","whirlwindtourofnix",{"id":122,"data":124,"body":130,"filePath":131,"digest":132,"deferredRender":26},{"title":125,"type":9,"date":16,"covers":126,"tags":128,"description":129},"Tour of Nix",[127],"https://static.404wolf.com/52042908-6c60-4f88-9880-22b486c7242c_0001.png",[34],"An introduction to Nix and its ecosystem. The basics of Nix as a language, its use as a package manager, build tool, and much more. Some brief discussion of core concepts like derivations and flakes, and a walkthrough of simple examples packaging programs. Learn how Nix can help create reproducible builds and consistent development environments, and do fun things like allow bash scripting with any binary, or making pure latex documents. Post is still in progress.\n","Okay, before we get started:\n\nNO, NIX IS NOT JUST AN OPERATING SYSTEM. I'm going to talk about what it is and what you can use it for, but `nixos` is merely a project that uses the `nix` language/tooling. You do not need to even use `linux` to use `nix`. Okay, now that we have that out of the way let's get started.\n\n# Why Nix?\n\nBefore even grappling with what it is, I think it's good to understand what it can do. Some awesome things that nix does:\n\n- Consistent developer environments that are decoratively specified. This means everyone working on a project can have the same LSP, binaries in their $PATH, etc.\n- Pure, reproducible project builds. Nix \"wraps\" onto tooling you already use for builds, and does the build in a sandbox. This means that there's no internet access, the time is set to `0` (UTC) (which means things like Latex \"builds\", which inject timestamps, will be reproducible too!), and more.\n- Let you configure your home directory using a project called \"home manager.\" You can specify how to set up various programs and their configurations fully declarativly with `nix` code.\n- Create very minimal docker images, that don't have a `FROM` (that are `FROM SCRATCH`), and only have exactly the things you need to dockerize your project.\n- Get an android emulator going in 4 lines of code.\n  And a million other things.\n\nWhere nix really shines is reproduability. If it works once, it will probably work again.\n\n# Some Terms\n\nWhat is **purity?**\nPurity just means when you put X, Y, and Z in, you'll get W out. If you put X, Y, and Z in two weeks later, on a Mac, in a different time zone, you'll get W out. If you install a different C compiler, you'll still get W out. Nix can guarantee that your builds are pure.\n\nWhat is **lazy**?\nLazy just means that the language won't try to evaluate anything it does not absolutely need to evaluate. If you import every package in existence, `nixpkgs`, `nix` will not literally build everything. But if you reference `foobar` from `nixpkgs` then it will build it.\n\n# What is [Nix](https://nixos.org/)?\n\nThe question with so many answers. Nix is at least 4 totally different things.\n\n![So many nix packages!](map_repo_size_fresh_0001.svg)\n\n1. It's a programming language. It's functional, lazy, and can be pure. You can do basic things like `{ a, b }: a + b`! You can also use it to do more advanced tasks like mapping over arrays or attrsets (dictionaries), and everything you'd want from a programming language.\n2. It's a [package manager](https://search.nixos.org/)! The nix repository has more packages than any other package manager (`apt`, `pacman`, etc), and more fresh (new) packages too! All the packages are not actual binary blobs, but `nix` code (remember, it's a language!) that defines build instructions for producing `derivations` (more on this later) containing over 100k different projects. This even includes pure build instructions for things like building `chrome` from source and packaged patched binaries like jetbrains IDEs (ikk). Your packages are all nicely managed in a `nix store`, again, more on this later.\n3. It's a utility library. `nixpkgs` is a library that contains a bunch of utilities that let you do super cool things, like defining pure builds for minimal `FROM SCRATCH` docker images. It fixes all the bloat of `docker`!\n4. It's an operating system. Because it gives you so much power in specifying an exact state of a build output (a \"derivation\"), people have used it to create [nixos](https://nixos.org/). NixOS is a completely decoratively specified Linux distribution where your entire computer configuration lies in `.nix` text files. I have configured my computer this way, and, though it was a long process, the declarativity/reproducitivity is really nice.\n\n# Derivations\n\n### The \"nix store\"\n\nThere is this thing called the \"nix store\". It's just a directory that's read only, usually at `/nix/store`. You can write nix code, using the `derivation` keyword, to create them, but that's very low level. I'll let you read more about them [here](https://nixos.org/guides/nix-pills/06-our-first-derivation), but in this post we'll just use `pkgs.stdenv.mkDerivation`, along with trivial builders like `pkgs.writeShellScriptBin`. They are handy helper functions from the `nixpkgs` utilities (remember when I said nix was a utility library!). Everything in the `nixpkgs` registry outputs derivations. These \"derivations\" are resulting directories in this \"nix store\", that look like `derivation /nix/store/z3hhlxbckx4g3n9sw91nnvlkjvyw754p-myname.drv` -- they are just subdirectories of the store. The outputs are read only because they are in the store, and we know that everything that made its way to the store did so in a way that was pure.\n\nIf you're interested in how derivations actually work, there's some intermediate steps, including producing a `drv` file with instructions to nix on how to create it. But I'm going to skip over that.\n\n## Flakes\n\nFlakes are really just entrypoints. The older way to do things with `nix` was to use channels. I don't want to go too far into those, since they are lame (sorry, it's true), but basically, they are global references to sources. They'd let you specify internet inputs to projects with `\u003C>` syntax:\n\n```nix\n{ pkgs ? import \u003Cnixpkgs> { }, }:\n...\n```\n\nWhere we by default get `nixpkgs` from the global \"channel\" (iickk)! That's im**pure**. What if nix decides to update their `unstable` branch (nixpkgs is just a git repo!) and your package references break?\n\nSo from this point on I'm going to pretend channels don't exist, and completely stop using them (mostly). Flakes are nix's solution to the problem. They guarantee real purity.\n\nA flake has this basic structure:\n\n```nix\n# flake.nix\n{\n  inputs = {}; # Specify inputs with URIs\n  outputs = {self, ...}: {}; # A function!\n}\n```\n\nIt lives in a `flake.nix`\n\nWhere we usually make inputs at least have `nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";`, the nix package registry, with all the packages we may ever want. `self` is a reference to the project at its previous commit, and flakes require you to use version control to ensure purity of file inputs.\n\nYou might be thinking, \"hey, `github:nixos/nixpkgs/nixos-unstable` doesn't sound very pure!\" True. `flakes` generate lockfiles with reversions and `sha256` hashes. When we run `nix flake lock` we get something like this.\n\n```json\n// flake.lock\n{\n  \"nodes\": {\n    \"nixpkgs\": {\n      \"locked\": {\n        \"lastModified\": 1719690277,\n        \"narHash\": \"sha256-0xSej1g7eP2kaUF+JQp8jdyNmpmCJKRpO12mKl/36Kc=\",\n        \"owner\": \"NixOS\",\n        \"repo\": \"nixpkgs\",\n        \"rev\": \"2741b4b489b55df32afac57bc4bfd220e8bf617e\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"NixOS\",\n        \"ref\": \"nixos-unstable\",\n        \"repo\": \"nixpkgs\",\n        \"type\": \"github\"\n      }\n    },\n  \"root\": \"root\",\n  \"version\": 7\n}\n```\n\nFlakes let us define ways to get to derivation (the things that \"build their way into\" the nix store).\n\n### A C hello world\n\nA grain of salt: I don't do much C. But I'm going to kick off this section with a C hello world, specifically because C is pretty easy to compile, and has a _build time_ requirement: a compiler like `gcc`.\n\nThis is a bit boring, but the way we package this will be helpful for some more fun stuff later.\n\nFirst, we make a `hello.c` with a hello world...\n\n```c\n#include \u003Cstdio.h>\n\nint main() {\n  printf(\"Hello, World!\\n\");\n  return 0;\n}\n```\n\nOkay, we've created the hello world. Now time to define the build with `nix`.\n\nTo start, a tiny bit more on flakes.\n\nWith flakes, there's some special `outputs` that `nix` will look for.\n\n- `packages.${system}.default`\n  - If we're running linux (this works on mac too though), this might be `packages.x86_64-linux.default`. This is basically the \"default\" thing that gets build when we do `nix build`, but if we specify something else, like `packages.${system}.foobar`, then we can build the derivation with the `nix` cli using `nix build .#foobar` instead.\n\nThis means that a flake often will look something like\n\n```nix\n{\n  inputs = { ... };\n  outputs = { self, ... }:\n    {\n      packages.x86_64-linux = {\n        default = pkgs.writeShellScriptBin \"hello\" \"echo 'hello!'\"\n      };\n    };\n}\n```\n\nHaving to define an export for every system when our project isn't really system specific is annoying, so nixers often use something called `flake-utils`, which provides a helpful `eachDefaultSystem` utility to generate the different outputs for different systems for us. Nix is `lazy`, so it'll only build for the system we need to build for when we run `nix build`.\n\n```nix\n...\n  outputs =\n    {\n      self,\n      nixpkgs,\n      flake-utils,\n    }:\n    flake-utils.lib.eachDefaultSystem (\n      system:\n      let\n        pkgs = import nixpkgs { inherit system; };\n      in\n      {\n        packages = {\n          default = # (The derivation / builder)\n        };\n...\n```\n\n`eachDefaultSystem` is a function that intakes a function that takes an argument `system`, that outputs the regular outputs based on it, and then provides outputs for all systems supported by nix (but only evaluates when you need to).\n\nReturning to our `C` hello world, we will use a helper function called `pkgs.stdenv.mkDerivation`. `nix` ships with a `derivation` keyword, but that takes a binary that expects to make the derivation, and is super raw. This helper will do a lot of the heavy lifting by breaking things up into stages -- the `unpack phase`, `build phase`, and `install phase` (and a ton others -- see [here](https://nixos.org/manual/nixpkgs/stable/#sec-stdenv-phases)).\n\n`buildInputs` specifies what should be available in the path of our builder's environment. We can use `nativeBuildInputs` since our `buildInputs` are only needed during building -- native implies that the program itself doesn't need them.\n\n#### The Sandbox\n\nAll `nix` builds happen in a special sandbox. The `src` argument specifies what should exist in the sandbox. We need to move the source code over!\n\nThe `sandbox` does a lot to ensure that we are declarative and the output is **pure**.\n\n> When sandbox builds are enabled, Nix will set up an isolated environment for each build process by constraining build inputsto improve reproducibility.\n\n> It is achieved by isolating build jobs from input sources whose contents are prone to change dynamically and without notice. For example, the mainfile system hierarchy is completely bypassed to prevent depending on files in global directories, such as`/usr/bin`, where a reference to an executable may point to different version as time goes by.\n\nAlso, it does things like disallow networking and sets the timestamp to UNIX 0 (even time is dynamic and could lead to impurity!).\n\nOkay. So let's look at the `flake`...\n\n```nix\n{\n  description = \"C Hello world\";\n\n  inputs = {\n    flake-utils.url = \"github:numtide/flake-utils\";\n    nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";\n  };\n\n  outputs =\n    {\n      self,\n      nixpkgs,\n      flake-utils,\n    }:\n    flake-utils.lib.eachDefaultSystem ( system:\n      let\n        pkgs = import nixpkgs { inherit system; };\n      in\n      {\n        packages.default = pkgs.stdenv.mkDerivation {\n          pname = \"hello\";\n          version = \"1.0\";\n          src = self; # code at the last commit\n          nativeBuildInputs = [ pkgs.gcc ];\n          buildPhase = ''\n            gcc -o hello ./hello.c\n          '';\n          installPhase = ''\n            mkdir -p $out/bin\n            cp hello $out/bin\n          '';\n        };\n      }\n    );\n}\n```\n\nWe're not quite ready. Remember when I said that we need to use version control to help our flakes guarantee purity?\n\n```bash\ngit init\ngit add *.c *.nix\ngit commit -m \"Initial commit\"\n```\n\nOkay, finally we can build and run it with nix.\n\n```\nnix run\n# OR\nnix build && ./result/bin/hello\n```\n\nAnd we get a \"hello world\"!\n\n`pkgs.stdenv.mkDerivation` resolves to a string (that's right, a piece of text). It's a path to a result in the nix store. `nix build` helps us out by making a symlinked folder `./result` that directs to that directory in the nix store (that is read only, because it's in the nix store).\n\nOkay, enough boring hello worlds in C nonsense. Here's something fun -- binary runtime dependencies, and Python!\n\n### A simple Python Script\n\nLet's consider a simple Python script that uses selenium to navigate to `https://example.com`. Usually this would be a little annoying since there's a dependency on `chrome` and `chromedriver`, but we can bundle those things with `nix` pretty easily.\n\nHere's our basic script that goes to `example.com` and yoinks the tab's title, printing it to `stdout`.\n\n```py\n#./main.py\n\nfrom selenium import webdriver\n\nif __name__ == \"__main__\":\n  driver = webdriver.Chrome()\n  driver.get('https://example.com')\n  print(driver.title)\n  driver.quit()\n```\n\nHere we have one dependency, `selenium`, and two secret ones, `chromedriver`, and `chromium`. Usually, we'd do something like\n\n```cmd\nsudo apt-get install chromium-browser\nsudo apt-get install chromium-chromedriver\n```\n\nGod forbid we're on Windows, and it's so much worse. We might have to add\n\n```py\noptions = webdriver.ChromeOptions()\noptions.binary_location = '/usr/bin/chromium-browser'\n```\n\nSo let's package it with `nix`, which will work on ALL systems (including native macos, and windows with wsl)...\n\nI have a collection of [Project Templates](https://github.com/404Wolf/Project-templates) for various different languages that use popular nix tools to create derivations for projects. I'm using [Cookiecutter](https://cookiecutter.readthedocs.io/), which is a bit of an aside, but lets you template entire projects using [Jinja](https://jinja.palletsprojects.com/en/3.1.x/), so you can specify what a \"directory\" of a, say, Python project would look like. We're going to use my python script template here, which you can get at that repo if you want, but we'll build up to it slowly.\n\n#### pkgs.writeShellScriptBin\n\n`writeShellScriptBin` is a nice little helper function\n\nIt is so simple, [here's the source code](https://github.com/NixOS/nixpkgs/blob/36e813a7630ff1bde9f6baa82757fabfbeb6e95c/pkgs/build-support/trivial-builders/default.nix#L169)...\n\n```nix\nwriteShellScriptBin = name: text:\n  writeTextFile {\n    inherit name;\n    executable = true;\n    destination = \"/bin/${name}\";\n    text = ''\n      #!${runtimeShell}\n      ${text}\n    '';\n    checkPhase = ''\n      ${stdenv.shellDryRun} \"$target\"\n    '';\n    meta.mainProgram = name;\n  };\n```\n\nruntimeShell is just `/nix/store/blahblahShaHashblahblah/bin/bash`. It literally just makes an executable bash script.\n\nOkay, so let's use it to package our `python` program...\n\nWe'll first write a `bash` script, that's roughly similar to what we want at the end of the day, but without binary dependencies\n\n```bash\nexport PATH=$PATH:${pkgs.chromedriver}/bin:${pkgs.ungoogled-chromium}/bin;\nexport PYTHONPATH= ???\n${python}/bin/python3 main.py\n```\n\nSomething like that. `nix` can handle the `python` packages for us. I'm not going to go too far into that for now. Notice the `python3.withPackages`. Most popular packages are already packaged with `nix`, and it's not hard to package ones that aren't (usually, some require messy runtime deps).\n\n```nix\n{\n  description = \"Some python script\";\n\n  inputs = {\n    flake-utils.url = \"github:numtide/flake-utils\";\n    nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";\n  };\n\n  outputs =\n    {\n      self,\n      nixpkgs,\n      flake-utils,\n    }:\n    flake-utils.lib.eachDefaultSystem (\n      system:\n      let\n        pkgs = import nixpkgs { inherit system; };\n        python = pkgs.python3.withPackages (python-pkgs: [ python-pkgs.selenium ]);\n      in\n      {\n        packages = {\n          default =\n            pkgs.writeShellScriptBin \"main\" /* bash */ # (treesitter directive)\n              ''\n                export PATH=$PATH:${pkgs.chromedriver}/bin:${pkgs.ungoogled-chromium}/bin;\n                ${python}/bin/python3 main.py\n              '';\n        };\n      }\n    );\n}\n```\n\nRemember to `git init`, `git add` and `git commit` (I didn't say this before, but commiting isn't actually necessary, if you don't `nix` will still build but complain that your codebase is dirty. Okay, now we're ready.\n\n```\nnix run\n```\n\nIt works! Chromedriver should open up, the tab should load `example.com`, and then it should grab the tab name and close.\n\n![It works!](https://static.404wolf.com/https://static.404wolf.com/Post-20240713170649585.webp)\n\nThis still is slightly impure (although, I should note that `nix` guarantees pure builds, not pure executions, so we may never be able to be 100% sure that the program will _run_ the same way) though because we're appending to our $PATH, which means that we're expecting things to exist in our PATH that may not. I'd like to avoid \"generating\" bash scripts, so instead I'm going to use a utility called `wrapProgram` to set the PATH to whatever we have at build time as our path, which has the necessary deps and is pure (because of nix), and only run the python script with a bash script (to make it executable -- we could use a python shebang too).\n\nHere's what that would look like...\n\n```nix\n{\n  description = \"Python with Selenium example\";\n\n  inputs = {\n    flake-utils.url = \"github:numtide/flake-utils\";\n    nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";\n  };\n\n  outputs =\n    {\n      self,\n      nixpkgs,\n      flake-utils,\n    }:\n    flake-utils.lib.eachDefaultSystem (\n      system:\n      let\n        pkgs = import nixpkgs { inherit system; };\n        python = pkgs.python3.withPackages (python-pkgs: [ python-pkgs.selenium ]);\n      in\n      {\n        packages = rec {\n          default = pkgs.stdenv.mkDerivation {\n            name = \"main\";\n            src = self;\n            dontUnpack = true;\n            buildInputs = [ pkgs.makeWrapper ];\n            installPhase = ''\n              mkdir -p $out/bin\n              cp ${script}/bin/* $out/bin\n            '';\n            postFixup = ''\n              wrapProgram $out/bin/main \\\n                --set PATH $PATH:${\n                  pkgs.lib.makeBinPath [\n                    pkgs.chromedriver\n                    pkgs.ungoogled-chromium\n                  ]\n                }\n            '';\n          };\n          script = pkgs.writeShellScriptBin \"main\" \"${python}/bin/python3 main.py\";\n        };\n      }\n    );\n}\n```\n\n### Bash scripts with ANY binary!\n\nIf we're okay with only kinda-pure stuff though, generating bash scripts is really fun with nix. We can write a script using literally any dependency we want.\n\nI'll provide a neat script I made to take screenshots of regions of my screen.\n\nUsually it'd look like this...\n\n```bash\nFILEPATH=/tmp/$(uuidgen)\ngrim -g \"$($slurp)\" \"https://static.404wolf.com/$FILEPATH.png\"\nwl-copy --type \"text/uri-list\" \"https://static.404wolf.com/file://$FILEPATH.png\"\nnotify-send \"Successfully saved screen capture!\" \"The png has been saved to $FILEPATH\"\n```\n\nBut that means I'd need to add `grim` (a utility to select a region of your screen on Wayland), `wl-copy` (a Wayland clipboard cli utility), and `notify-send` (a notification daemon connector cli) to my global $PATH. That's gross, and not nixy at all. What if I want 50 more random binary dependencies? If you say that's a bad idea, it's probably because you don't want to have to have people install so many things to their global `/usr/bin` to install it.\n\nI really like this paradigm my friend Trevor showed me...\n\n```nix\npartial-screenshot = pkgs.writeShellScriptBin \"partial-screenshot.sh\" ''\n  slurp=${pkgs.slurp}/bin/slurp;\n  grim=${pkgs.grim}/bin/grim;\n  wl_copy=${pkgs.wl-clipboard}/bin/wl-copy;\n  notify=${pkgs.libnotify}/bin/notify-send;\n  ${builtins.readFile ./scripts/partial-screenshot.sh}\n'';\n```\n\nThe various binary dependencies will resolve to their `nix store` paths, and since nix is lazily evaluated we can use ANY binary, and it will poof into existence at the right store path at build time. `builtins.readFile` will then inject our regular bash script that uses the stuff...\n\n```bash\nFILEPATH=/tmp/$(uuidgen)\n$grim -g \"$($slurp)\" \"https://static.404wolf.com/$FILEPATH.png\"\n$wl_copy --type \"text/uri-list\" \"https://static.404wolf.com/file://$FILEPATH.png\"\n$notify \"Successfully saved screen capture!\" \"The png has been saved to $FILEPATH\"\n```\n\nAnd if we do a `nix build`, we'll get\n\n```bash\n#!/nix/store/agkxax48k35wdmkhmmija2i2sxg8i7ny-bash-5.2p26/bin/bash\nslurp=/nix/store/hfii9xxi8vwmlq86vh2j9dl73zzy7s1w-slurp-1.5.0/bin/slurp;\ngrim=/nix/store/jkv33a361c8nlgp2kcx1azncipxdn4nh-grim-1.4.1/bin/grim;\nwl_copy=/nix/store/18rwzxp6m29m8c5bxgpxci1ad1q4kl94-wl-clipboard-2.2.1/bin/wl-copy;\nnotify=/nix/store/w141cbf1p9mcyp7vqv6a4fw4hm093qb5-libnotify-0.8.3/bin/notify-send;\nFILEPATH=/tmp/$(uuidgen)\n$grim -g \"$($slurp)\" \"https://static.404wolf.com/$FILEPATH.png\"\n$wl_copy --type \"text/uri-list\" \"https://static.404wolf.com/file://$FILEPATH.png\"\n$notify \"Successfully saved screen capture!\" \"The png has been saved to $FILEPATH\"\n```\n\nA nice, executable script, with absolute references to packages that are NOT in our path. This is one of my favorite nix things to do.\n\n## Reproducible Developer Environments\n\nAnother nice thing `nix` can do is let you create developer environments very easily. You use `pkgs.mkShell`, which creates a derivation (remember those still?), and then enters the environment of the derivation.\n\nYou can specify `buildInputs` (although you should use `packages`) to add all the things you'd want for developing the project.\n\n```nix\ndevShells = {\n    default = pkgs.mkShell {\n      packages = [\n        python\n        pkgs.pyright\n        pkgs.black\n      ];\n    };\n  };\n```\n\nAnd then you can plop it in your `flake.nix` (here's the one we worked on earlier)\n\n```nix\n{\n  description = \"{{ cookiecutter.description }}\";\n\n  inputs = {\n    flake-utils.url = \"github:numtide/flake-utils\";\n    nixpkgs.url = \"github:nixos/nixpkgs/nixos-unstable\";\n  };\n\n  outputs =\n    {\n      self,\n      nixpkgs,\n      flake-utils,\n    }:\n    flake-utils.lib.eachDefaultSystem (\n      system:\n      let\n        pkgs = import nixpkgs { inherit system; };\n        python = pkgs.python3.withPackages (python-pkgs: [ python-pkgs.selenium ]);\n      in\n      {\n        packages = {}#snip ...\n        devShells = {\n          default = pkgs.mkShell {\n            shellHook = ''\n             # run when they enter\n              echo \"Welcome to the dev environment!\"\n            '';\n            packages = [\n              python\n              pkgs.pyright\n              pkgs.black\n            ];\n          };\n        };\n      }\n    );\n}\n```\n\nThis will give you access to what our output had access to -- it updates our $PYTHONPATH so that our LSP can see the dependencies, so we can get nice red squiggles and all the good language server support. We can add any dependencies we want, and even add shell hooks to set up the developer environment further.\n\nTo enter the `devshell`, you just do `nix develop`. You can also use `direnv`, by [installing it](https://direnv.net/), and then creating a `.envrc` with the contents\n\n```env\nuse flake\n```\n\nAnd then typing `direnv allow`. It's a neat utility made so that when you cd into directories it automatically enters their respective devshell. This is great because you don't need your python and rust and android and javascript and typescript bun node encryption cli tools and so much other crap in the global path. It reduces conflicts, and all developers working on your project can have the same environment.\n\nWhen you're ready for a real, pure build, you can then just slap in a `packages.default`, and then you're set.\n\n## Some really cool builders\n\nThere's a lot of nice nix abstractions out there. This includes 3rd party builders and builtin ones. Some cool ones are\n\n- [Poetry2nix](https://github.com/nix-community/poetry2nix) to build poetry python projects easily\n- [buildMavenPackage](https://github.com/NixOS/nixpkgs/blob/master/doc/languages-frameworks/maven.section.md) to build java projects\n- [buildNpmPackage](https://github.com/NixOS/nixpkgs/blob/master/doc/languages-frameworks/javascript.section.md) for building maven packages\n- [buildGoModule](https://nixos.wiki/wiki/Go) for building go modules\n- And builders for most other languages too!\n\nOne of the issues with packaging with nix is that the `sandbox` that the building happens in must use `nix`'s primitive `fetchers` like `fetchURL` and `fetchTAR` before unpacking, and there's no internet during the build step. This poses a challenge since you can't do things like `pip install` during the build. These fancy builders basically let you do the downloads and specify hashes before the build, using the `hashes` to guarantee purity (a common `nix` technique)\n\n# Living the Nix Life (NixOS)\n\nHere is a pure, complete, declarative, plug and play NixOS configuration to describe an entire linux system.\n\n```nix\n{\n  description = \"An entire system configuration\";\n\n  inputs = {\n    nixpkgs.url = \"github:NixOS/nixpkgs/nixos-unstable\";\n  };\n\n  outputs = { self, nixpkgs, ... }@inputs: {\n    nixosConfigurations.my-nixos = nixpkgs.lib.nixosSystem {\n      system = \"x86_64-linux\";\n      modules = [];\n    };\n  };\n}\n```\n\nJust a plain old super super minimal linux setup. But this isn't really useful. There's no shell, no packages, no users, nothing useful.\n\nSo let's add some user and set up ssh...\n\n```nix\n# Credit (inspiration): https://nixos-and-flakes.thiscute.world/nixos-with-flakes/get-started-with-nixos\n{\n  description = \"An entire system configuration\";\n\n  inputs = {\n    nixpkgs.url = \"github:NixOS/nixpkgs/nixos-unstable\";\n  };\n\n  outputs = {\n    self,\n    nixpkgs,\n    ...\n  } @ inputs: {\n    nixosConfigurations.my-nixos = nixpkgs.lib.nixosSystem {\n      system = \"x86_64-linux\";\n      modules = [\n        {\n          users.users.wolf = {\n            description = \"wolf\";\n            openssh.authorizedKeys.keys = [\n              \"ssh-ed25519 %3Csome-public-key%3E wolf@wolf-pc\"\n            ];\n            packages = with pkgs; [firefox];\n          };\n          services.openssh = {\n            enable = true;\n            settings = {\n              PermitRootLogin = \"no\";\n              PasswordAuthentication = false;\n            };\n            openFirewall = true;\n          };\n        }\n      ];\n    };\n  };\n}\n```\n\n# Other Cool Things I've Come Accross\n\nThere's a million neat nix things that I come across each week. Here's a list of some cool ones that might be worth checking out...\n\n## Nix Dev Containers on Windows w/Nix WSL\n\nThanks to this [Nix on WSL](https://github.com/nix-community/NixOS-WSL) you can set up developer containers with nix devshells, defined with flakes, on Windows. You can also configure an entire NixOS configuration on Windows. This is much better than using docker dev containers!\n\n# Credits\n\nThanks to Paolo Holinski for inspiration from a Recurse Center nix presentation and Trevor Nichols for getting me into nix in the first place.","src/posts/WhirlwindTourOfNix.mdx","339a51e5a55fc56f"]